{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English to Indonesian attention based translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "1. https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk import word_tokenize\n",
    "\n",
    "fp = open('./corpus/SMERU-26870.en', 'r')\n",
    "#fp = open('./corpus/SMERU-26870.en', 'r')\n",
    "eng_text = fp.read()\n",
    "eng_text = eng_text.splitlines()\n",
    "fp.close()\n",
    "\n",
    "fp2 = open('./corpus/SMERU-26870.id', 'r')\n",
    "#fp2 = open('./corpus/SMERU-26870.id', 'r')\n",
    "id_text = fp2.read()\n",
    "id_text = id_text.splitlines()\n",
    "fp2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_text = pd.DataFrame(eng_text)\n",
    "df_eng_text = df_eng_text.rename(columns={0:'English'})\n",
    "\n",
    "df_id_text = pd.DataFrame(id_text)\n",
    "df_id_text = df_id_text.rename(columns={0:'Indonesian'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Indonesian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACKNOWLEDGEMENTS This report of Access and Equ...</td>\n",
       "      <td>UCAPAN TERIMA KASIH Laporan mengenai Akses ter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We would like to express our genuine appreciat...</td>\n",
       "      <td>Kami ingin menyampaikan apresiasi yang sebesar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We are truly grateful to the Family Court of A...</td>\n",
       "      <td>Kami sangat berterima kasih kepada Family Cour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We would also like to express our sincere grat...</td>\n",
       "      <td>Kami juga ingin menyampaikan rasa terima kasih...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We especially appreciate the support and accep...</td>\n",
       "      <td>Kami berterima kasih secara khusus atas dukung...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "0  ACKNOWLEDGEMENTS This report of Access and Equ...   \n",
       "1  We would like to express our genuine appreciat...   \n",
       "2  We are truly grateful to the Family Court of A...   \n",
       "3  We would also like to express our sincere grat...   \n",
       "4  We especially appreciate the support and accep...   \n",
       "\n",
       "                                          Indonesian  \n",
       "0  UCAPAN TERIMA KASIH Laporan mengenai Akses ter...  \n",
       "1  Kami ingin menyampaikan apresiasi yang sebesar...  \n",
       "2  Kami sangat berterima kasih kepada Family Cour...  \n",
       "3  Kami juga ingin menyampaikan rasa terima kasih...  \n",
       "4  Kami berterima kasih secara khusus atas dukung...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eng_text['English'] = df_eng_text['English'].apply(lambda x : x.strip())\n",
    "df_id_text['Indonesian'] = df_id_text['Indonesian'].apply(lambda x : x.strip())\n",
    "\n",
    "import pandas as pd\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk import word_tokenize\n",
    "\n",
    "df = pd.concat([df_eng_text, df_id_text], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Indonesian</th>\n",
       "      <th>keep_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACKNOWLEDGEMENTS This report of Access and Equ...</td>\n",
       "      <td>UCAPAN TERIMA KASIH Laporan mengenai Akses ter...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We would like to express our genuine appreciat...</td>\n",
       "      <td>Kami ingin menyampaikan apresiasi yang sebesar...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We are truly grateful to the Family Court of A...</td>\n",
       "      <td>Kami sangat berterima kasih kepada Family Cour...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We would also like to express our sincere grat...</td>\n",
       "      <td>Kami juga ingin menyampaikan rasa terima kasih...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We especially appreciate the support and accep...</td>\n",
       "      <td>Kami berterima kasih secara khusus atas dukung...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "0  ACKNOWLEDGEMENTS This report of Access and Equ...   \n",
       "1  We would like to express our genuine appreciat...   \n",
       "2  We are truly grateful to the Family Court of A...   \n",
       "3  We would also like to express our sincere grat...   \n",
       "4  We especially appreciate the support and accep...   \n",
       "\n",
       "                                          Indonesian  keep_column  \n",
       "0  UCAPAN TERIMA KASIH Laporan mengenai Akses ter...        False  \n",
       "1  Kami ingin menyampaikan apresiasi yang sebesar...        False  \n",
       "2  Kami sangat berterima kasih kepada Family Cour...         True  \n",
       "3  Kami juga ingin menyampaikan rasa terima kasih...        False  \n",
       "4  Kami berterima kasih secara khusus atas dukung...         True  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keeping to 20 to heavily restrict the scope for now to be improved as we progress\n",
    "MAX_LEN = 25\n",
    "\n",
    "def check_sentence_len(row):\n",
    "    indo_num_words = len(word_tokenize(row[\"Indonesian\"]))\n",
    "    eng_num_words = len(word_tokenize(row[\"English\"]))\n",
    "    num_words_required = MAX_LEN - 2\n",
    "    return (indo_num_words <= num_words_required) and (eng_num_words <= num_words_required)\n",
    "\n",
    "#df[\"Indo_num_words\"] = df[\"Indonesian\"].apply(str.lower).apply(word_tokenize).apply(len)\n",
    "#df[\"Eng_num_words\"] = df[\"English\"].apply(str.lower).apply(word_tokenize).apply(len)\n",
    "df[\"keep_column\"] = df.apply(check_sentence_len, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current shape: (26966, 3)\n",
      "New shape: (14003, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>English</th>\n",
       "      <th>Indonesian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>We are truly grateful to the Family Court of A...</td>\n",
       "      <td>Kami sangat berterima kasih kepada Family Cour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>We especially appreciate the support and accep...</td>\n",
       "      <td>Kami berterima kasih secara khusus atas dukung...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>SMERU would also like to thank the Australian ...</td>\n",
       "      <td>SMERU juga berterima kasih kepada Pemerintah A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>ABSTRACT</td>\n",
       "      <td>ABSTRAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>ACCESS TO JUSTICE</td>\n",
       "      <td>AKSES TERHADAP KEADILAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            English  \\\n",
       "0      2  We are truly grateful to the Family Court of A...   \n",
       "1      4  We especially appreciate the support and accep...   \n",
       "2      6  SMERU would also like to thank the Australian ...   \n",
       "3      7                                           ABSTRACT   \n",
       "4      8                                  ACCESS TO JUSTICE   \n",
       "\n",
       "                                          Indonesian  \n",
       "0  Kami sangat berterima kasih kepada Family Cour...  \n",
       "1  Kami berterima kasih secara khusus atas dukung...  \n",
       "2  SMERU juga berterima kasih kepada Pemerintah A...  \n",
       "3                                            ABSTRAK  \n",
       "4                            AKSES TERHADAP KEADILAN  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Current shape: \" + str(df.shape))\n",
    "df = df[df[\"keep_column\"]]\n",
    "print(\"New shape: \" + str(df.shape))\n",
    "df.head()\n",
    "df = df.reset_index().drop(columns=[\"keep_column\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First English sentence: ['<s>', 'acknowledgements', 'this', 'report', 'of', 'access', 'and', 'equity', 'survey', 'in', 'family', 'law', 'and', 'civil', 'status', 'issues', 'for', 'the', 'courts', 'in', 'indonesia', 'could', 'only', 'be', 'finished', 'with', 'the', 'support', 'and', 'cooperation', 'of', 'a', 'number', 'of', 'people', '</s>']\n",
      "First Indo sentence: ['<s>', 'ucapan', 'terima', 'kasih', 'laporan', 'mengenai', 'akses', 'terhadap', 'keadilan', 'pemberdayaan', 'perempuan', 'kepala', 'keluarga', 'di', 'indonesia', 'ini', 'hanya', 'dapat', 'terselesaikan', 'berkat', 'dukungan', 'dan', 'kerja', 'sama', 'dari', 'seluruh', 'pihak', 'yang', 'terlibat', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# Use a unique string to indicate START and END of a sentence.\n",
    "# Assign a unique index to them.\n",
    "START, START_IDX = '<s>',  0\n",
    "END, END_IDX = '</s>', 1\n",
    "UNK, UNK_IDX = 'UNK', 2\n",
    "\n",
    "# We use this idiom to tokenize our sentences in the dataframe column:\n",
    "# >>> DataFrame['column'].apply(str.lower).apply(word_tokenize)\n",
    "\n",
    "# Also we added the START and the END symbol to the sentences. \n",
    "english_sents = [START] + df['English'].apply(str.lower).apply(word_tokenize) + [END]\n",
    "indo_sents = [START] + df['Indonesian'].apply(str.lower).apply(word_tokenize) + [END]\n",
    "\n",
    "# We're sort of getting into the data into the shape we want. \n",
    "# But now it's still too humanly readable and redundant.\n",
    "## Cut-away: Computers like it to be simpler, more concise. -_-|||\n",
    "print('First English sentence:', english_sents[0])\n",
    "print('First Indo sentence:', indo_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 Indonesian words in Dictionary:\n",
      " [(0, '<s>'), (1, '</s>'), (2, 'UNK'), (3, 'akses'), (4, 'berkat'), (5, 'dan'), (6, 'dapat'), (7, 'dari'), (8, 'di'), (9, 'dukungan')]\n",
      "\n",
      "First 10 English words in Dictionary:\n",
      " [(0, '<s>'), (1, '</s>'), (2, 'UNK'), (3, 'a'), (4, 'access'), (5, 'acknowledgements'), (6, 'and'), (7, 'be'), (8, 'civil'), (9, 'cooperation')]\n"
     ]
    }
   ],
   "source": [
    "english_vocab = Dictionary([['<s>'], ['</s>'],['UNK']])\n",
    "english_vocab.add_documents(english_sents)\n",
    "\n",
    "indo_vocab = Dictionary([['<s>'], ['</s>'], ['UNK']])\n",
    "indo_vocab.add_documents(indo_sents)\n",
    "\n",
    "# First ten words in the vocabulary.\n",
    "print('First 10 Indonesian words in Dictionary:\\n', sorted(indo_vocab.items())[:10])\n",
    "print()\n",
    "print('First 10 English words in Dictionary:\\n', sorted(english_vocab.items())[:10])\n",
    "\n",
    "import pickle\n",
    "# Lets save our dictionaries.\n",
    "with open('./vocabs/smeru_indo_vocab.Dictionary.pkl', 'wb') as fout:\n",
    "    pickle.dump(indo_vocab, fout)\n",
    "    \n",
    "with open('./vocabs/smeru_english_vocab.Dictionary.pkl', 'wb') as fout:\n",
    "    pickle.dump(english_vocab, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0],\n",
       "        [   2],\n",
       "        [1971],\n",
       "        [7058],\n",
       "        [  15],\n",
       "        [3522],\n",
       "        [   2],\n",
       "        [   1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorizes a sentence with a given vocab\n",
    "def vectorize_sent(sent, vocab):\n",
    "    return vocab.doc2idx([START] + word_tokenize(sent.lower()) + [END], unknown_word_index=2)\n",
    "\n",
    "# Creates a PyTorch variable from a sentence against a given vocab\n",
    "def variable_from_sent(sent, vocab):\n",
    "    vsent = vectorize_sent(sent, vocab)\n",
    "    #print(vsent)\n",
    "    result = Variable(torch.LongTensor(vsent).view(-1, 1))\n",
    "    #print(result)\n",
    "    return result.cuda() if use_cuda else result\n",
    "\n",
    "# Test\n",
    "new_kopi = \"French Muslims fined for face veils\"\n",
    "variable_from_sent(new_kopi, english_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22921, 2)\n",
      "(4045, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_val = train_test_split(df, test_size=0.15)\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the training and the validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adanya kesalahan sasaran (mistargeting) juga diungkapkan oleh responden rumah tangga karena terdapat penerima yang berasal dari keluarga relatif mampu dengan indikasi memiliki kendaraan roda dua\n",
      "tensor([[ 0],\n",
      "        [29],\n",
      "        [26],\n",
      "        [13],\n",
      "        [18],\n",
      "        [19],\n",
      "        [ 3],\n",
      "        [25],\n",
      "        [14],\n",
      "        [20],\n",
      "        [21],\n",
      "        [16],\n",
      "        [15],\n",
      "        [ 8],\n",
      "        [11],\n",
      "        [12],\n",
      "        [10],\n",
      "        [ 6],\n",
      "        [28],\n",
      "        [ 4],\n",
      "        [ 9],\n",
      "        [ 5],\n",
      "        [17],\n",
      "        [23],\n",
      "        [ 7],\n",
      "        [24],\n",
      "        [22],\n",
      "        [30],\n",
      "        [27],\n",
      "        [ 1]])\n",
      "Household respondents also said that there was mistargeting because there have been instances of recipients coming from relatively well-off families who already own motorcycles\n",
      "tensor([[ 0],\n",
      "        [ 5],\n",
      "        [29],\n",
      "        [24],\n",
      "        [21],\n",
      "        [ 4],\n",
      "        [ 6],\n",
      "        [12],\n",
      "        [27],\n",
      "        [16],\n",
      "        [13],\n",
      "        [19],\n",
      "        [ 6],\n",
      "        [ 8],\n",
      "        [25],\n",
      "        [18],\n",
      "        [15],\n",
      "        [28],\n",
      "        [11],\n",
      "        [16],\n",
      "        [17],\n",
      "        [10],\n",
      "        [22],\n",
      "        [ 7],\n",
      "        [14],\n",
      "        [30],\n",
      "        [28],\n",
      "        [26],\n",
      "        [ 6],\n",
      "        [ 9],\n",
      "        [21],\n",
      "        [ 3],\n",
      "        [20],\n",
      "        [21],\n",
      "        [23],\n",
      "        [ 1]])\n"
     ]
    }
   ],
   "source": [
    "# Prepare the whole training corpus.\n",
    "indo_tensors = df_train['Indonesian'].apply(lambda s: variable_from_sent(s, indo_vocab))\n",
    "print(df_train.iloc[0]['Indonesian'])\n",
    "print(indo_tensors[0])\n",
    "english_tensors = df_train['English'].apply(lambda s: variable_from_sent(s, english_vocab))\n",
    "print(df_train.iloc[0]['English'])\n",
    "print(english_tensors[0])\n",
    "# Now, each item in `sent_pairs` is our data point. \n",
    "sent_pairs = list(zip(english_tensors, indo_tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
