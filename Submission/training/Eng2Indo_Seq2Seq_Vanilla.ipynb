{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gDr-usY9p-V7"
   },
   "source": [
    "## English to Indonesian translation using attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pIR6m0HWp-V8",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oeRMKk0hp-V_",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Loss function: https://pytorch.org/docs/stable/nn.html#torch.nn.NLLLoss\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6BwzOdjOp-WB",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "P6eOyoJEp-WE",
    "outputId": "12c20a2d-620e-4989-8b30-55c2bd0d59a3",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "fp = open('../corpus/eng-indo-augmented.txt', 'r')\n",
    "text = fp.read()\n",
    "text = text.splitlines()\n",
    "fp.close()\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "text_dict = {\"English\": [], \"Indonesian\": []}\n",
    "for l in text:\n",
    "    split_text = l.split(\"\\t\")\n",
    "    text_dict[\"English\"].append(normalizeString(split_text[0]))\n",
    "    text_dict[\"Indonesian\"].append(normalizeString(split_text[1]))\n",
    "    \n",
    "df = pd.DataFrame.from_dict(text_dict)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eGVYxutbp-WG",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 25\n",
    "MIN_LENGTH = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "colab_type": "code",
    "id": "i0YKRNLpp-WI",
    "outputId": "a7b15194-f0e0-46e8-f6cc-688f2428cfe1",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \",\n",
    "    \"tom is\", \"tom s\",\n",
    "    \"what s\", \"what a\",\n",
    "   \"are you\", \"do you\",\n",
    "   \"what is\", \"tom was\",\n",
    "   \"don t\", \"it s\", \"where s\",\n",
    "   \"where did\", \"where is\",\n",
    ")\n",
    "\n",
    "def should_keep_row(row):\n",
    "    \"\"\" Should the current row be kept as training set\"\"\"\n",
    "    # indo_num_words = len(word_tokenize(row[\"Indonesian\"]))\n",
    "    eng_num_words = len(word_tokenize(row[\"English\"]))\n",
    "    max_words_required = MAX_LENGTH - 2\n",
    "\n",
    "    return eng_num_words <= max_words_required\n",
    "\n",
    "df[\"keep_row\"] = df.apply(should_keep_row, axis=1)\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "print(\"Current shape: \" + str(df.shape))\n",
    "df = df[df[\"keep_row\"]]\n",
    "print(\"New shape: \" + str(df.shape))\n",
    "df.head()\n",
    "df = df.reset_index().drop(columns=[\"keep_row\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "pHnwWXPlp-WM",
    "outputId": "aaa5de9c-bb0e-4a39-d5a4-165d3d4889b8",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# Use a unique string to indicate START and END of a sentence.\n",
    "# Assign a unique index to them.\n",
    "START, START_IDX = '<s>',  0\n",
    "END, END_IDX = '</s>', 1\n",
    "UNK, UNK_IDX = 'UNK', 2\n",
    "\n",
    "SOS_token = START_IDX\n",
    "EOS_token = END_IDX\n",
    "\n",
    "# We use this idiom to tokenize our sentences in the dataframe column:\n",
    "# >>> DataFrame['column'].apply(str.lower).apply(word_tokenize)\n",
    "\n",
    "# Also we added the START and the END symbol to the sentences. \n",
    "english_sents = [START] + df['English'].apply(str.lower).apply(word_tokenize) + [END]\n",
    "indo_sents = [START] + df['Indonesian'].apply(str.lower).apply(word_tokenize) + [END]\n",
    "\n",
    "# We're sort of getting into the data into the shape we want. \n",
    "# But now it's still too humanly readable and redundant.\n",
    "## Cut-away: Computers like it to be simpler, more concise. -_-|||\n",
    "print('First English sentence:', english_sents[0])\n",
    "print('First Indo sentence:', indo_sents[0])\n",
    "\n",
    "english_vocab = Dictionary([['<s>'], ['</s>'],['UNK']])\n",
    "english_vocab.add_documents(english_sents)\n",
    "\n",
    "indo_vocab = Dictionary([['<s>'], ['</s>'], ['UNK']])\n",
    "indo_vocab.add_documents(indo_sents)\n",
    "\n",
    "# First ten words in the vocabulary.\n",
    "print('First 10 Indonesian words in Dictionary:\\n', sorted(indo_vocab.items())[:10])\n",
    "print()\n",
    "print('First 10 English words in Dictionary:\\n', sorted(english_vocab.items())[:10])\n",
    "\n",
    "english_vocab = Dictionary([['<s>'], ['</s>'],['UNK']])\n",
    "english_vocab.add_documents(english_sents)\n",
    "\n",
    "indo_vocab = Dictionary([['<s>'], ['</s>'], ['UNK']])\n",
    "indo_vocab.add_documents(indo_sents)\n",
    "\n",
    "# First ten words in the vocabulary.\n",
    "print('First 10 Indonesian words in Dictionary:\\n', sorted(indo_vocab.items())[:10])\n",
    "print()\n",
    "print('First 10 English words in Dictionary:\\n', sorted(english_vocab.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "czemQI48p-WO"
   },
   "source": [
    "## BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9a9U3uOlp-WP"
   },
   "outputs": [],
   "source": [
    "#input val_sent_pairs[0] english input to translate output is candidate\n",
    "#val_sent_pairs[1] reference \n",
    "def calculate_bleu_score(reference_sent,candidate_sent):\n",
    "    reference = [word_tokenize(reference_sent)]\n",
    "    candidate = word_tokenize(candidate_sent)\n",
    "    \n",
    "    if '<s>' in candidate:\n",
    "        candidate.remove('<s>')\n",
    "    if '</s>' in candidate:\n",
    "        candidate.remove('</s>')         \n",
    "    gram_1_score = sentence_bleu(reference,candidate,weights=(1, 0, 0, 0))\n",
    "    gram_2_score = sentence_bleu(reference,candidate,weights=(0.5, 0.5, 0, 0))\n",
    "    gram_3_score = sentence_bleu(reference,candidate,weights=(0.33, 0.33, 0.33, 0))\n",
    "    gram_4_score = sentence_bleu(reference,candidate,weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    blue_score = (gram_1_score+gram_2_score+gram_3_score+gram_4_score)/4\n",
    "    #print(blue_score)\n",
    "    return blue_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ILYUNA9Bp-WR",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Lets save our dictionaries.\n",
    "#with open('./vocabs/simple_indo_vocab.Dictionary.pkl', 'wb') as fout:\n",
    "#    pickle.dump(indo_vocab, fout)\n",
    "    \n",
    "#with open('./vocabs/simple_english_vocab.Dictionary.pkl', 'wb') as fout:\n",
    "#    pickle.dump(english_vocab, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "HntAoFqGp-WW",
    "outputId": "f5435af6-697c-4fac-fb48-97cbbf8101b4",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# Vectorizes a sentence with a given vocab\n",
    "def vectorize_sent(sent, vocab):\n",
    "    return vocab.doc2idx([START] + word_tokenize(sent.lower()) + [END], unknown_word_index=2)\n",
    "\n",
    "# Creates a PyTorch variable from a sentence against a given vocab\n",
    "def variable_from_sent(sent, vocab):\n",
    "    vsent = vectorize_sent(sent, vocab)\n",
    "    #print(vsent)\n",
    "    result = Variable(torch.LongTensor(vsent).view(-1, 1))\n",
    "    #print(result)\n",
    "    return result.cuda() if use_cuda else result\n",
    "\n",
    "# Test\n",
    "new_kopi = \"Is it love?\"\n",
    "variable_from_sent(new_kopi, english_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VJsfM5MRp-WZ",
    "pycharm": {}
   },
   "source": [
    "## Split into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "2FHDR32sp-WZ",
    "outputId": "a5a90cd0-a03f-4613-8838-dc16f3970cb7",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_val = train_test_split(df, test_size=0.15)\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_train.head()\n",
    "\n",
    "indo_tensors = df_train['Indonesian'].apply(lambda s: variable_from_sent(s, indo_vocab))\n",
    "print(df_train.iloc[0]['Indonesian'])\n",
    "df_train\n",
    "\n",
    "english_tensors = df_train['English'].apply(lambda s: variable_from_sent(s, english_vocab))\n",
    "#print(df_train.iloc[0]['English'])\n",
    "#print(english_tensors[0])\n",
    "# Now, each item in `sent_pairs` is our data point. \n",
    "#print(\"############################\")\n",
    "sent_pairs = list(zip(english_tensors.values, indo_tensors.values))\n",
    "#print(sent_pairs[:5])\n",
    "#print(\"############################\")\n",
    "pairs = list(zip(df_train['English'], df_train['Indonesian']))\n",
    "print(pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2z5n6tFjp-Wc",
    "outputId": "d0c9b1bc-2770-422d-eac1-a54a100a8d9a",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def get_validation_pairs(df_val_in): #MOD Anurag\n",
    "    indo_val_tensors = df_val_in['Indonesian'].apply(lambda s: variable_from_sent(s, indo_vocab))\n",
    "    english_val_tensors = df_val_in['English'].apply(lambda s: variable_from_sent(s, english_vocab))\n",
    "    val_sent_tensor_pairs = list(zip(english_val_tensors.values, indo_val_tensors.values))\n",
    "    val_sent_pairs = list(zip(df_val_in['English'], df_val_in['Indonesian']))\n",
    "    return val_sent_pairs, val_sent_tensor_pairs\n",
    "\n",
    "\n",
    "val_sent_pairs, val_sent_tensor_pairs = get_validation_pairs(df_val) #MOD Anurag\n",
    "print(val_sent_pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "S5roJrm8p-Wf",
    "outputId": "c5adab0b-ad21-45aa-cc7c-e8862350c933",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "print(val_sent_pairs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "apAoSCbup-Wh",
    "outputId": "06d68f39-46b1-4e77-9eb0-40a57780f286",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "print(val_sent_pairs[154])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "id": "jhK0Iqjzp-Wj",
    "outputId": "d6e67a2c-fd59-471d-bdbd-a4b137b41e59",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "print(val_sent_pairs[154][0])\n",
    "\n",
    "for w in val_sent_pairs[154][0].split(' '):\n",
    "    print(english_vocab.doc2idx([w]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TIMz1Kzfp-Wl",
    "pycharm": {}
   },
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DLZyDHRZp-Wm",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2BYgy7OPp-Wo",
    "pycharm": {}
   },
   "source": [
    "## Get training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aisQ_LGVp-Wp",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def get_train_loss(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    #print(\"Train\")\n",
    "    #print(\"Input tensor shape: {0}\".format(input_tensor.shape))\n",
    "    #print(\"Target tensor shape: {0}\".format(target_tensor.shape))\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "\n",
    "def get_validation_loss(input_tensor, target_tensor, encoder, decoder, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    #print(\"Validation\")\n",
    "    #print(\"Input tensor shape: {0}\".format(input_tensor.shape))\n",
    "    #print(\"Target tensor shape: {0}\".format(target_tensor.shape))\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss = criterion(decoder_output, target_tensor[di])\n",
    "            total_loss += float(loss.item())\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    return total_loss / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sXA4vX8dp-Wr",
    "pycharm": {}
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cAxSFCEXp-Ws",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "SAVE_PATH = 'results/vanila_seq2seq'\n",
    "\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "  os.makedirs(SAVE_PATH)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points, filename): # pier mod\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YbjQrQPPp-Wv"
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        # input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_tensor = variable_from_sent(sentence, english_vocab)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('</s>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(indo_vocab.id2token[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nXUXH83fp-Wy"
   },
   "outputs": [],
   "source": [
    "def translate(input_sentence, enc, dec):\n",
    "    output_words = evaluate(enc, dec, input_sentence)\n",
    "#     print('input =', input_sentence)\n",
    "#     print('output =', ' '.join(output_words))\n",
    "    candidate = ' '.join(output_words)\n",
    "    return candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9qNoZvZHp-W0",
    "pycharm": {}
   },
   "source": [
    "## Training loop and get evaluation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DG9pppqwp-W1",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, batch_size=1, print_every=1000, save_every=1000, plot_every=100,\n",
    "               learning_rate=0.0001):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    val_losses = []\n",
    "    bleu_scores = []\n",
    "    \n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    # training_pairs = [sent_pairs[i] for i in range(n_iters)]\n",
    "    training_pairs = [random.sample(sent_pairs, batch_size) for i in range(n_iters)]\n",
    "\n",
    "    # training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "    MAX_PATIENCE = 50\n",
    "    patience = MAX_PATIENCE  \n",
    "    prev_val_loss =lowest_so_far = prev_bleu =  999\n",
    "    highest_so_far = -np.inf # for bleu\n",
    "    stopping_criteria_on = True\n",
    "    using_bleu_stopping = False\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        # print(\"################################\")\n",
    "        # print(training_pair)\n",
    "        input_tensor = training_pair[0][0]\n",
    "        target_tensor = training_pair[0][1]\n",
    "        # print(\"printing tensors for training...\")\n",
    "        # print(input_tensor)\n",
    "        # print(target_tensor)\n",
    "\n",
    "        loss = get_train_loss(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "                              criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        stopping_delta = 0.001  # if improvement is not more than this amount after n tries, exit the loop\n",
    "\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('Training loss: %s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                                        iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "            total_val_loss = 0\n",
    "            total_bleu_score = 0\n",
    "            total_val_pairs = len(val_sent_tensor_pairs)\n",
    "\n",
    "            for itr in range(0, len(val_sent_tensor_pairs)):\n",
    "                val_input_tensor = val_sent_tensor_pairs[itr][0]\n",
    "                val_target_tensor = val_sent_tensor_pairs[itr][1]\n",
    "                # print(\"Validation record: {0}\".format(itr))\n",
    "                # print(val_sent_pairs[itr])\n",
    "                #calc blue score\n",
    "                reference_sent = val_sent_pairs[itr][1]\n",
    "                candidate_sent = translate(val_sent_pairs[itr][0], encoder, decoder)\n",
    "                bleu_score = calculate_bleu_score(reference_sent,candidate_sent)\n",
    "                total_bleu_score += bleu_score\n",
    "                val_loss = get_validation_loss(val_input_tensor, val_target_tensor, encoder, decoder, criterion)\n",
    "                total_val_loss += val_loss\n",
    "\n",
    "            avg_val_loss = total_val_loss / total_val_pairs\n",
    "            val_losses.append(avg_val_loss)\n",
    "            avg_bleu_scores = total_bleu_score / total_val_pairs\n",
    "            bleu_scores.append(avg_bleu_scores)\n",
    "            \n",
    "            print('Validation loss: %s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                                          iter, iter / n_iters * 100, avg_val_loss))\n",
    "            print('Bleu scores: %s (%d %d%%) %.8f' % (timeSince(start, iter / n_iters),\n",
    "                                                          iter, iter / n_iters * 100, avg_bleu_scores))\n",
    "            if  stopping_criteria_on:\n",
    "                if not using_bleu_stopping:\n",
    "                    if (prev_val_loss - avg_val_loss) > stopping_delta and avg_val_loss < lowest_so_far:\n",
    "                        print(f\"Improvement in validation loss, saving model. Prev {prev_val_loss} Curr {avg_val_loss}\")\n",
    "                        lowest_so_far = avg_val_loss\n",
    "                        encoder_save_path = '%s/%s.pth' % (SAVE_PATH, 'best_encoder')\n",
    "                        print('save encoder weights to ', encoder_save_path)\n",
    "                        torch.save(encoder.state_dict(), encoder_save_path)\n",
    "                        decoder_save_path = '%s/%s.pth' % (SAVE_PATH, 'best_decoder')\n",
    "                        print('save decoder weights to ', decoder_save_path)\n",
    "                        torch.save(decoder.state_dict(), decoder_save_path)\n",
    "                        patience = MAX_PATIENCE # reset to max\n",
    "                    else:\n",
    "                        print(f\"No improvement in validation loss, losing patience {patience}\")\n",
    "                        patience -= 1\n",
    "\n",
    "                    if patience == 0:  # break out of training\n",
    "                        break\n",
    "\n",
    "                    prev_val_loss = avg_val_loss\n",
    "                else: # bleu\n",
    "                    if (avg_bleu_scores - prev_bleu) > stopping_delta and avg_bleu_scores > highest_so_far: \n",
    "                        print(f\"Improvement in bleu scores, saving model. Prev {prev_bleu} Curr {avg_bleu_scores}\")\n",
    "                        highest_so_far = avg_bleu_scores\n",
    "                        encoder_save_path = '%s/%s.pth' % (SAVE_PATH, 'best_encoder')\n",
    "                        print('save encoder weights to ', encoder_save_path)\n",
    "                        torch.save(encoder.state_dict(), encoder_save_path)\n",
    "                        decoder_save_path = '%s/%s.pth' % (SAVE_PATH, 'best_decoder')\n",
    "                        print('save decoder weights to ', decoder_save_path)\n",
    "                        torch.save(decoder.state_dict(), decoder_save_path)\n",
    "                        patience = MAX_PATIENCE # reset to max\n",
    "                    else:\n",
    "                        print(f\"No improvement in bleu scores, losing patience {patience}\")\n",
    "                        patience -= 1\n",
    "\n",
    "                    if patience == 0:  # break out of training\n",
    "                        break \n",
    "                    \n",
    "                    prev_bleu = avg_bleu_scores\n",
    "                \n",
    "\n",
    "            print(\"##########################################################\")\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "        # # save trained encoder and decoder\n",
    "        # if iter % save_every == 0:\n",
    "        #     encoder_save_path = '%s/%s-%d.pth' % (SAVE_PATH, 'encoder', iter)\n",
    "        #     print('save encoder weights to ', encoder_save_path)\n",
    "        #     torch.save(encoder.state_dict(), encoder_save_path)\n",
    "        #     decoder_save_path = '%s/%s-%d.pth' % (SAVE_PATH, 'decoder', iter)\n",
    "        #     print('save decoder weights to ', decoder_save_path)\n",
    "        #     torch.save(decoder.state_dict(), decoder_save_path)\n",
    "\n",
    "    showPlot(plot_losses, 'train_plot.png')\n",
    "    showPlot(val_losses, 'validation_plot.png')\n",
    "    showPlot(bleu_scores,'bleu_scores_plot.png')\n",
    "    return plot_losses, val_losses, bleu_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bJ3YTGgZp-W3"
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LRio-rM_p-W5",
    "pycharm": {}
   },
   "source": [
    "## Perform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3465
    },
    "colab_type": "code",
    "id": "VRZp2jlYp-W5",
    "outputId": "344ece78-a596-4b37-84e5-8dd5c51260bb",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "encoder1 = EncoderRNN(len(english_vocab), hidden_size).to(device)\n",
    "decoder1 = DecoderRNN(hidden_size, len(indo_vocab)).to(device)\n",
    "\n",
    "plot_losses, val_losses, bleu_scores = trainIters(encoder1, decoder1, 75000, print_every=1000)\n",
    "\n",
    "showPlot(plot_losses, 'train_plot.png')\n",
    "showPlot(val_losses, 'validation_plot.png')\n",
    "showPlot(bleu_scores, 'bleu_plot.png')\n",
    "\n",
    "evaluateRandomly(encoder1, decoder1)\n",
    "\n",
    "output_words = evaluate(encoder1, decoder1, \"do you love me?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3jjv9Fiyp-W7",
    "pycharm": {}
   },
   "source": [
    "## Check some translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XHkh_erOp-W9",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "print(translate(\"tom is playing with ball .\", encoder1, decoder1))\n",
    "\n",
    "print(translate(\"she is standing there .\", encoder1, decoder1))\n",
    "\n",
    "print(translate(\"he is a bad man .\", encoder1, decoder1))\n",
    "\n",
    "print(translate(\"he wants to sleep .\", encoder1, decoder1))\n",
    "\n",
    "print(translate(\"i can't see you crying .\", encoder1, decoder1))\n",
    "\n",
    "print(translate(\"my dog is running around .\", encoder1, decoder1))\n",
    "\n",
    "print(translate(\"it is very popular .\", encoder1, decoder1))\n",
    "\n",
    "print(translate(\"she speaks american english to tom's father .\", encoder1, decoder1))\n",
    "\n",
    "print(translate(\"please eat lunch in the afternoon .\", encoder1, decoder1))\n",
    "\n",
    "print(translate(\"i see red roses in the garden .\", encoder1, decoder1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F_0qlyXup-W_",
    "pycharm": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Eng2Indo Vanila Simple corpus.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
