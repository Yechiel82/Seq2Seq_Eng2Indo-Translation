{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English to Bahasa Indonesian Seq-2-Seq Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython candies...\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "# Imports we need.\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Munging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk import word_tokenize\n",
    "\n",
    "fp = open('./corpus/bbc-468.en', 'r')\n",
    "eng_text = fp.read()\n",
    "eng_text = eng_text.splitlines()\n",
    "fp.close()\n",
    "\n",
    "fp2 = open('./corpus/bbc-468.id', 'r')\n",
    "id_text = fp2.read()\n",
    "id_text = id_text.splitlines()\n",
    "fp2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_text = pd.DataFrame(eng_text)\n",
    "df_eng_text = df_eng_text.rename(columns={0:'English'})\n",
    "\n",
    "df_id_text = pd.DataFrame(id_text)\n",
    "df_id_text = df_id_text.rename(columns={0:'Indonesian'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_text['English'] = df_eng_text['English'].apply(lambda x : x.lstrip())\n",
    "df_id_text['Indonesian'] = df_id_text['Indonesian'].apply(lambda x : x.lstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Indonesian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>French Muslims fined for face veils</td>\n",
       "      <td>Muslimah Prancis didenda karena mengenakan burka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Two French Muslim women have become the first ...</td>\n",
       "      <td>Dua Muslimah Prancis menjadi orang-orang perta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hind Ahmas and Najaite Ali were each ordered t...</td>\n",
       "      <td>Hind Ahmas dan Najaite Ali diperintahkan memba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Both say they'll appeal as far as the European...</td>\n",
       "      <td>Keduanya menyatakan akan mengajukan banding hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Some Muslim groups say veiled women have been ...</td>\n",
       "      <td>Sejumlah organisasi Muslim mengatakan wanita-w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "0                French Muslims fined for face veils   \n",
       "1  Two French Muslim women have become the first ...   \n",
       "2  Hind Ahmas and Najaite Ali were each ordered t...   \n",
       "3  Both say they'll appeal as far as the European...   \n",
       "4  Some Muslim groups say veiled women have been ...   \n",
       "\n",
       "                                          Indonesian  \n",
       "0   Muslimah Prancis didenda karena mengenakan burka  \n",
       "1  Dua Muslimah Prancis menjadi orang-orang perta...  \n",
       "2  Hind Ahmas dan Najaite Ali diperintahkan memba...  \n",
       "3  Keduanya menyatakan akan mengajukan banding hi...  \n",
       "4  Sejumlah organisasi Muslim mengatakan wanita-w...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df_eng_text, df_id_text], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Singlish sentence: ['<s>', 'french', 'muslims', 'fined', 'for', 'face', 'veils', '</s>']\n",
      "First English sentence: ['<s>', 'muslimah', 'prancis', 'didenda', 'karena', 'mengenakan', 'burka', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# Use a unique string to indicate START and END of a sentence.\n",
    "# Assign a unique index to them.\n",
    "START, START_IDX = '<s>',  0\n",
    "END, END_IDX = '</s>', 1\n",
    "\n",
    "# We use this idiom to tokenize our sentences in the dataframe column:\n",
    "# >>> DataFrame['column'].apply(str.lower).apply(word_tokenize)\n",
    "\n",
    "# Also we added the START and the END symbol to the sentences. \n",
    "english_sents = [START] + df_eng_text['English'].apply(str.lower).apply(word_tokenize) + [END]\n",
    "indo_sents = [START] + df_id_text['Indonesian'].apply(str.lower).apply(word_tokenize) + [END]\n",
    "\n",
    "# We're sort of getting into the data into the shape we want. \n",
    "# But now it's still too humanly readable and redundant.\n",
    "## Cut-away: Computers like it to be simpler, more concise. -_-|||\n",
    "print('First Singlish sentence:', english_sents[0])\n",
    "print('First English sentence:', indo_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 Indonesian words in Dictionary:\n",
      " [(0, '<s>'), (1, '</s>'), (2, 'burka'), (3, 'didenda'), (4, 'karena'), (5, 'mengenakan'), (6, 'muslimah'), (7, 'prancis'), (8, ','), (9, '.')]\n",
      "\n",
      "First 10 English words in Dictionary:\n",
      " [(0, '<s>'), (1, '</s>'), (2, 'face'), (3, 'fined'), (4, 'for'), (5, 'french'), (6, 'muslims'), (7, 'veils'), (8, ','), (9, '.')]\n"
     ]
    }
   ],
   "source": [
    "english_vocab = Dictionary([['<s>'], ['</s>']])\n",
    "english_vocab.add_documents(english_sents)\n",
    "\n",
    "indo_vocab = Dictionary([['<s>'], ['</s>']])\n",
    "indo_vocab.add_documents(indo_sents)\n",
    "\n",
    "# First ten words in the vocabulary.\n",
    "print('First 10 Indonesian words in Dictionary:\\n', sorted(indo_vocab.items())[:10])\n",
    "print()\n",
    "print('First 10 English words in Dictionary:\\n', sorted(english_vocab.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Lets save our dictionaries.\n",
    "with open('./vocabs/indo_vocab.Dictionary.pkl', 'wb') as fout:\n",
    "    pickle.dump(indo_vocab, fout)\n",
    "    \n",
    "with open('./vocabs/english_vocab.Dictionary.pkl', 'wb') as fout:\n",
    "    pickle.dump(english_vocab, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [5],\n",
       "        [6],\n",
       "        [3],\n",
       "        [4],\n",
       "        [2],\n",
       "        [7],\n",
       "        [1]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorizes a sentence with a given vocab\n",
    "def vectorize_sent(sent, vocab):\n",
    "    return vocab.doc2idx([START] + word_tokenize(sent.lower()) + [END])\n",
    "\n",
    "# Creates a PyTorch variable from a sentence against a given vocab\n",
    "def variable_from_sent(sent, vocab):\n",
    "    vsent = vectorize_sent(sent, vocab)\n",
    "    #print(vsent)\n",
    "    result = Variable(torch.LongTensor(vsent).view(-1, 1))\n",
    "    #print(result)\n",
    "    return result.cuda() if use_cuda else result\n",
    "\n",
    "# Test\n",
    "new_kopi = \"French Muslims fined for face veils\"\n",
    "variable_from_sent(new_kopi, english_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muslimah Prancis didenda karena mengenakan burka\n",
      "tensor([[0],\n",
      "        [6],\n",
      "        [7],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [2],\n",
      "        [1]], device='cuda:0')\n",
      "French Muslims fined for face veils\n",
      "tensor([[0],\n",
      "        [5],\n",
      "        [6],\n",
      "        [3],\n",
      "        [4],\n",
      "        [2],\n",
      "        [7],\n",
      "        [1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Prepare the whole training corpus.\n",
    "indo_tensors = df['Indonesian'].apply(lambda s: variable_from_sent(s, indo_vocab))\n",
    "print(df.iloc[0]['Indonesian'])\n",
    "print(indo_tensors[0])\n",
    "english_tensors = df['English'].apply(lambda s: variable_from_sent(s, english_vocab))\n",
    "print(df.iloc[0]['English'])\n",
    "print(english_tensors[0])\n",
    "# Now, each item in `sent_pairs` is our data point. \n",
    "sent_pairs = list(zip(english_tensors, indo_tensors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        # Set the no. of nodes for the hidden layer.\n",
    "        self.hidden_size = hidden_size\n",
    "        # Initialize the embedding layer with the \n",
    "        # - size of input (i.e. no. of words in input vocab)\n",
    "        # - no. of hidden nodes in the embedding layer\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # Initialize the GRU with the \n",
    "        # - size of the hidden layer from the previous state\n",
    "        # - size of the hidden layer from the current state\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # Feed the input into the embedding layer.\n",
    "        embedded = self.embedding(input).view(1, 1, -1) # Question: why (1, 1, -1)\n",
    "        # Feed the embedded layer with the hidden layer to the GRU.\n",
    "        # Update the output and hidden layer.\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initialize_hidden_states(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        return result.cuda() if use_cuda else result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # Set the no. of nodes for the hidden layer.\n",
    "        self.hidden_size = hidden_size\n",
    "        # Initialize the embedding layer with the \n",
    "        # - size of output (i.e. no. of words in output vocab)\n",
    "        # - no. of hidden nodes in the embedding layer\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        # Initialize the GRU with the \n",
    "        # - size of the hidden layer from the previous state\n",
    "        # - size of the hidden layer from the current state\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        # Set the output layer to output a specific symbol \n",
    "        # from the output vocabulary\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # Feed the input into the embedding layer.\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        # Transform the embedded output with a relu function. \n",
    "        output = F.relu(output)\n",
    "        # Feed the embedded layer with the hidden layer to the GRU.\n",
    "        # Update the output and hidden layer.\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        # Take the updated output and find the most appropriate\n",
    "        # output symbol. \n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initialize_hidden_states(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        return result.cuda() if use_cuda else result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 10\n",
    "learning_rate=0.01\n",
    "batch_size = 2\n",
    "epochs = 30 # Since we are taking batch_size=2 and epochs=30, we only look at 60 data points.\n",
    "criterion = nn.NLLLoss()\n",
    "MAX_LENGTH=80\n",
    "\n",
    "# Initialize the network for encoder and decoder.\n",
    "input_vocab, output_vocab = english_vocab, indo_vocab\n",
    "encoder = EncoderRNN(len(input_vocab), hidden_size)\n",
    "decoder = DecoderRNN(hidden_size, len(output_vocab))\n",
    "if use_cuda:\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "\n",
    "# Initialize the optimizer for encoder and decoder.\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "# If batchsize == 1, choose 1 data points per batch:\n",
    "##training_data = [[random.choice(sent_pairs)] for i in range(epochs)]\n",
    "\n",
    "# If batch_size > 1, use random.sample() instead of random.choice:\n",
    "training_data = [random.sample(sent_pairs, batch_size) for i in range(epochs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 2.3.2. Loop through the batches.\n",
    "#############################################\n",
    "# Start the training.\n",
    "for data_batch in training_data:\n",
    "    # (Re-)Initialize the optimizers, clear all gradients after every batch.\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    # Reset the loss for every batch.\n",
    "    loss = 0\n",
    "    for input_variable, target_variable in data_batch:\n",
    "        # Initialize the hidden_states for the encoder.\n",
    "        encoder_hidden = encoder.initialize_hidden_states()\n",
    "        # Initialize the length of the PyTorch variables.\n",
    "        input_length = input_variable.size()[0]\n",
    "        target_length = target_variable.size()[0]\n",
    "        encoder_outputs = Variable(torch.zeros(MAX_LENGTH, encoder.hidden_size))\n",
    "        encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input variable first: tensor([[   0],\n",
      "        [ 221],\n",
      "        [2085],\n",
      "        [2095],\n",
      "        [  55],\n",
      "        [ 154],\n",
      "        [2096],\n",
      "        [  25],\n",
      "        [2081],\n",
      "        [  23],\n",
      "        [ 281],\n",
      "        [2094],\n",
      "        [   8],\n",
      "        [ 398],\n",
      "        [  25],\n",
      "        [2081],\n",
      "        [  23],\n",
      "        [  25],\n",
      "        [ 393],\n",
      "        [   9],\n",
      "        [   1]], device='cuda:0')\n",
      "Target variable first: tensor([[   0],\n",
      "        [ 211],\n",
      "        [ 151],\n",
      "        [1095],\n",
      "        [1924],\n",
      "        [  53],\n",
      "        [  67],\n",
      "        [1143],\n",
      "        [1643],\n",
      "        [ 102],\n",
      "        [ 859],\n",
      "        [1938],\n",
      "        [   8],\n",
      "        [ 433],\n",
      "        [1928],\n",
      "        [ 376],\n",
      "        [   9],\n",
      "        [   1]], device='cuda:0')\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(\"Input variable first: {0}\".format(data_batch[0][0]))\n",
    "print(\"Target variable first: {0}\".format(data_batch[0][1]))\n",
    "print(input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 2.3.2. Loop through the batches.\n",
    "#############################################\n",
    "# Start the training.\n",
    "for data_batch in training_data:\n",
    "    # (Re-)Initialize the optimizers, clear all gradients after every batch.\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    # Reset the loss for every batch.\n",
    "    loss = 0\n",
    "    for input_variable, target_variable in data_batch:\n",
    "        # Initialize the hidden_states for the encoder.\n",
    "        encoder_hidden = encoder.initialize_hidden_states()\n",
    "        # Initialize the length of the PyTorch variables.\n",
    "        input_length = input_variable.size()[0]\n",
    "        target_length = target_variable.size()[0]\n",
    "        encoder_outputs = Variable(torch.zeros(MAX_LENGTH, encoder.hidden_size))\n",
    "        encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "        #############################################\n",
    "        # 2.3.3.  Iterating through each word in the encoder.\n",
    "        #############################################\n",
    "        # Iterating through each word in the input.\n",
    "        for ei in range(input_length):\n",
    "            # We move forward through each state.\n",
    "            encoder_output, encoder_hidden = encoder(input_variable[ei], encoder_hidden)\n",
    "            # And we save the encoder outputs. \n",
    "            # Note: We're retrieving [0][0] cos remember the weird .view(1,1,-1) for the GRU.\n",
    "            encoder_outputs[ei] = encoder_output[0][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN(\n",
      "  (embedding): Embedding(2562, 10)\n",
      "  (gru): GRU(10, 10)\n",
      ") \n",
      "\n",
      "Dictionary(2562 unique tokens: ['<s>', '</s>', 'face', 'fined', 'for']...)\n",
      "\n",
      "########\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The encoded output for the last sentence in out training_data\"\n",
    "\n",
    "# The encoder has 68 unique words\n",
    "print(encoder, '\\n')\n",
    "print(english_vocab)\n",
    "print('\\n########\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0],\n",
      "        [2484],\n",
      "        [2486],\n",
      "        [2485],\n",
      "        [1446],\n",
      "        [1624],\n",
      "        [   1]], device='cuda:0')\n",
      "########\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The last input sentence, in PyTorch Tensor data structure.\n",
    "print(data_batch[-1][0]) \n",
    "print('########\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2484, 2486, 2485, 1446, 1624, 1] \n",
      "\n",
      "########\n",
      "\n",
      "<s> czech republic refuses eu agreement </s>\n",
      "\n",
      "########\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The last input sentence as list(int)\n",
    "print(list(map(int, data_batch[-1][0])), '\\n')\n",
    "print('########\\n')\n",
    "\n",
    "# The last input sentence as list(int)\n",
    "print(' '.join([english_vocab[i] for i in map(int, data_batch[-1][0])]))\n",
    "print('\\n########\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4113, -0.2140,  0.3234,  0.5526, -0.1176, -0.5111, -0.3271,  0.3486,\n",
      "          0.1407,  0.0346],\n",
      "        [-0.0838, -0.5461,  0.5056,  0.7701, -0.2462, -0.3602,  0.1335,  0.2513,\n",
      "          0.6860,  0.1603],\n",
      "        [-0.1429, -0.6137,  0.0163,  0.3232, -0.0795,  0.0557, -0.2959,  0.4834,\n",
      "          0.0827,  0.0179],\n",
      "        [ 0.0740, -0.6394,  0.4826,  0.6482, -0.2731,  0.0048, -0.2065,  0.2635,\n",
      "          0.3000, -0.3355],\n",
      "        [ 0.5175, -0.1751,  0.5719,  0.4216, -0.4161, -0.3309,  0.1406, -0.3828,\n",
      "          0.5342, -0.3755],\n",
      "        [-0.1616, -0.7102,  0.3664,  0.0643, -0.4170, -0.0997,  0.2203,  0.2008,\n",
      "         -0.1831, -0.1250],\n",
      "        [-0.0018, -0.4342,  0.0568,  0.1878, -0.2933, -0.3445,  0.0737,  0.1993,\n",
      "         -0.0430, -0.0533],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "tensor([[[-0.0018, -0.4342,  0.0568,  0.1878, -0.2933, -0.3445,  0.0737,\n",
      "           0.1993, -0.0430, -0.0533]]], device='cuda:0',\n",
      "       grad_fn=<CudnnRnnBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_outputs)\n",
    "# The last hidden state of the last input sentence. \n",
    "# Note: For vanilla RNN (Elman Net), the last hidden state of the encoder\n",
    "#       is the start state of the decoder's hidden state.\n",
    "print(encoder_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 2.3.2. Loop through the batches.\n",
    "#############################################\n",
    "# Start the training.\n",
    "for data_batch in training_data:\n",
    "    # (Re-)Initialize the optimizers, clear all gradients after every batch.\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    # Reset the loss for every batch.\n",
    "    loss = 0\n",
    "    for input_variable, target_variable in data_batch:\n",
    "        # Initialize the hidden_states for the encoder.\n",
    "        encoder_hidden = encoder.initialize_hidden_states()\n",
    "        # Initialize the length of the PyTorch variables.\n",
    "        input_length = input_variable.size()[0]\n",
    "        target_length = target_variable.size()[0]\n",
    "        encoder_outputs = Variable(torch.zeros(MAX_LENGTH, encoder.hidden_size))\n",
    "        encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "        \n",
    "        #############################################\n",
    "        # 2.3.3.  Iterating through each word in the encoder.\n",
    "        #############################################\n",
    "        # Iterating through each word in the input.\n",
    "        for ei in range(input_length):\n",
    "            # We move forward through each state.\n",
    "            encoder_output, encoder_hidden = encoder(input_variable[ei], encoder_hidden)\n",
    "            # And we save the encoder outputs. \n",
    "            # Note: We're retrieving [0][0] cos remember the weird .view(1,1,-1) -_-|||\n",
    "            encoder_outputs[ei] = encoder_output[0][0] \n",
    "            \n",
    "            #############################################\n",
    "            # 2.3.4.  Iterating through each word in the decoder.\n",
    "            #############################################\n",
    "            # Initialize the variable input with the index of the START.\n",
    "            decoder_input = Variable(torch.LongTensor([[START_IDX]]))\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            # As the first state of the decoder, we take the last step of the encoder.\n",
    "            decoder_hidden = encoder_hidden\n",
    "            # Iterate through each state in the decoder.\n",
    "            # Note: when we are training we know the length of the decoder.\n",
    "            #       so we can use the trick to restrict the loop when decoding.\n",
    "            for di in range(target_length):\n",
    "                # We move forward through each state.\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                # What are all these weird syntax, refer to 2.3.4.1\n",
    "                topv, topi = decoder_output.data.topk(1) # topk gives k largest values along a certain dimension, A tuple of (values, indices) is returned,\n",
    "                ni = topi[0][0]\n",
    "\n",
    "                # Replace our decoder input for the next state with the\n",
    "                # embedding of the decoded topi guess. \n",
    "                decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "                decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "                \n",
    "                # Update our loss for this batch.\n",
    "                loss += criterion(decoder_output, target_variable[di])\n",
    "                \n",
    "                # If we see the </s> symbol, break the training.\n",
    "                if ni == END_IDX:\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecoderRNN(\n",
      "  (embedding): Embedding(2342, 10)\n",
      "  (gru): GRU(10, 10)\n",
      "  (softmax): LogSoftmax()\n",
      "  (out): Linear(in_features=10, out_features=2342, bias=True)\n",
      ") \n",
      "\n",
      "Dictionary(2342 unique tokens: ['<s>', '</s>', 'burka', 'didenda', 'karena']...)\n",
      "\n",
      "########\n",
      "\n",
      "<s> czech republic refuses eu agreement </s>\n",
      "<s> republik ceko tolak perjanjian uni eropa </s>\n",
      "\n",
      "########\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cut-away: The decoded output for the last sentence in out training_data\"\n",
    "\n",
    "# The encoder has 117 unique words\n",
    "print(decoder, '\\n')\n",
    "print(indo_vocab)\n",
    "print('\\n########\\n')\n",
    "\n",
    "# The last input sentence.\n",
    "print(' '.join([english_vocab[i] for i in map(int, data_batch[-1][0])]))\n",
    "# The last target sentence.\n",
    "print(' '.join([indo_vocab[i] for i in map(int, data_batch[-1][1])]))\n",
    "\n",
    "print('\\n########\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Training Starts Here - Backpropagation Portion Added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 2.3.2. Loop through the batches.\n",
    "#############################################\n",
    "# Start the training.\n",
    "for data_batch in training_data:\n",
    "    # (Re-)Initialize the optimizers, clear all gradients after every batch.\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    # Reset the loss for every batch.\n",
    "    loss = 0\n",
    "    for input_variable, target_variable in data_batch:\n",
    "        # Initialize the hidden_states for the encoder.\n",
    "        encoder_hidden = encoder.initialize_hidden_states()\n",
    "        # Initialize the length of the PyTorch variables.\n",
    "        input_length = input_variable.size()[0]\n",
    "        target_length = target_variable.size()[0]\n",
    "        encoder_outputs = Variable(torch.zeros(MAX_LENGTH, encoder.hidden_size))\n",
    "        encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "        \n",
    "        #############################################\n",
    "        # 2.3.3.  Iterating through each word in the encoder.\n",
    "        #############################################\n",
    "        # Iterating through each word in the input.\n",
    "        for ei in range(input_length):\n",
    "            # We move forward through each state.\n",
    "            encoder_output, encoder_hidden = encoder(input_variable[ei], encoder_hidden)\n",
    "            # And we save the encoder outputs. \n",
    "            # Note: We're retrieving [0][0] cos remember the weird .view(1,1,-1) -_-|||\n",
    "            encoder_outputs[ei] = encoder_output[0][0] \n",
    "            \n",
    "            #############################################\n",
    "            # 2.3.4.  Iterating through each word in the decoder.\n",
    "            #############################################\n",
    "            # Initialize the variable input with the index of the START.\n",
    "            decoder_input = Variable(torch.LongTensor([[START_IDX]]))\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            # As the first state of the decoder, we take the last step of the encoder.\n",
    "            decoder_hidden = encoder_hidden\n",
    "            # Iterate through each state in the decoder.\n",
    "            # Note: when we are training we know the length of the decoder.\n",
    "            #       so we can use the trick to restrict the loop when decoding.\n",
    "            for di in range(target_length):\n",
    "                # We move forward through each state.\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                # What are alll these weird syntax, refer to 2.3.4.1\n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                ni = topi[0][0]\n",
    "\n",
    "                # Replace our decoder input for the next state with the\n",
    "                # embedding of the decoded topi guess. \n",
    "                decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "                decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "                \n",
    "                # Update our loss for this batch.\n",
    "                loss += criterion(decoder_output, target_variable[di])\n",
    "                \n",
    "                # If we see the </s> symbol, break the training.\n",
    "                if ni == END_IDX:\n",
    "                    break\n",
    "    #####################################################\n",
    "    # 2.3.5 Backpropagate the Loss and Optimizers Takes a Step.\n",
    "    #####################################################\n",
    "    loss.backward() # Backpropagate.\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 10])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translator(encoder, decoder, input_variable, max_length=MAX_LENGTH):\n",
    "    # The length of the input.\n",
    "    input_length = input_variable.size()[0]\n",
    "    # For each sentence, initilize the hidden states with zeros.\n",
    "    encoder_hidden = encoder.initialize_hidden_states()\n",
    "    # Initialize the encoder outputs. \n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "    # Iterate through the input words.\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei], encoder_hidden)\n",
    "    # Initialize the decoder with the start symbol <s>.\n",
    "    decoder_input = Variable(torch.LongTensor([[START_IDX]])) \n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    # Use the last encoder hidden state as the first decoder's hidden state.\n",
    "    decoder_hidden = encoder_hidden\n",
    "    # Keep a list of the decoded words.\n",
    "    decoded_words = []\n",
    "    \n",
    "    # Iterate through the decoder states.\n",
    "    for di in range(max_length):\n",
    "        # Very similar to how the training works.\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, \n",
    "                                                 decoder_hidden)\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == END_IDX:\n",
    "            decoded_words.append(END_IDX)\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(ni.item())# changed from ni to ni.item()\n",
    "        # Replace the new decoder input for the next state \n",
    "        # with the top guess of this state.\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0],\n",
       "        [222],\n",
       "        [229],\n",
       "        [ 25],\n",
       "        [220],\n",
       "        [  1]], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'caffeine protects the brain' # if words don't appear in corpus, it will be marked as -1\n",
    "variable_from_sent(sent, english_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 80\n",
    "batches = 100000 # In this case, the PyTorch train_per_epoch() and train() code is using batch_size=1\n",
    "hidden_size = 100\n",
    "\n",
    "my_encoder = EncoderRNN(len(english_vocab), hidden_size)\n",
    "my_decoder = DecoderRNN(hidden_size, len(indo_vocab))\n",
    "\n",
    "if use_cuda:\n",
    "    my_encoder = my_encoder.cuda()\n",
    "    my_decoder = my_decoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_words = translator(my_encoder, my_decoder, \n",
    "                          variable_from_sent(sent, english_vocab))\n",
    "len(output_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kekebalan',\n",
       " 'ceausescu',\n",
       " 'ceausescu',\n",
       " 'ceausescu',\n",
       " 'pintar',\n",
       " 'ceausescu',\n",
       " 'ceausescu',\n",
       " 'pintar',\n",
       " 'ceausescu',\n",
       " 'ceausescu',\n",
       " 'pintar',\n",
       " 'ceausescu',\n",
       " 'ceausescu',\n",
       " 'pintar',\n",
       " 'ceausescu',\n",
       " 'ceausescu',\n",
       " 'pintar',\n",
       " 'ceausescu',\n",
       " 'ceausescu',\n",
       " 'pintar',\n",
       " 'ceausescu',\n",
       " 'ceausescu',\n",
       " 'pintar',\n",
       " 'ceausescu',\n",
       " 'ceausescu',\n",
       " 'pintar',\n",
       " 'ceausescu',\n",
       " 'ceausescu',\n",
       " 'pintar']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[indo_vocab[i] for i in output_words[1:30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# Some Logging and Plotting Candies to Monitor Training\n",
    "#########################################################\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    \n",
    "    \n",
    "#########################################################\n",
    "# Training per epoch,\n",
    "# Iterates across data points per epoch.\n",
    "#########################################################\n",
    "def train_one_epoch(input_variable, target_variable, encoder, decoder, \n",
    "                    encoder_optimizer, decoder_optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Function to put the variables, decoder and optimizers to train per epoch.\n",
    "    \"\"\"\n",
    "    encoder_hidden = encoder.initialize_hidden_states()\n",
    "\n",
    "    # (Re-)Initialize the optimizers, clear all gradients. \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    # Initialize the length of the PyTorch variables.\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "    encoder_outputs = Variable(torch.zeros(MAX_LENGTH, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    loss = 0\n",
    "    \n",
    "    # Iterating through each word in the input.\n",
    "    for ei in range(input_length):\n",
    "        # We move forward through each state.\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei], encoder_hidden)\n",
    "        # And we save the encoder outputs. \n",
    "        encoder_outputs[ei] = encoder_output[0][0]\n",
    "\n",
    "    # Initialize the variable input with the index of the START.\n",
    "    decoder_input = Variable(torch.LongTensor([[START_IDX]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    \n",
    "    # As the first state of the decoder, we take the last step of the encoder.\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    # Without teacher forcing: use its own predictions as the next input\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "        loss += criterion(decoder_output, target_variable[di])\n",
    "        if ni == END_IDX:\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    #return loss.data[0] / target_length\n",
    "    return loss.item() / target_length\n",
    "\n",
    "#########################################################\n",
    "# Top-level function to start the training,\n",
    "# iterates across epochs.\n",
    "#########################################################\n",
    "\n",
    "def train(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    training_pairs = [random.choice(sent_pairs) for i in range(n_iters)]\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_variable = training_pair[0]\n",
    "        target_variable = training_pair[1]\n",
    "\n",
    "        loss = train_one_epoch(input_variable, target_variable, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    \n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 2s (- 9m 42s) (100 0%) 4.5034\n",
      "0m 3s (- 8m 41s) (200 0%) 2.9066\n",
      "0m 5s (- 8m 22s) (300 1%) 3.1097\n",
      "0m 7s (- 8m 17s) (400 1%) 2.9042\n",
      "0m 9s (- 8m 22s) (500 1%) 3.0901\n",
      "0m 10s (- 8m 19s) (600 2%) 2.8719\n",
      "0m 12s (- 8m 18s) (700 2%) 2.6439\n",
      "0m 14s (- 8m 15s) (800 2%) 3.1052\n",
      "0m 16s (- 8m 11s) (900 3%) 3.1381\n",
      "0m 17s (- 8m 4s) (1000 3%) 2.6853\n",
      "0m 19s (- 8m 1s) (1100 3%) 2.9447\n",
      "0m 21s (- 7m 54s) (1200 4%) 2.3078\n",
      "0m 22s (- 7m 46s) (1300 4%) 2.6322\n",
      "0m 24s (- 7m 42s) (1400 4%) 2.5052\n",
      "0m 25s (- 7m 40s) (1500 5%) 2.5802\n",
      "0m 27s (- 7m 37s) (1600 5%) 2.6479\n",
      "0m 29s (- 7m 33s) (1700 6%) 2.5936\n",
      "0m 30s (- 7m 29s) (1800 6%) 2.6722\n",
      "0m 32s (- 7m 25s) (1900 6%) 2.6361\n",
      "0m 33s (- 7m 23s) (2000 7%) 2.5362\n",
      "0m 35s (- 7m 19s) (2100 7%) 2.5553\n",
      "0m 37s (- 7m 17s) (2200 7%) 2.8032\n",
      "0m 38s (- 7m 14s) (2300 8%) 2.5662\n",
      "0m 40s (- 7m 11s) (2400 8%) 2.7887\n",
      "0m 41s (- 7m 9s) (2500 8%) 2.6946\n",
      "0m 43s (- 7m 6s) (2600 9%) 2.6317\n",
      "0m 45s (- 7m 3s) (2700 9%) 2.7987\n",
      "0m 46s (- 7m 0s) (2800 9%) 2.5685\n",
      "0m 48s (- 6m 58s) (2900 10%) 2.9978\n",
      "0m 49s (- 6m 58s) (3000 10%) 3.2536\n",
      "0m 51s (- 6m 56s) (3100 11%) 2.9670\n",
      "0m 53s (- 6m 57s) (3200 11%) 3.4836\n",
      "0m 55s (- 6m 56s) (3300 11%) 3.8536\n",
      "0m 57s (- 6m 56s) (3400 12%) 3.5981\n",
      "0m 59s (- 6m 55s) (3500 12%) 3.3248\n",
      "1m 1s (- 6m 55s) (3600 12%) 3.8815\n",
      "1m 3s (- 6m 56s) (3700 13%) 4.2251\n",
      "1m 5s (- 6m 57s) (3800 13%) 4.2704\n",
      "1m 7s (- 6m 57s) (3900 13%) 3.9272\n",
      "1m 9s (- 6m 57s) (4000 14%) 4.0474\n",
      "1m 11s (- 6m 57s) (4100 14%) 4.0376\n",
      "1m 13s (- 6m 58s) (4200 14%) 4.6355\n",
      "1m 15s (- 6m 59s) (4300 15%) 4.2940\n",
      "1m 17s (- 6m 59s) (4400 15%) 4.1606\n",
      "1m 20s (- 6m 59s) (4500 16%) 4.3157\n",
      "1m 22s (- 6m 59s) (4600 16%) 4.1558\n",
      "1m 24s (- 6m 59s) (4700 16%) 4.1977\n",
      "1m 26s (- 6m 58s) (4800 17%) 3.5916\n",
      "1m 28s (- 6m 57s) (4900 17%) 4.0046\n",
      "1m 30s (- 6m 57s) (5000 17%) 3.9987\n",
      "1m 32s (- 6m 56s) (5100 18%) 3.9225\n",
      "1m 34s (- 6m 55s) (5200 18%) 4.1320\n",
      "1m 36s (- 6m 54s) (5300 18%) 4.3434\n",
      "1m 38s (- 6m 54s) (5400 19%) 3.9964\n",
      "1m 40s (- 6m 53s) (5500 19%) 4.0787\n",
      "1m 42s (- 6m 53s) (5600 19%) 3.9322\n",
      "1m 44s (- 6m 52s) (5700 20%) 4.3113\n",
      "1m 47s (- 6m 51s) (5800 20%) 3.9042\n",
      "1m 49s (- 6m 51s) (5900 20%) 4.4822\n",
      "1m 51s (- 6m 50s) (6000 21%) 3.9590\n",
      "1m 53s (- 6m 48s) (6100 21%) 4.0170\n",
      "1m 55s (- 6m 47s) (6200 22%) 3.9999\n",
      "1m 57s (- 6m 47s) (6300 22%) 4.3591\n",
      "1m 59s (- 6m 46s) (6400 22%) 4.0797\n",
      "2m 2s (- 6m 45s) (6500 23%) 4.2387\n",
      "2m 4s (- 6m 44s) (6600 23%) 3.8828\n",
      "2m 6s (- 6m 43s) (6700 23%) 4.3886\n",
      "2m 8s (- 6m 43s) (6800 24%) 4.2931\n",
      "2m 10s (- 6m 41s) (6900 24%) 3.9169\n",
      "2m 12s (- 6m 40s) (7000 24%) 4.1039\n",
      "2m 14s (- 6m 38s) (7100 25%) 3.9287\n",
      "2m 17s (- 6m 37s) (7200 25%) 4.1875\n",
      "2m 19s (- 6m 36s) (7300 25%) 4.1464\n",
      "2m 21s (- 6m 35s) (7400 26%) 4.0409\n",
      "2m 23s (- 6m 34s) (7500 26%) 4.3783\n",
      "2m 25s (- 6m 32s) (7600 27%) 3.9576\n",
      "2m 27s (- 6m 31s) (7700 27%) 3.9068\n",
      "2m 29s (- 6m 29s) (7800 27%) 4.2149\n",
      "2m 32s (- 6m 28s) (7900 28%) 4.1726\n",
      "2m 34s (- 6m 27s) (8000 28%) 4.1866\n",
      "2m 36s (- 6m 26s) (8100 28%) 4.1914\n",
      "2m 38s (- 6m 24s) (8200 29%) 3.9357\n",
      "2m 40s (- 6m 23s) (8300 29%) 4.1800\n",
      "2m 42s (- 6m 21s) (8400 29%) 4.0706\n",
      "2m 44s (- 6m 20s) (8500 30%) 3.9059\n",
      "2m 47s (- 6m 18s) (8600 30%) 4.0426\n",
      "2m 49s (- 6m 17s) (8700 30%) 4.0019\n",
      "2m 51s (- 6m 16s) (8800 31%) 3.9688\n",
      "2m 53s (- 6m 14s) (8900 31%) 3.8793\n",
      "2m 55s (- 6m 13s) (9000 32%) 3.8424\n",
      "2m 58s (- 6m 11s) (9100 32%) 4.2937\n",
      "3m 0s (- 6m 10s) (9200 32%) 4.1034\n",
      "3m 2s (- 6m 9s) (9300 33%) 4.1601\n",
      "3m 4s (- 6m 7s) (9400 33%) 3.6341\n",
      "3m 6s (- 6m 5s) (9500 33%) 3.8033\n",
      "3m 8s (- 6m 4s) (9600 34%) 3.8989\n",
      "3m 11s (- 6m 2s) (9700 34%) 3.8436\n",
      "3m 13s (- 6m 1s) (9800 34%) 4.1582\n",
      "3m 15s (- 6m 0s) (9900 35%) 4.2159\n",
      "3m 18s (- 5m 58s) (10000 35%) 4.0473\n",
      "3m 20s (- 5m 57s) (10100 35%) 3.9491\n",
      "3m 22s (- 5m 55s) (10200 36%) 3.8869\n",
      "3m 25s (- 5m 54s) (10300 36%) 3.8375\n",
      "3m 27s (- 5m 52s) (10400 37%) 3.8734\n",
      "3m 29s (- 5m 50s) (10500 37%) 3.6208\n",
      "3m 31s (- 5m 49s) (10600 37%) 3.7774\n",
      "3m 33s (- 5m 47s) (10700 38%) 3.7531\n",
      "3m 36s (- 5m 46s) (10800 38%) 3.9751\n",
      "3m 38s (- 5m 44s) (10900 38%) 3.8449\n",
      "3m 40s (- 5m 42s) (11000 39%) 3.6930\n",
      "3m 42s (- 5m 41s) (11100 39%) 3.6769\n",
      "3m 44s (- 5m 39s) (11200 39%) 3.5979\n",
      "3m 47s (- 5m 37s) (11300 40%) 3.9171\n",
      "3m 49s (- 5m 35s) (11400 40%) 3.7871\n",
      "3m 51s (- 5m 34s) (11500 40%) 3.9007\n",
      "3m 53s (- 5m 32s) (11600 41%) 3.8576\n",
      "3m 55s (- 5m 30s) (11700 41%) 3.6472\n",
      "3m 58s (- 5m 28s) (11800 41%) 3.8052\n",
      "4m 0s (- 5m 27s) (11900 42%) 3.6412\n",
      "4m 2s (- 5m 25s) (12000 42%) 3.7447\n",
      "4m 4s (- 5m 23s) (12100 43%) 3.7585\n",
      "4m 7s (- 5m 22s) (12200 43%) 3.6348\n",
      "4m 9s (- 5m 20s) (12300 43%) 3.7660\n",
      "4m 12s (- 5m 19s) (12400 44%) 3.7917\n",
      "4m 14s (- 5m 17s) (12500 44%) 3.8869\n",
      "4m 16s (- 5m 15s) (12600 44%) 3.6534\n",
      "4m 18s (- 5m 13s) (12700 45%) 3.6502\n",
      "4m 21s (- 5m 12s) (12800 45%) 3.5398\n",
      "4m 23s (- 5m 10s) (12900 45%) 3.5737\n",
      "4m 25s (- 5m 8s) (13000 46%) 3.7148\n",
      "4m 27s (- 5m 6s) (13100 46%) 3.5668\n",
      "4m 30s (- 5m 4s) (13200 46%) 3.6484\n",
      "4m 32s (- 5m 3s) (13300 47%) 3.7115\n",
      "4m 34s (- 5m 1s) (13400 47%) 3.4599\n",
      "4m 37s (- 4m 59s) (13500 48%) 3.4514\n",
      "4m 39s (- 4m 57s) (13600 48%) 3.6721\n",
      "4m 41s (- 4m 56s) (13700 48%) 3.6116\n",
      "4m 43s (- 4m 54s) (13800 49%) 3.4161\n",
      "4m 46s (- 4m 52s) (13900 49%) 3.3961\n",
      "4m 48s (- 4m 50s) (14000 49%) 3.4679\n",
      "4m 50s (- 4m 48s) (14100 50%) 3.3002\n",
      "4m 52s (- 4m 46s) (14200 50%) 3.5045\n",
      "4m 55s (- 4m 44s) (14300 50%) 3.4701\n",
      "4m 57s (- 4m 43s) (14400 51%) 3.4573\n",
      "4m 59s (- 4m 41s) (14500 51%) 3.2660\n",
      "5m 2s (- 4m 39s) (14600 51%) 3.4336\n",
      "5m 4s (- 4m 37s) (14700 52%) 3.4026\n",
      "5m 7s (- 4m 35s) (14800 52%) 3.4996\n",
      "5m 9s (- 4m 33s) (14900 53%) 3.2144\n",
      "5m 11s (- 4m 31s) (15000 53%) 3.1542\n",
      "5m 13s (- 4m 30s) (15100 53%) 3.1540\n",
      "5m 15s (- 4m 28s) (15200 54%) 3.2786\n",
      "5m 18s (- 4m 26s) (15300 54%) 3.3257\n",
      "5m 20s (- 4m 24s) (15400 54%) 3.2446\n",
      "5m 22s (- 4m 22s) (15500 55%) 3.2496\n",
      "5m 25s (- 4m 20s) (15600 55%) 3.2518\n",
      "5m 27s (- 4m 18s) (15700 55%) 3.0107\n",
      "5m 29s (- 4m 16s) (15800 56%) 3.4872\n",
      "5m 32s (- 4m 14s) (15900 56%) 3.2096\n",
      "5m 34s (- 4m 13s) (16000 56%) 3.2248\n",
      "5m 36s (- 4m 11s) (16100 57%) 3.1489\n",
      "5m 39s (- 4m 9s) (16200 57%) 2.9144\n",
      "5m 41s (- 4m 7s) (16300 58%) 2.9794\n",
      "5m 43s (- 4m 5s) (16400 58%) 3.2539\n",
      "5m 46s (- 4m 3s) (16500 58%) 3.2685\n",
      "5m 48s (- 4m 1s) (16600 59%) 2.9917\n",
      "5m 50s (- 3m 59s) (16700 59%) 3.1208\n",
      "5m 53s (- 3m 57s) (16800 59%) 3.0572\n",
      "5m 55s (- 3m 55s) (16900 60%) 2.8801\n",
      "5m 57s (- 3m 53s) (17000 60%) 2.9993\n",
      "6m 0s (- 3m 51s) (17100 60%) 3.1003\n",
      "6m 2s (- 3m 49s) (17200 61%) 2.9823\n",
      "6m 4s (- 3m 47s) (17300 61%) 3.1057\n",
      "6m 7s (- 3m 45s) (17400 61%) 2.9209\n",
      "6m 9s (- 3m 43s) (17500 62%) 2.8468\n",
      "6m 11s (- 3m 41s) (17600 62%) 2.8544\n",
      "6m 14s (- 3m 39s) (17700 62%) 2.9019\n",
      "6m 16s (- 3m 37s) (17800 63%) 2.7735\n",
      "6m 18s (- 3m 35s) (17900 63%) 2.8323\n",
      "6m 20s (- 3m 33s) (18000 64%) 2.8974\n",
      "6m 23s (- 3m 31s) (18100 64%) 3.1450\n",
      "6m 25s (- 3m 29s) (18200 64%) 2.9254\n",
      "6m 27s (- 3m 27s) (18300 65%) 2.8242\n",
      "6m 30s (- 3m 25s) (18400 65%) 2.8613\n",
      "6m 32s (- 3m 23s) (18500 65%) 2.7667\n",
      "6m 34s (- 3m 21s) (18600 66%) 2.8032\n",
      "6m 37s (- 3m 19s) (18700 66%) 2.8144\n",
      "6m 39s (- 3m 17s) (18800 66%) 2.9489\n",
      "6m 41s (- 3m 15s) (18900 67%) 2.7871\n",
      "6m 44s (- 3m 13s) (19000 67%) 2.6367\n",
      "6m 46s (- 3m 11s) (19100 67%) 2.7101\n",
      "6m 48s (- 3m 9s) (19200 68%) 2.7591\n",
      "6m 51s (- 3m 7s) (19300 68%) 2.5593\n",
      "6m 53s (- 3m 5s) (19400 69%) 2.5637\n",
      "6m 55s (- 3m 3s) (19500 69%) 2.6688\n",
      "6m 58s (- 3m 1s) (19600 69%) 2.6137\n",
      "7m 0s (- 2m 59s) (19700 70%) 2.6494\n",
      "7m 2s (- 2m 57s) (19800 70%) 2.7182\n",
      "7m 4s (- 2m 55s) (19900 70%) 2.7560\n",
      "7m 7s (- 2m 53s) (20000 71%) 2.9349\n",
      "7m 9s (- 2m 51s) (20100 71%) 2.7552\n",
      "7m 11s (- 2m 48s) (20200 71%) 2.5446\n",
      "7m 14s (- 2m 46s) (20300 72%) 2.5161\n",
      "7m 16s (- 2m 44s) (20400 72%) 2.6483\n",
      "7m 18s (- 2m 42s) (20500 72%) 2.6001\n",
      "7m 21s (- 2m 40s) (20600 73%) 2.6253\n",
      "7m 23s (- 2m 38s) (20700 73%) 2.5330\n",
      "7m 25s (- 2m 36s) (20800 74%) 2.6536\n",
      "7m 28s (- 2m 34s) (20900 74%) 2.5431\n",
      "7m 30s (- 2m 32s) (21000 74%) 2.5743\n",
      "7m 32s (- 2m 30s) (21100 75%) 2.4008\n",
      "7m 35s (- 2m 28s) (21200 75%) 2.6550\n",
      "7m 37s (- 2m 26s) (21300 75%) 2.4933\n",
      "7m 39s (- 2m 23s) (21400 76%) 2.4674\n",
      "7m 42s (- 2m 21s) (21500 76%) 2.4908\n",
      "7m 44s (- 2m 19s) (21600 76%) 2.4608\n",
      "7m 46s (- 2m 17s) (21700 77%) 2.2882\n",
      "7m 49s (- 2m 15s) (21800 77%) 2.4535\n",
      "7m 51s (- 2m 13s) (21900 77%) 2.4893\n",
      "7m 53s (- 2m 11s) (22000 78%) 2.3108\n",
      "7m 56s (- 2m 9s) (22100 78%) 2.4088\n",
      "7m 58s (- 2m 7s) (22200 79%) 2.5283\n",
      "8m 0s (- 2m 5s) (22300 79%) 2.2256\n",
      "8m 3s (- 2m 2s) (22400 79%) 2.2748\n",
      "8m 5s (- 2m 0s) (22500 80%) 2.2755\n",
      "8m 7s (- 1m 58s) (22600 80%) 2.1680\n",
      "8m 10s (- 1m 56s) (22700 80%) 2.2965\n",
      "8m 12s (- 1m 54s) (22800 81%) 2.4264\n",
      "8m 14s (- 1m 52s) (22900 81%) 2.2574\n",
      "8m 17s (- 1m 50s) (23000 81%) 2.2038\n",
      "8m 19s (- 1m 48s) (23100 82%) 2.5650\n",
      "8m 22s (- 1m 46s) (23200 82%) 2.3071\n",
      "8m 24s (- 1m 43s) (23300 82%) 2.3351\n",
      "8m 26s (- 1m 41s) (23400 83%) 2.3868\n",
      "8m 28s (- 1m 39s) (23500 83%) 2.3846\n",
      "8m 31s (- 1m 37s) (23600 83%) 2.2439\n",
      "8m 33s (- 1m 35s) (23700 84%) 2.4563\n",
      "8m 35s (- 1m 33s) (23800 84%) 2.2247\n",
      "8m 38s (- 1m 31s) (23900 85%) 2.3893\n",
      "8m 40s (- 1m 28s) (24000 85%) 2.6280\n",
      "8m 42s (- 1m 26s) (24100 85%) 2.7572\n",
      "8m 44s (- 1m 24s) (24200 86%) 2.3239\n",
      "8m 47s (- 1m 22s) (24300 86%) 2.4445\n",
      "8m 49s (- 1m 20s) (24400 86%) 2.3903\n",
      "8m 52s (- 1m 18s) (24500 87%) 2.3430\n",
      "8m 54s (- 1m 16s) (24600 87%) 2.4358\n",
      "8m 56s (- 1m 13s) (24700 87%) 2.2404\n",
      "8m 59s (- 1m 11s) (24800 88%) 2.2478\n",
      "9m 1s (- 1m 9s) (24900 88%) 2.0975\n",
      "9m 3s (- 1m 7s) (25000 88%) 2.1443\n",
      "9m 6s (- 1m 5s) (25100 89%) 2.1676\n",
      "9m 8s (- 1m 3s) (25200 89%) 2.3463\n",
      "9m 10s (- 1m 0s) (25300 90%) 2.3071\n",
      "9m 13s (- 0m 58s) (25400 90%) 2.2560\n",
      "9m 15s (- 0m 56s) (25500 90%) 2.1680\n",
      "9m 17s (- 0m 54s) (25600 91%) 2.3020\n",
      "9m 20s (- 0m 52s) (25700 91%) 2.1806\n",
      "9m 22s (- 0m 50s) (25800 91%) 2.1260\n",
      "9m 24s (- 0m 47s) (25900 92%) 2.0096\n",
      "9m 27s (- 0m 45s) (26000 92%) 2.0245\n",
      "9m 29s (- 0m 43s) (26100 92%) 1.9280\n",
      "9m 31s (- 0m 41s) (26200 93%) 2.1142\n",
      "9m 34s (- 0m 39s) (26300 93%) 2.1662\n",
      "9m 36s (- 0m 37s) (26400 93%) 2.0808\n",
      "9m 38s (- 0m 34s) (26500 94%) 2.0131\n",
      "9m 41s (- 0m 32s) (26600 94%) 2.0216\n",
      "9m 43s (- 0m 30s) (26700 95%) 2.0199\n",
      "9m 45s (- 0m 28s) (26800 95%) 2.0424\n",
      "9m 48s (- 0m 26s) (26900 95%) 1.9975\n",
      "9m 50s (- 0m 24s) (27000 96%) 2.0084\n",
      "9m 53s (- 0m 21s) (27100 96%) 2.1927\n",
      "9m 55s (- 0m 19s) (27200 96%) 1.9154\n",
      "9m 57s (- 0m 17s) (27300 97%) 1.8993\n",
      "9m 59s (- 0m 15s) (27400 97%) 1.9479\n",
      "10m 2s (- 0m 13s) (27500 97%) 1.9149\n",
      "10m 4s (- 0m 10s) (27600 98%) 1.9849\n",
      "10m 6s (- 0m 8s) (27700 98%) 1.9895\n",
      "10m 9s (- 0m 6s) (27800 98%) 1.7515\n",
      "10m 11s (- 0m 4s) (27900 99%) 2.0053\n",
      "10m 13s (- 0m 2s) (28000 99%) 1.8344\n",
      "10m 16s (- 0m 0s) (28100 100%) 1.9125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_LENGTH = 80\n",
    "batches = 28100 # In this case, the PyTorch train_per_epoch() and train() code is using batch_size=1\n",
    "hidden_size = 100\n",
    "\n",
    "my_encoder = EncoderRNN(len(english_vocab), hidden_size)\n",
    "my_decoder = DecoderRNN(hidden_size, len(indo_vocab))\n",
    "\n",
    "if use_cuda:\n",
    "    my_encoder = my_encoder.cuda()\n",
    "    my_decoder = my_decoder.cuda()\n",
    "\n",
    "train(my_encoder, my_decoder, batches, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 28100\n",
      "encoder_vanilla_100_28100.pkl\n"
     ]
    }
   ],
   "source": [
    "# Here's a nice bleeding edge Python trick, (only works on Python3.6)\n",
    "# F-strings for the win!!\n",
    "# See https://www.python.org/dev/peps/pep-0498/\n",
    "print(hidden_size, batches)\n",
    "print(f'encoder_vanilla_{hidden_size}_{batches}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "# In Python >= 3.6\n",
    "with open(f'./models/encoder_vanilla_{hidden_size}_{batches}.pkl', 'wb') as fout:\n",
    "    pickle.dump(my_encoder, fout)\n",
    "with open(f'./models/decoder_vanilla_{hidden_size}_{batches}.pkl', 'wb') as fout:\n",
    "    pickle.dump(my_decoder, fout)\n",
    "\n",
    "# For Python < 3.6\n",
    "with open('./models/encoder_vanilla_{}_{}.pkl'.format(hidden_size, batches), 'wb') as fout:\n",
    "    pickle.dump(my_encoder, fout)\n",
    "with open('./models/decoder_vanilla_{}_{}.pkl'.format(hidden_size, batches), 'wb') as fout:\n",
    "    pickle.dump(my_decoder, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(kopi_order):\n",
    "    output_words = translator(my_encoder, my_decoder, variable_from_sent(kopi_order, english_vocab))\n",
    "    print(output_words)\n",
    "    output_sentence = [indo_vocab[i] for i in output_words[1:output_words.index(1)]]\n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 6, 7, 3, 383, 5, 2, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'muslimah prancis didenda : mengenakan burka'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('French Muslims fined for face veils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
