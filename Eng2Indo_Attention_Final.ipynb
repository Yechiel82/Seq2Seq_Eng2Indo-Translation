{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Eng2Indo Attention Simple corpus - working copy with stopping.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "rycMa52tWIy4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## English to Indonesian translation using attention"
      ]
    },
    {
      "metadata": {
        "id": "l2qpBg3uWIy6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from nltk.translate.bleu_score import sentence_bleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tuy6ysg_WIy-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Loss function: https://pytorch.org/docs/stable/nn.html#torch.nn.NLLLoss\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "use_cuda = torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l4LvE_r2WIzB",
        "colab_type": "code",
        "outputId": "2e8f940e-bc78-4c96-cd6e-5f0ac477df4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from nltk import word_tokenize\n",
        "\n",
        "!pip install nltk  \n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "kqBsanwyWIzE",
        "colab_type": "code",
        "outputId": "428053ee-79ad-49a4-8cf5-f09fc5d74dc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "fp = open('eng-indo.txt', 'r')\n",
        "text = fp.read()\n",
        "text = text.splitlines()\n",
        "fp.close()\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "text_dict = {\"English\": [], \"Indonesian\": []}\n",
        "for l in text:\n",
        "    split_text = l.split(\"\\t\")\n",
        "    text_dict[\"English\"].append(normalizeString(split_text[0]))\n",
        "    text_dict[\"Indonesian\"].append(normalizeString(split_text[1]))\n",
        "    \n",
        "df = pd.DataFrame.from_dict(text_dict)\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6752, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Indonesian</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>run !</td>\n",
              "      <td>lari !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>who ?</td>\n",
              "      <td>siapa ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>wow !</td>\n",
              "      <td>wow !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>help !</td>\n",
              "      <td>tolong !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>jump !</td>\n",
              "      <td>lompat !</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  English Indonesian\n",
              "0   run !     lari !\n",
              "1   who ?    siapa ?\n",
              "2   wow !      wow !\n",
              "3  help !   tolong !\n",
              "4  jump !   lompat !"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "O4BBhVu5WIzL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 25\n",
        "MIN_LENGTH = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "man1RTr1WIzO",
        "colab_type": "code",
        "outputId": "68996a6a-87cb-4cde-f69e-8125ad5565d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s\",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \",\n",
        "    \"tom is\", \"tom s\",\n",
        "    \"what s\", \"what a\",\n",
        "   \"are you\", \"do you\",\n",
        "   \"what is\", \"tom was\",\n",
        "   \"don t\", \"it s\", \"where s\",\n",
        "   \"where did\", \"where is\",\n",
        ")\n",
        "\n",
        "def should_keep_row(row):\n",
        "    \"\"\" Should the current row be kept as training set\"\"\"\n",
        "    # indo_num_words = len(word_tokenize(row[\"Indonesian\"]))\n",
        "    eng_num_words = len(word_tokenize(row[\"English\"]))\n",
        "    max_words_required = MAX_LENGTH - 2\n",
        "    min_words_required = MIN_LENGTH\n",
        "\n",
        "    return min_words_required <= eng_num_words <= max_words_required\n",
        "\n",
        "df[\"keep_row\"] = df.apply(should_keep_row, axis=1)\n",
        "print(df.shape)\n",
        "df.head()\n",
        "\n",
        "print(\"Current shape: \" + str(df.shape))\n",
        "df = df[df[\"keep_row\"]]\n",
        "print(\"New shape: \" + str(df.shape))\n",
        "df.head()\n",
        "df = df.reset_index().drop(columns=[\"keep_row\"])\n",
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6752, 3)\n",
            "Current shape: (6752, 3)\n",
            "New shape: (6609, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>English</th>\n",
              "      <th>Indonesian</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>34</td>\n",
              "      <td>i m sad .</td>\n",
              "      <td>saya sedih .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>35</td>\n",
              "      <td>it s me !</td>\n",
              "      <td>ini aku !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>53</td>\n",
              "      <td>i get it .</td>\n",
              "      <td>aku mengerti .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>54</td>\n",
              "      <td>i got it .</td>\n",
              "      <td>aku mengerti .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57</td>\n",
              "      <td>i m okay .</td>\n",
              "      <td>aku baik baik saja .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index     English            Indonesian\n",
              "0     34   i m sad .          saya sedih .\n",
              "1     35   it s me !             ini aku !\n",
              "2     53  i get it .        aku mengerti .\n",
              "3     54  i got it .        aku mengerti .\n",
              "4     57  i m okay .  aku baik baik saja ."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "cZZf29huWIzS",
        "colab_type": "code",
        "outputId": "5fbc5d92-5991-47a4-e901-3e5be81c46aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "# Use a unique string to indicate START and END of a sentence.\n",
        "# Assign a unique index to them.\n",
        "START, START_IDX = '<s>',  0\n",
        "END, END_IDX = '</s>', 1\n",
        "UNK, UNK_IDX = 'UNK', 2\n",
        "\n",
        "SOS_token = START_IDX\n",
        "EOS_token = END_IDX\n",
        "\n",
        "# We use this idiom to tokenize our sentences in the dataframe column:\n",
        "# >>> DataFrame['column'].apply(str.lower).apply(word_tokenize)\n",
        "\n",
        "# Also we added the START and the END symbol to the sentences. \n",
        "english_sents = [START] + df['English'].apply(str.lower).apply(word_tokenize) + [END]\n",
        "indo_sents = [START] + df['Indonesian'].apply(str.lower).apply(word_tokenize) + [END]\n",
        "\n",
        "# We're sort of getting into the data into the shape we want. \n",
        "# But now it's still too humanly readable and redundant.\n",
        "## Cut-away: Computers like it to be simpler, more concise. -_-|||\n",
        "print('First English sentence:', english_sents[0])\n",
        "print('First Indo sentence:', indo_sents[0])\n",
        "\n",
        "english_vocab = Dictionary([['<s>'], ['</s>'],['UNK']])\n",
        "english_vocab.add_documents(english_sents)\n",
        "\n",
        "indo_vocab = Dictionary([['<s>'], ['</s>'], ['UNK']])\n",
        "indo_vocab.add_documents(indo_sents)\n",
        "\n",
        "# First ten words in the vocabulary.\n",
        "print('First 10 Indonesian words in Dictionary:\\n', sorted(indo_vocab.items())[:10])\n",
        "print()\n",
        "print('First 10 English words in Dictionary:\\n', sorted(english_vocab.items())[:10])\n",
        "\n",
        "english_vocab = Dictionary([['<s>'], ['</s>'],['UNK']])\n",
        "english_vocab.add_documents(english_sents)\n",
        "\n",
        "indo_vocab = Dictionary([['<s>'], ['</s>'], ['UNK']])\n",
        "indo_vocab.add_documents(indo_sents)\n",
        "\n",
        "# First ten words in the vocabulary.\n",
        "print('First 10 Indonesian words in Dictionary:\\n', sorted(indo_vocab.items())[:10])\n",
        "print()\n",
        "print('First 10 English words in Dictionary:\\n', sorted(english_vocab.items())[:10])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First English sentence: ['<s>', 'i', 'm', 'sad', '.', '</s>']\n",
            "First Indo sentence: ['<s>', 'saya', 'sedih', '.', '</s>']\n",
            "First 10 Indonesian words in Dictionary:\n",
            " [(0, '<s>'), (1, '</s>'), (2, 'UNK'), (3, '.'), (4, 'saya'), (5, 'sedih'), (6, '!'), (7, 'aku'), (8, 'ini'), (9, 'mengerti')]\n",
            "\n",
            "First 10 English words in Dictionary:\n",
            " [(0, '<s>'), (1, '</s>'), (2, 'UNK'), (3, '.'), (4, 'i'), (5, 'm'), (6, 'sad'), (7, '!'), (8, 'it'), (9, 'me')]\n",
            "First 10 Indonesian words in Dictionary:\n",
            " [(0, '<s>'), (1, '</s>'), (2, 'UNK'), (3, '.'), (4, 'saya'), (5, 'sedih'), (6, '!'), (7, 'aku'), (8, 'ini'), (9, 'mengerti')]\n",
            "\n",
            "First 10 English words in Dictionary:\n",
            " [(0, '<s>'), (1, '</s>'), (2, 'UNK'), (3, '.'), (4, 'i'), (5, 'm'), (6, 'sad'), (7, '!'), (8, 'it'), (9, 'me')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RjdR62hJWIzW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#input val_sent_pairs[0] english input to translate output is candidate\n",
        "#val_sent_pairs[1] reference \n",
        "def calculate_bleu_score(reference_sent,candidate_sent):\n",
        "    reference = [word_tokenize(reference_sent)]\n",
        "    candidate = word_tokenize(candidate_sent)\n",
        "    \n",
        "    if '<s>' in candidate:\n",
        "        candidate.remove('<s>')\n",
        "    if '</s>' in candidate:\n",
        "        candidate.remove('</s>')         \n",
        "    gram_1_score = sentence_bleu(reference,candidate,weights=(1, 0, 0, 0))\n",
        "    gram_2_score = sentence_bleu(reference,candidate,weights=(0.5, 0.5, 0, 0))\n",
        "    gram_3_score = sentence_bleu(reference,candidate,weights=(0.33, 0.33, 0.33, 0))\n",
        "    gram_4_score = sentence_bleu(reference,candidate,weights=(0.25, 0.25, 0.25, 0.25))\n",
        "    blue_score = (gram_1_score+gram_2_score+gram_3_score+gram_4_score)/4\n",
        "    #print(blue_score)\n",
        "    return blue_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kWpJQ74mWIza",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# Lets save our dictionaries.\n",
        "#with open('./vocabs/simple_indo_vocab.Dictionary.pkl', 'wb') as fout:\n",
        "#    pickle.dump(indo_vocab, fout)\n",
        "    \n",
        "#with open('./vocabs/simple_english_vocab.Dictionary.pkl', 'wb') as fout:\n",
        "#    pickle.dump(english_vocab, fout)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aLocWKZ_WIzd",
        "colab_type": "code",
        "outputId": "2da4f69f-d088-463d-827e-64c6b8a881a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "# Vectorizes a sentence with a given vocab\n",
        "def vectorize_sent(sent, vocab):\n",
        "    return vocab.doc2idx([START] + word_tokenize(sent.lower()) + [END], unknown_word_index=2)\n",
        "\n",
        "# Creates a PyTorch variable from a sentence against a given vocab\n",
        "def variable_from_sent(sent, vocab):\n",
        "    vsent = vectorize_sent(sent, vocab)\n",
        "    #print(vsent)\n",
        "    result = Variable(torch.LongTensor(vsent).view(-1, 1))\n",
        "    #print(result)\n",
        "    return result.cuda() if use_cuda else result\n",
        "\n",
        "# Test\n",
        "new_kopi = \"Is it love?\"\n",
        "variable_from_sent(new_kopi, english_vocab)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0],\n",
              "        [32],\n",
              "        [ 8],\n",
              "        [45],\n",
              "        [15],\n",
              "        [ 1]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "nXn7WpKGWIzg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Split into train and validation"
      ]
    },
    {
      "metadata": {
        "id": "34nfNaefWIzh",
        "colab_type": "code",
        "outputId": "41f4902b-3109-4924-ac76-d316c5501a8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "df_train, df_val = train_test_split(df, test_size=0.15)\n",
        "print(df_train.shape)\n",
        "print(df_val.shape)\n",
        "\n",
        "df_train = df_train.reset_index(drop=True)\n",
        "df_val = df_val.reset_index(drop=True)\n",
        "df_train.head()\n",
        "\n",
        "indo_tensors = df_train['Indonesian'].apply(lambda s: variable_from_sent(s, indo_vocab))\n",
        "print(df_train.iloc[0]['Indonesian'])\n",
        "df_train\n",
        "\n",
        "english_tensors = df_train['English'].apply(lambda s: variable_from_sent(s, english_vocab))\n",
        "#print(df_train.iloc[0]['English'])\n",
        "#print(english_tensors[0])\n",
        "# Now, each item in `sent_pairs` is our data point. \n",
        "#print(\"############################\")\n",
        "sent_pairs = list(zip(english_tensors.values, indo_tensors.values))\n",
        "#print(sent_pairs[:5])\n",
        "#print(\"############################\")\n",
        "pairs = list(zip(df_train['English'], df_train['Indonesian']))\n",
        "print(pairs[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5617, 3)\n",
            "(992, 3)\n",
            "kau harus menjaga mulutmu agar tetap diam .\n",
            "('you should keep your mouth shut .', 'kau harus menjaga mulutmu agar tetap diam .')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "efvf_EetWIzn",
        "colab_type": "code",
        "outputId": "54523297-023a-4500-aea9-2fe33458b648",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def get_validation_pairs(df_val_in): #MOD Anurag\n",
        "    indo_val_tensors = df_val_in['Indonesian'].apply(lambda s: variable_from_sent(s, indo_vocab))\n",
        "    english_val_tensors = df_val_in['English'].apply(lambda s: variable_from_sent(s, english_vocab))\n",
        "    val_sent_tensor_pairs = list(zip(english_val_tensors.values, indo_val_tensors.values))\n",
        "    val_sent_pairs = list(zip(df_val_in['English'], df_val_in['Indonesian']))\n",
        "    return val_sent_pairs, val_sent_tensor_pairs\n",
        "\n",
        "\n",
        "val_sent_pairs, val_sent_tensor_pairs = get_validation_pairs(df_val) #MOD Anurag\n",
        "print(val_sent_pairs[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('we climbed mt . fuji last summer .', 'kami mendaki gunung fuji musim panas tahun lalu .')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v2T2sGmRWIzs",
        "colab_type": "code",
        "outputId": "aeb87e11-b511-4016-bd6e-55d0811ed606",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(val_sent_pairs[-1])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('tom seems stunned .', 'tom sepertinya terkejut .')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bYb5Q3tyWIzy",
        "colab_type": "code",
        "outputId": "e9c1a85a-085e-48cf-cc8f-96374c4b05a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(val_sent_pairs[154])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('what sports do you like ?', 'olahraga apa yang kau sukai ?')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HN_KA6_GWIz4",
        "colab_type": "code",
        "outputId": "f4b9582f-a649-4dff-b06c-7707e286a752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "print(val_sent_pairs[154][0])\n",
        "\n",
        "for w in val_sent_pairs[154][0].split(' '):\n",
        "    print(english_vocab.doc2idx([w]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "what sports do you like ?\n",
            "[31]\n",
            "[1247]\n",
            "[66]\n",
            "[35]\n",
            "[44]\n",
            "[15]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "s4gcyKGOWIz_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define model"
      ]
    },
    {
      "metadata": {
        "id": "nkjIwC9vWI0B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fy8wdwWLWI0F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Get training and validation loss <- Bug here for evaluation loss"
      ]
    },
    {
      "metadata": {
        "id": "nDRV7_JHWI0H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def get_train_loss(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "    \n",
        "    #print(\"Train\")\n",
        "    #print(\"Input tensor shape: {0}\".format(input_tensor.shape))\n",
        "    #print(\"Target tensor shape: {0}\".format(target_tensor.shape))\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length\n",
        "\n",
        "def get_validation_loss(input_tensor, target_tensor, encoder, decoder, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "    \n",
        "    #print(\"Validation\")\n",
        "    #print(\"Input tensor shape: {0}\".format(input_tensor.shape))\n",
        "    #print(\"Target tensor shape: {0}\".format(target_tensor.shape))\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss = criterion(decoder_output, target_tensor[di])\n",
        "            total_loss += float(loss.item())\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    return total_loss / target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ePf3q2gFWI0R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Utilities"
      ]
    },
    {
      "metadata": {
        "id": "9jQVkj64WI0T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "\n",
        "SAVE_PATH = 'results'\n",
        "\n",
        "if not os.path.exists(SAVE_PATH):\n",
        "  os.makedirs(SAVE_PATH)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xDZRZpqhWI0W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training loop and get evaluation result"
      ]
    },
    {
      "metadata": {
        "id": "IJx18gLeWI0Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainIters(encoder, decoder, n_iters, batch_size = 1, print_every=1000, save_every=1000, plot_every=100, learning_rate=0.0001):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    val_losses = []\n",
        "    bleu_scores = []\n",
        "    \n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    #training_pairs = [sent_pairs[i] for i in range(n_iters)]\n",
        "    training_pairs = [random.sample(sent_pairs, batch_size) for i in range(n_iters)]\n",
        "\n",
        "    # training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "    MAX_PATIENCE = 50\n",
        "    patience = MAX_PATIENCE  \n",
        "    prev_val_loss =lowest_so_far = prev_bleu =  999\n",
        "    highest_so_far = -np.inf # for bleu\n",
        "    stopping_criteria_on = True\n",
        "    using_bleu_stopping = False\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        #print(\"################################\")\n",
        "        #print(training_pair)\n",
        "        input_tensor = training_pair[0][0]\n",
        "        target_tensor = training_pair[0][1]\n",
        "        #print(\"printing tensors for training...\")\n",
        "        #print(input_tensor)\n",
        "        #print(target_tensor)\n",
        "\n",
        "        loss = get_train_loss(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        stopping_delta = 0.001  # if improvement is not more than this amount after n tries, exit the loop\n",
        "\n",
        "\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('Training loss: %s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "            total_val_loss = 0\n",
        "            total_bleu_score = 0\n",
        "            total_val_pairs = len(val_sent_tensor_pairs)\n",
        "            \n",
        "            for itr in range(0, len(val_sent_tensor_pairs)):\n",
        "                val_input_tensor = val_sent_tensor_pairs[itr][0]\n",
        "                val_target_tensor = val_sent_tensor_pairs[itr][1]\n",
        "                # print(\"Validation record: {0}\".format(itr))\n",
        "                # print(val_sent_pairs[itr])\n",
        "                #calc blue score\n",
        "                reference_sent = val_sent_pairs[itr][1]\n",
        "                candidate_sent = translate(val_sent_pairs[itr][0], encoder, decoder)\n",
        "                bleu_score = calculate_bleu_score(reference_sent,candidate_sent)\n",
        "                total_bleu_score += bleu_score\n",
        "                val_loss = get_validation_loss(val_input_tensor, val_target_tensor, encoder, decoder, criterion)\n",
        "                total_val_loss += val_loss\n",
        "\n",
        "            avg_val_loss = total_val_loss / total_val_pairs\n",
        "            val_losses.append(avg_val_loss)\n",
        "            avg_bleu_scores = total_bleu_score / total_val_pairs\n",
        "            bleu_scores.append(avg_bleu_scores)\n",
        "            \n",
        "            print('Validation loss: %s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                                          iter, iter / n_iters * 100, avg_val_loss))\n",
        "            print('Bleu scores: %s (%d %d%%) %.8f' % (timeSince(start, iter / n_iters),\n",
        "                                                          iter, iter / n_iters * 100, avg_bleu_scores))\n",
        "            if  stopping_criteria_on:\n",
        "                if not using_bleu_stopping:\n",
        "                    if (prev_val_loss - avg_val_loss) > stopping_delta and avg_val_loss < lowest_so_far:\n",
        "                        print(f\"Improvement in validation loss, saving model. Prev {prev_val_loss} Curr {avg_val_loss}\")\n",
        "                        lowest_so_far = avg_val_loss\n",
        "                        encoder_save_path = '%s/%s.pth' % (SAVE_PATH, 'best_encoder')\n",
        "                        print('save encoder weights to ', encoder_save_path)\n",
        "                        torch.save(encoder.state_dict(), encoder_save_path)\n",
        "                        decoder_save_path = '%s/%s.pth' % (SAVE_PATH, 'best_decoder')\n",
        "                        print('save decoder weights to ', decoder_save_path)\n",
        "                        torch.save(decoder.state_dict(), decoder_save_path)\n",
        "                        patience = MAX_PATIENCE # reset to max\n",
        "                    else:\n",
        "                        print(f\"No improvement in validation loss, losing patience {patience}\")\n",
        "                        patience -= 1\n",
        "\n",
        "                    if patience == 0:  # break out of training\n",
        "                        break\n",
        "\n",
        "                    prev_val_loss = avg_val_loss\n",
        "                else: # bleu\n",
        "                    if (avg_bleu_scores - prev_bleu) > stopping_delta and avg_bleu_scores > highest_so_far: \n",
        "                        print(f\"Improvement in bleu scores, saving model. Prev {prev_bleu} Curr {avg_bleu_scores}\")\n",
        "                        highest_so_far = avg_bleu_scores\n",
        "                        encoder_save_path = '%s/%s.pth' % (SAVE_PATH, 'best_encoder')\n",
        "                        print('save encoder weights to ', encoder_save_path)\n",
        "                        torch.save(encoder.state_dict(), encoder_save_path)\n",
        "                        decoder_save_path = '%s/%s.pth' % (SAVE_PATH, 'best_decoder')\n",
        "                        print('save decoder weights to ', decoder_save_path)\n",
        "                        torch.save(decoder.state_dict(), decoder_save_path)\n",
        "                        patience = MAX_PATIENCE # reset to max\n",
        "                    else:\n",
        "                        print(f\"No improvement in bleu scores, losing patience {patience}\")\n",
        "                        patience -= 1\n",
        "\n",
        "                    if patience == 0:  # break out of training\n",
        "                        break \n",
        "                    \n",
        "                    prev_bleu = avg_bleu_scores\n",
        "                \n",
        "\n",
        "            print(\"##########################################################\")\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "        \n",
        "        # save trained encoder and decoder\n",
        "        if iter % save_every == 0:\n",
        "            encoder_save_path = '%s/%s-%d.pth' % (SAVE_PATH, 'encoder', iter)\n",
        "            print('save encoder weights to ', encoder_save_path)\n",
        "            torch.save(encoder.state_dict(), encoder_save_path)\n",
        "            decoder_save_path = '%s/%s-%d.pth' % (SAVE_PATH, 'decoder', iter)\n",
        "            print('save decoder weights to ', decoder_save_path)\n",
        "            torch.save(decoder.state_dict(), decoder_save_path)\n",
        "\n",
        "    showPlot(plot_losses, 'train_plot.png')\n",
        "    showPlot(val_losses, 'validation_plot.png')\n",
        "    showPlot(bleu_scores,'bleu_scores_plot.png')\n",
        "    return plot_losses, val_losses, bleu_scores\n",
        "\n",
        "\n",
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        # input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_tensor = variable_from_sent(sentence, english_vocab)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('</s>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(indo_vocab.id2token[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]\n",
        "\n",
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n",
        "        \n",
        "def translate(input_sentence, enc, dec):\n",
        "    output_words, attentions = evaluate(\n",
        "        enc, dec, input_sentence)\n",
        "#     print('input =', input_sentence)\n",
        "#     print('output =', ' '.join(output_words))\n",
        "    candidate = ' '.join(output_words)\n",
        "    return candidate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uERT7riuWI0b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Perform training"
      ]
    },
    {
      "metadata": {
        "id": "vLkx3FDdWI0c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8466
        },
        "outputId": "0baaaed0-b3a4-486a-c560-f4373b38fb99"
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 512\n",
        "encoder1 = EncoderRNN(len(english_vocab), hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, len(indo_vocab), dropout_p=0.5).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 75000, print_every=1000)\n",
        "\n",
        "evaluateRandomly(encoder1, attn_decoder1)\n",
        "\n",
        "output_words, attentions = evaluate(\n",
        "    encoder1, attn_decoder1, \"do you love me?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss: 0m 40s (- 49m 23s) (1000 1%) 4.1156\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 1m 1s (- 76m 12s) (1000 1%) 3.8766\n",
            "Bleu scores: 1m 1s (- 76m 12s) (1000 1%) 0.39367282\n",
            "Improvement in validation loss, saving model. Prev 999 Curr 3.8766165508673813\n",
            "save encoder weights to  results/best_encoder.pth\n",
            "save decoder weights to  results/best_decoder.pth\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-1000.pth\n",
            "save decoder weights to  results/decoder-1000.pth\n",
            "Training loss: 1m 46s (- 64m 35s) (2000 2%) 3.8343\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 2m 8s (- 78m 8s) (2000 2%) 3.8322\n",
            "Bleu scores: 2m 8s (- 78m 9s) (2000 2%) 0.40150818\n",
            "Improvement in validation loss, saving model. Prev 3.8766165508673813 Curr 3.8321860623367847\n",
            "save encoder weights to  results/best_encoder.pth\n",
            "save decoder weights to  results/best_decoder.pth\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-2000.pth\n",
            "save decoder weights to  results/decoder-2000.pth\n",
            "Training loss: 2m 53s (- 69m 34s) (3000 4%) 3.7448\n",
            "Validation loss: 3m 16s (- 78m 35s) (3000 4%) 3.7728\n",
            "Bleu scores: 3m 16s (- 78m 35s) (3000 4%) 0.40160369\n",
            "Improvement in validation loss, saving model. Prev 3.8321860623367847 Curr 3.772836396717293\n",
            "save encoder weights to  results/best_encoder.pth\n",
            "save decoder weights to  results/best_decoder.pth\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-3000.pth\n",
            "save decoder weights to  results/decoder-3000.pth\n",
            "Training loss: 4m 2s (- 71m 37s) (4000 5%) 3.5779\n",
            "Validation loss: 4m 25s (- 78m 30s) (4000 5%) 3.8186\n",
            "Bleu scores: 4m 25s (- 78m 30s) (4000 5%) 0.38995177\n",
            "No improvement in validation loss, losing patience 50\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-4000.pth\n",
            "save decoder weights to  results/decoder-4000.pth\n",
            "Training loss: 5m 9s (- 72m 18s) (5000 6%) 3.4311\n",
            "Validation loss: 5m 33s (- 77m 44s) (5000 6%) 3.7384\n",
            "Bleu scores: 5m 33s (- 77m 44s) (5000 6%) 0.38838940\n",
            "Improvement in validation loss, saving model. Prev 3.818642883233773 Curr 3.73838187339038\n",
            "save encoder weights to  results/best_encoder.pth\n",
            "save decoder weights to  results/best_decoder.pth\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-5000.pth\n",
            "save decoder weights to  results/decoder-5000.pth\n",
            "Training loss: 6m 19s (- 72m 38s) (6000 8%) 3.4114\n",
            "Validation loss: 6m 41s (- 76m 56s) (6000 8%) 3.5684\n",
            "Bleu scores: 6m 41s (- 76m 56s) (6000 8%) 0.39985904\n",
            "Improvement in validation loss, saving model. Prev 3.73838187339038 Curr 3.5684131795380294\n",
            "save encoder weights to  results/best_encoder.pth\n",
            "save decoder weights to  results/best_decoder.pth\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-6000.pth\n",
            "save decoder weights to  results/decoder-6000.pth\n",
            "Training loss: 7m 27s (- 72m 24s) (7000 9%) 3.2977\n",
            "Validation loss: 7m 50s (- 76m 7s) (7000 9%) 3.5923\n",
            "Bleu scores: 7m 50s (- 76m 7s) (7000 9%) 0.39833369\n",
            "No improvement in validation loss, losing patience 50\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-7000.pth\n",
            "save decoder weights to  results/decoder-7000.pth\n",
            "Training loss: 8m 35s (- 71m 59s) (8000 10%) 3.2509\n",
            "Validation loss: 8m 58s (- 75m 9s) (8000 10%) 3.5609\n",
            "Bleu scores: 8m 58s (- 75m 9s) (8000 10%) 0.39838438\n",
            "Improvement in validation loss, saving model. Prev 3.592335250575911 Curr 3.5609383738768847\n",
            "save encoder weights to  results/best_encoder.pth\n",
            "save decoder weights to  results/best_decoder.pth\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-8000.pth\n",
            "save decoder weights to  results/decoder-8000.pth\n",
            "Training loss: 9m 44s (- 71m 23s) (9000 12%) 3.1352\n",
            "Validation loss: 10m 7s (- 74m 12s) (9000 12%) 3.5875\n",
            "Bleu scores: 10m 7s (- 74m 12s) (9000 12%) 0.38999371\n",
            "No improvement in validation loss, losing patience 50\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-9000.pth\n",
            "save decoder weights to  results/decoder-9000.pth\n",
            "Training loss: 10m 52s (- 70m 42s) (10000 13%) 3.1000\n",
            "Validation loss: 11m 15s (- 73m 9s) (10000 13%) 3.4590\n",
            "Bleu scores: 11m 15s (- 73m 9s) (10000 13%) 0.39741446\n",
            "Improvement in validation loss, saving model. Prev 3.5875480290449575 Curr 3.4589525152168985\n",
            "save encoder weights to  results/best_encoder.pth\n",
            "save decoder weights to  results/best_decoder.pth\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-10000.pth\n",
            "save decoder weights to  results/decoder-10000.pth\n",
            "Training loss: 12m 1s (- 69m 55s) (11000 14%) 2.9799\n",
            "Validation loss: 12m 25s (- 72m 18s) (11000 14%) 3.6249\n",
            "Bleu scores: 12m 25s (- 72m 18s) (11000 14%) 0.37893566\n",
            "No improvement in validation loss, losing patience 50\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-11000.pth\n",
            "save decoder weights to  results/decoder-11000.pth\n",
            "Training loss: 13m 11s (- 69m 15s) (12000 16%) 2.9776\n",
            "Validation loss: 13m 34s (- 71m 15s) (12000 16%) 3.4530\n",
            "Bleu scores: 13m 34s (- 71m 15s) (12000 16%) 0.39770650\n",
            "Improvement in validation loss, saving model. Prev 3.624872669992468 Curr 3.4529975620685813\n",
            "save encoder weights to  results/best_encoder.pth\n",
            "save decoder weights to  results/best_decoder.pth\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-12000.pth\n",
            "save decoder weights to  results/decoder-12000.pth\n",
            "Training loss: 14m 20s (- 68m 25s) (13000 17%) 2.8988\n",
            "Validation loss: 14m 44s (- 70m 17s) (13000 17%) 3.4820\n",
            "Bleu scores: 14m 44s (- 70m 17s) (13000 17%) 0.38888408\n",
            "No improvement in validation loss, losing patience 50\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-13000.pth\n",
            "save decoder weights to  results/decoder-13000.pth\n",
            "Training loss: 15m 32s (- 67m 41s) (14000 18%) 2.8534\n",
            "Validation loss: 15m 56s (- 69m 25s) (14000 18%) 3.5310\n",
            "Bleu scores: 15m 56s (- 69m 25s) (14000 18%) 0.38561395\n",
            "No improvement in validation loss, losing patience 49\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-14000.pth\n",
            "save decoder weights to  results/decoder-14000.pth\n",
            "Training loss: 16m 42s (- 66m 49s) (15000 20%) 2.7492\n",
            "Validation loss: 17m 5s (- 68m 22s) (15000 20%) 3.3428\n",
            "Bleu scores: 17m 5s (- 68m 22s) (15000 20%) 0.39210422\n",
            "Improvement in validation loss, saving model. Prev 3.5310091389566676 Curr 3.3427871117691157\n",
            "save encoder weights to  results/best_encoder.pth\n",
            "save decoder weights to  results/best_decoder.pth\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-15000.pth\n",
            "save decoder weights to  results/decoder-15000.pth\n",
            "Training loss: 17m 52s (- 65m 55s) (16000 21%) 2.6681\n",
            "Validation loss: 18m 16s (- 67m 21s) (16000 21%) 3.3834\n",
            "Bleu scores: 18m 16s (- 67m 21s) (16000 21%) 0.39254257\n",
            "No improvement in validation loss, losing patience 50\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-16000.pth\n",
            "save decoder weights to  results/decoder-16000.pth\n",
            "Training loss: 19m 2s (- 64m 58s) (17000 22%) 2.6385\n",
            "Validation loss: 19m 25s (- 66m 18s) (17000 22%) 3.3134\n",
            "Bleu scores: 19m 25s (- 66m 18s) (17000 22%) 0.39239849\n",
            "Improvement in validation loss, saving model. Prev 3.383446550370487 Curr 3.3133993357683313\n",
            "save encoder weights to  results/best_encoder.pth\n",
            "save decoder weights to  results/best_decoder.pth\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-17000.pth\n",
            "save decoder weights to  results/decoder-17000.pth\n",
            "Training loss: 20m 13s (- 64m 2s) (18000 24%) 2.6280\n",
            "Validation loss: 20m 37s (- 65m 19s) (18000 24%) 3.4164\n",
            "Bleu scores: 20m 37s (- 65m 19s) (18000 24%) 0.38134552\n",
            "No improvement in validation loss, losing patience 50\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-18000.pth\n",
            "save decoder weights to  results/decoder-18000.pth\n",
            "Training loss: 21m 25s (- 63m 8s) (19000 25%) 2.5322\n",
            "Validation loss: 21m 49s (- 64m 19s) (19000 25%) 3.3627\n",
            "Bleu scores: 21m 49s (- 64m 19s) (19000 25%) 0.38068331\n",
            "No improvement in validation loss, losing patience 49\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-19000.pth\n",
            "save decoder weights to  results/decoder-19000.pth\n",
            "Training loss: 22m 36s (- 62m 9s) (20000 26%) 2.4606\n",
            "Validation loss: 22m 59s (- 63m 14s) (20000 26%) 3.2982\n",
            "Bleu scores: 22m 59s (- 63m 14s) (20000 26%) 0.38660429\n",
            "Improvement in validation loss, saving model. Prev 3.3627211954287186 Curr 3.298174146009048\n",
            "save encoder weights to  results/best_encoder.pth\n",
            "save decoder weights to  results/best_decoder.pth\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-20000.pth\n",
            "save decoder weights to  results/decoder-20000.pth\n",
            "Training loss: 23m 46s (- 61m 8s) (21000 28%) 2.4192\n",
            "Validation loss: 24m 9s (- 62m 7s) (21000 28%) 3.2656\n",
            "Bleu scores: 24m 9s (- 62m 7s) (21000 28%) 0.38653219\n",
            "Improvement in validation loss, saving model. Prev 3.298174146009048 Curr 3.2656274818946214\n",
            "save encoder weights to  results/best_encoder.pth\n",
            "save decoder weights to  results/best_decoder.pth\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-21000.pth\n",
            "save decoder weights to  results/decoder-21000.pth\n",
            "Training loss: 24m 57s (- 60m 7s) (22000 29%) 2.3801\n",
            "Validation loss: 25m 21s (- 61m 4s) (22000 29%) 3.3004\n",
            "Bleu scores: 25m 21s (- 61m 4s) (22000 29%) 0.38239012\n",
            "No improvement in validation loss, losing patience 50\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-22000.pth\n",
            "save decoder weights to  results/decoder-22000.pth\n",
            "Training loss: 26m 9s (- 59m 8s) (23000 30%) 2.2780\n",
            "Validation loss: 26m 33s (- 60m 1s) (23000 30%) 3.2989\n",
            "Bleu scores: 26m 33s (- 60m 1s) (23000 30%) 0.38104127\n",
            "No improvement in validation loss, losing patience 49\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-23000.pth\n",
            "save decoder weights to  results/decoder-23000.pth\n",
            "Training loss: 27m 20s (- 58m 5s) (24000 32%) 2.2681\n",
            "Validation loss: 27m 44s (- 58m 56s) (24000 32%) 3.2887\n",
            "Bleu scores: 27m 44s (- 58m 56s) (24000 32%) 0.38520372\n",
            "No improvement in validation loss, losing patience 48\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-24000.pth\n",
            "save decoder weights to  results/decoder-24000.pth\n",
            "Training loss: 28m 30s (- 57m 0s) (25000 33%) 2.1685\n",
            "Validation loss: 28m 54s (- 57m 48s) (25000 33%) 3.2856\n",
            "Bleu scores: 28m 54s (- 57m 48s) (25000 33%) 0.38284888\n",
            "No improvement in validation loss, losing patience 47\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-25000.pth\n",
            "save decoder weights to  results/decoder-25000.pth\n",
            "Training loss: 29m 42s (- 55m 59s) (26000 34%) 2.2273\n",
            "Validation loss: 30m 5s (- 56m 43s) (26000 34%) 3.1871\n",
            "Bleu scores: 30m 5s (- 56m 43s) (26000 34%) 0.37936418\n",
            "Improvement in validation loss, saving model. Prev 3.28555040927553 Curr 3.1870976224858123\n",
            "save encoder weights to  results/best_encoder.pth\n",
            "save decoder weights to  results/best_decoder.pth\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-26000.pth\n",
            "save decoder weights to  results/decoder-26000.pth\n",
            "Training loss: 30m 54s (- 54m 56s) (27000 36%) 2.1661\n",
            "Validation loss: 31m 17s (- 55m 38s) (27000 36%) 3.2143\n",
            "Bleu scores: 31m 17s (- 55m 38s) (27000 36%) 0.38195037\n",
            "No improvement in validation loss, losing patience 50\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-27000.pth\n",
            "save decoder weights to  results/decoder-27000.pth\n",
            "Training loss: 32m 5s (- 53m 52s) (28000 37%) 2.1221\n",
            "Validation loss: 32m 28s (- 54m 31s) (28000 37%) 3.1625\n",
            "Bleu scores: 32m 28s (- 54m 31s) (28000 37%) 0.37972595\n",
            "Improvement in validation loss, saving model. Prev 3.2143399547549825 Curr 3.1624739661499373\n",
            "save encoder weights to  results/best_encoder.pth\n",
            "save decoder weights to  results/best_decoder.pth\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-28000.pth\n",
            "save decoder weights to  results/decoder-28000.pth\n",
            "Training loss: 33m 15s (- 52m 45s) (29000 38%) 1.9826\n",
            "Validation loss: 33m 38s (- 53m 22s) (29000 38%) 3.1830\n",
            "Bleu scores: 33m 38s (- 53m 22s) (29000 38%) 0.38157939\n",
            "No improvement in validation loss, losing patience 50\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-29000.pth\n",
            "save decoder weights to  results/decoder-29000.pth\n",
            "Training loss: 34m 26s (- 51m 40s) (30000 40%) 1.9977\n",
            "Validation loss: 34m 50s (- 52m 15s) (30000 40%) 3.2489\n",
            "Bleu scores: 34m 50s (- 52m 15s) (30000 40%) 0.37720139\n",
            "No improvement in validation loss, losing patience 49\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-30000.pth\n",
            "save decoder weights to  results/decoder-30000.pth\n",
            "Training loss: 35m 39s (- 50m 36s) (31000 41%) 1.9823\n",
            "Validation loss: 36m 2s (- 51m 9s) (31000 41%) 3.1802\n",
            "Bleu scores: 36m 2s (- 51m 9s) (31000 41%) 0.38069411\n",
            "No improvement in validation loss, losing patience 48\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-31000.pth\n",
            "save decoder weights to  results/decoder-31000.pth\n",
            "Training loss: 36m 50s (- 49m 30s) (32000 42%) 1.9418\n",
            "Validation loss: 37m 14s (- 50m 2s) (32000 42%) 3.1659\n",
            "Bleu scores: 37m 14s (- 50m 2s) (32000 42%) 0.38500078\n",
            "No improvement in validation loss, losing patience 47\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-32000.pth\n",
            "save decoder weights to  results/decoder-32000.pth\n",
            "Training loss: 38m 1s (- 48m 24s) (33000 44%) 1.8054\n",
            "Validation loss: 38m 25s (- 48m 54s) (33000 44%) 3.2244\n",
            "Bleu scores: 38m 25s (- 48m 54s) (33000 44%) 0.37845507\n",
            "No improvement in validation loss, losing patience 46\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-33000.pth\n",
            "save decoder weights to  results/decoder-33000.pth\n",
            "Training loss: 39m 13s (- 47m 18s) (34000 45%) 1.8143\n",
            "Validation loss: 39m 37s (- 47m 47s) (34000 45%) 3.1608\n",
            "Bleu scores: 39m 37s (- 47m 47s) (34000 45%) 0.38129520\n",
            "Improvement in validation loss, saving model. Prev 3.2244439560356235 Curr 3.160776107658083\n",
            "save encoder weights to  results/best_encoder.pth\n",
            "save decoder weights to  results/best_decoder.pth\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-34000.pth\n",
            "save decoder weights to  results/decoder-34000.pth\n",
            "Training loss: 40m 26s (- 46m 12s) (35000 46%) 1.8145\n",
            "Validation loss: 40m 49s (- 46m 39s) (35000 46%) 3.1561\n",
            "Bleu scores: 40m 49s (- 46m 39s) (35000 46%) 0.37979978\n",
            "Improvement in validation loss, saving model. Prev 3.160776107658083 Curr 3.1560605627941754\n",
            "save encoder weights to  results/best_encoder.pth\n",
            "save decoder weights to  results/best_decoder.pth\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-35000.pth\n",
            "save decoder weights to  results/decoder-35000.pth\n",
            "Training loss: 41m 38s (- 45m 6s) (36000 48%) 1.7176\n",
            "Validation loss: 42m 2s (- 45m 32s) (36000 48%) 3.1710\n",
            "Bleu scores: 42m 2s (- 45m 32s) (36000 48%) 0.37647424\n",
            "No improvement in validation loss, losing patience 50\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-36000.pth\n",
            "save decoder weights to  results/decoder-36000.pth\n",
            "Training loss: 42m 49s (- 43m 58s) (37000 49%) 1.6736\n",
            "Validation loss: 43m 13s (- 44m 23s) (37000 49%) 3.1786\n",
            "Bleu scores: 43m 13s (- 44m 23s) (37000 49%) 0.37998460\n",
            "No improvement in validation loss, losing patience 49\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-37000.pth\n",
            "save decoder weights to  results/decoder-37000.pth\n",
            "Training loss: 43m 59s (- 42m 50s) (38000 50%) 1.6380\n",
            "Validation loss: 44m 23s (- 43m 13s) (38000 50%) 3.2093\n",
            "Bleu scores: 44m 23s (- 43m 13s) (38000 50%) 0.37640189\n",
            "No improvement in validation loss, losing patience 48\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-38000.pth\n",
            "save decoder weights to  results/decoder-38000.pth\n",
            "Training loss: 45m 12s (- 41m 44s) (39000 52%) 1.6558\n",
            "Validation loss: 45m 36s (- 42m 6s) (39000 52%) 3.2308\n",
            "Bleu scores: 45m 36s (- 42m 6s) (39000 52%) 0.37316641\n",
            "No improvement in validation loss, losing patience 47\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-39000.pth\n",
            "save decoder weights to  results/decoder-39000.pth\n",
            "Training loss: 46m 26s (- 40m 37s) (40000 53%) 1.5729\n",
            "Validation loss: 46m 50s (- 40m 58s) (40000 53%) 3.2339\n",
            "Bleu scores: 46m 50s (- 40m 58s) (40000 53%) 0.37275724\n",
            "No improvement in validation loss, losing patience 46\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-40000.pth\n",
            "save decoder weights to  results/decoder-40000.pth\n",
            "Training loss: 47m 38s (- 39m 30s) (41000 54%) 1.4973\n",
            "Validation loss: 48m 2s (- 39m 50s) (41000 54%) 3.1586\n",
            "Bleu scores: 48m 2s (- 39m 50s) (41000 54%) 0.37820836\n",
            "No improvement in validation loss, losing patience 45\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-41000.pth\n",
            "save decoder weights to  results/decoder-41000.pth\n",
            "Training loss: 48m 50s (- 38m 22s) (42000 56%) 1.5279\n",
            "Validation loss: 49m 13s (- 38m 40s) (42000 56%) 3.1276\n",
            "Bleu scores: 49m 13s (- 38m 40s) (42000 56%) 0.37559863\n",
            "Improvement in validation loss, saving model. Prev 3.158633600545272 Curr 3.1275627597719735\n",
            "save encoder weights to  results/best_encoder.pth\n",
            "save decoder weights to  results/best_decoder.pth\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-42000.pth\n",
            "save decoder weights to  results/decoder-42000.pth\n",
            "Training loss: 50m 1s (- 37m 13s) (43000 57%) 1.4735\n",
            "Validation loss: 50m 25s (- 37m 31s) (43000 57%) 3.1989\n",
            "Bleu scores: 50m 25s (- 37m 31s) (43000 57%) 0.37259627\n",
            "No improvement in validation loss, losing patience 50\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-43000.pth\n",
            "save decoder weights to  results/decoder-43000.pth\n",
            "Training loss: 51m 13s (- 36m 5s) (44000 58%) 1.4561\n",
            "Validation loss: 51m 37s (- 36m 22s) (44000 58%) 3.1233\n",
            "Bleu scores: 51m 37s (- 36m 22s) (44000 58%) 0.37862804\n",
            "Improvement in validation loss, saving model. Prev 3.1988735343856427 Curr 3.123297779260481\n",
            "save encoder weights to  results/best_encoder.pth\n",
            "save decoder weights to  results/best_decoder.pth\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-44000.pth\n",
            "save decoder weights to  results/decoder-44000.pth\n",
            "Training loss: 52m 25s (- 34m 56s) (45000 60%) 1.4165\n",
            "Validation loss: 52m 48s (- 35m 12s) (45000 60%) 3.1537\n",
            "Bleu scores: 52m 48s (- 35m 12s) (45000 60%) 0.37506346\n",
            "No improvement in validation loss, losing patience 50\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-45000.pth\n",
            "save decoder weights to  results/decoder-45000.pth\n",
            "Training loss: 53m 37s (- 33m 48s) (46000 61%) 1.3815\n",
            "Validation loss: 54m 0s (- 34m 3s) (46000 61%) 3.1517\n",
            "Bleu scores: 54m 0s (- 34m 3s) (46000 61%) 0.37377934\n",
            "No improvement in validation loss, losing patience 49\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-46000.pth\n",
            "save decoder weights to  results/decoder-46000.pth\n",
            "Training loss: 54m 48s (- 32m 38s) (47000 62%) 1.3734\n",
            "Validation loss: 55m 11s (- 32m 53s) (47000 62%) 3.1788\n",
            "Bleu scores: 55m 11s (- 32m 53s) (47000 62%) 0.37176373\n",
            "No improvement in validation loss, losing patience 48\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-47000.pth\n",
            "save decoder weights to  results/decoder-47000.pth\n",
            "Training loss: 55m 59s (- 31m 29s) (48000 64%) 1.2837\n",
            "Validation loss: 56m 22s (- 31m 42s) (48000 64%) 3.1531\n",
            "Bleu scores: 56m 22s (- 31m 42s) (48000 64%) 0.37451000\n",
            "No improvement in validation loss, losing patience 47\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-48000.pth\n",
            "save decoder weights to  results/decoder-48000.pth\n",
            "Training loss: 57m 9s (- 30m 19s) (49000 65%) 1.2997\n",
            "Validation loss: 57m 33s (- 30m 32s) (49000 65%) 3.1745\n",
            "Bleu scores: 57m 33s (- 30m 32s) (49000 65%) 0.37373546\n",
            "No improvement in validation loss, losing patience 46\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-49000.pth\n",
            "save decoder weights to  results/decoder-49000.pth\n",
            "Training loss: 58m 20s (- 29m 10s) (50000 66%) 1.2364\n",
            "Validation loss: 58m 44s (- 29m 22s) (50000 66%) 3.2022\n",
            "Bleu scores: 58m 44s (- 29m 22s) (50000 66%) 0.37152789\n",
            "No improvement in validation loss, losing patience 45\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-50000.pth\n",
            "save decoder weights to  results/decoder-50000.pth\n",
            "Training loss: 59m 31s (- 28m 0s) (51000 68%) 1.1858\n",
            "Validation loss: 59m 55s (- 28m 11s) (51000 68%) 3.1831\n",
            "Bleu scores: 59m 55s (- 28m 11s) (51000 68%) 0.37368991\n",
            "No improvement in validation loss, losing patience 44\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-51000.pth\n",
            "save decoder weights to  results/decoder-51000.pth\n",
            "Training loss: 60m 44s (- 26m 52s) (52000 69%) 1.2085\n",
            "Validation loss: 61m 8s (- 27m 2s) (52000 69%) 3.2204\n",
            "Bleu scores: 61m 8s (- 27m 2s) (52000 69%) 0.36854131\n",
            "No improvement in validation loss, losing patience 43\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-52000.pth\n",
            "save decoder weights to  results/decoder-52000.pth\n",
            "Training loss: 61m 57s (- 25m 43s) (53000 70%) 1.1160\n",
            "Validation loss: 62m 21s (- 25m 53s) (53000 70%) 3.1548\n",
            "Bleu scores: 62m 21s (- 25m 53s) (53000 70%) 0.37364422\n",
            "No improvement in validation loss, losing patience 42\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-53000.pth\n",
            "save decoder weights to  results/decoder-53000.pth\n",
            "Training loss: 63m 10s (- 24m 33s) (54000 72%) 1.1322\n",
            "Validation loss: 63m 34s (- 24m 43s) (54000 72%) 3.2562\n",
            "Bleu scores: 63m 34s (- 24m 43s) (54000 72%) 0.37330062\n",
            "No improvement in validation loss, losing patience 41\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-54000.pth\n",
            "save decoder weights to  results/decoder-54000.pth\n",
            "Training loss: 64m 22s (- 23m 24s) (55000 73%) 1.0742\n",
            "Validation loss: 64m 45s (- 23m 33s) (55000 73%) 3.1544\n",
            "Bleu scores: 64m 45s (- 23m 33s) (55000 73%) 0.37336127\n",
            "No improvement in validation loss, losing patience 40\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-55000.pth\n",
            "save decoder weights to  results/decoder-55000.pth\n",
            "Training loss: 65m 34s (- 22m 14s) (56000 74%) 1.0987\n",
            "Validation loss: 65m 57s (- 22m 22s) (56000 74%) 3.1936\n",
            "Bleu scores: 65m 57s (- 22m 22s) (56000 74%) 0.37266986\n",
            "No improvement in validation loss, losing patience 39\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-56000.pth\n",
            "save decoder weights to  results/decoder-56000.pth\n",
            "Training loss: 66m 47s (- 21m 5s) (57000 76%) 1.0500\n",
            "Validation loss: 67m 11s (- 21m 13s) (57000 76%) 3.2157\n",
            "Bleu scores: 67m 11s (- 21m 13s) (57000 76%) 0.37524420\n",
            "No improvement in validation loss, losing patience 38\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-57000.pth\n",
            "save decoder weights to  results/decoder-57000.pth\n",
            "Training loss: 68m 0s (- 19m 56s) (58000 77%) 1.0554\n",
            "Validation loss: 68m 24s (- 20m 2s) (58000 77%) 3.2081\n",
            "Bleu scores: 68m 24s (- 20m 2s) (58000 77%) 0.37263901\n",
            "No improvement in validation loss, losing patience 37\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-58000.pth\n",
            "save decoder weights to  results/decoder-58000.pth\n",
            "Training loss: 69m 12s (- 18m 46s) (59000 78%) 1.0238\n",
            "Validation loss: 69m 36s (- 18m 52s) (59000 78%) 3.1995\n",
            "Bleu scores: 69m 36s (- 18m 52s) (59000 78%) 0.37233874\n",
            "No improvement in validation loss, losing patience 36\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-59000.pth\n",
            "save decoder weights to  results/decoder-59000.pth\n",
            "Training loss: 70m 24s (- 17m 36s) (60000 80%) 1.0186\n",
            "Validation loss: 70m 48s (- 17m 42s) (60000 80%) 3.2071\n",
            "Bleu scores: 70m 48s (- 17m 42s) (60000 80%) 0.36848254\n",
            "No improvement in validation loss, losing patience 35\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-60000.pth\n",
            "save decoder weights to  results/decoder-60000.pth\n",
            "Training loss: 71m 36s (- 16m 26s) (61000 81%) 0.9793\n",
            "Validation loss: 72m 0s (- 16m 31s) (61000 81%) 3.2699\n",
            "Bleu scores: 72m 0s (- 16m 31s) (61000 81%) 0.36753521\n",
            "No improvement in validation loss, losing patience 34\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-61000.pth\n",
            "save decoder weights to  results/decoder-61000.pth\n",
            "Training loss: 72m 48s (- 15m 16s) (62000 82%) 0.9839\n",
            "Validation loss: 73m 12s (- 15m 20s) (62000 82%) 3.2022\n",
            "Bleu scores: 73m 12s (- 15m 20s) (62000 82%) 0.37011460\n",
            "No improvement in validation loss, losing patience 33\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-62000.pth\n",
            "save decoder weights to  results/decoder-62000.pth\n",
            "Training loss: 74m 1s (- 14m 6s) (63000 84%) 0.9008\n",
            "Validation loss: 74m 25s (- 14m 10s) (63000 84%) 3.1910\n",
            "Bleu scores: 74m 25s (- 14m 10s) (63000 84%) 0.36731743\n",
            "No improvement in validation loss, losing patience 32\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-63000.pth\n",
            "save decoder weights to  results/decoder-63000.pth\n",
            "Training loss: 75m 12s (- 12m 55s) (64000 85%) 0.9237\n",
            "Validation loss: 75m 36s (- 12m 59s) (64000 85%) 3.2572\n",
            "Bleu scores: 75m 36s (- 12m 59s) (64000 85%) 0.36820093\n",
            "No improvement in validation loss, losing patience 31\n",
            "##########################################################\n",
            "save encoder weights to  results/encoder-64000.pth\n",
            "save decoder weights to  results/decoder-64000.pth\n",
            "Training loss: 76m 24s (- 11m 45s) (65000 86%) 0.9127\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Uc9fOh1SWI0i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Check some translations"
      ]
    },
    {
      "metadata": {
        "id": "05i43EgzWI0j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def translate(input_sentence):\n",
        "#     output_words, attentions = evaluate(\n",
        "#         encoder1, attn_decoder1, input_sentence)\n",
        "#     print('input =', input_sentence)\n",
        "#     print('output =', ' '.join(output_words))\n",
        "\n",
        "# translate(\"tom is playing with ball.\")\n",
        "\n",
        "# translate(\"she is standing there .\")\n",
        "\n",
        "# translate(\"he is a bad man .\")\n",
        "\n",
        "# translate(\"he wants to sleep .\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ruGN6hXufBC5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the best model \n",
        "#hidden_size = 1024\n",
        "#encoder1 = EncoderRNN(len(english_vocab), hidden_size).to(device)\n",
        "#attn_decoder1 = AttnDecoderRNN(hidden_size, len(indo_vocab), dropout_p=0.1).to(device)\n",
        "#encoder1.load_state_dict(torch.load(\"./results/encoder-1800.pth\"))\n",
        "#encoder1.eval()\n",
        "\n",
        "#attn_decoder1.load_state_dict(torch.load(\"./results/decoder-1800.pth\"))\n",
        "#attn_decoder1.eval()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Da2782FxWI0l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}