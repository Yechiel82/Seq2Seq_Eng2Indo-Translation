{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English to Bahasa Indonesian Seq-2-Seq Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython candies...\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "# Imports we need.\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Munging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk import word_tokenize\n",
    "\n",
    "fp = open('./corpus/bbc-468.en', 'r')\n",
    "eng_text = fp.read()\n",
    "eng_text = eng_text.splitlines()\n",
    "fp.close()\n",
    "\n",
    "fp2 = open('./corpus/bbc-468.id', 'r')\n",
    "id_text = fp2.read()\n",
    "id_text = id_text.splitlines()\n",
    "fp2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_text = pd.DataFrame(eng_text)\n",
    "df_eng_text = df_eng_text.rename(columns={0:'English'})\n",
    "\n",
    "df_id_text = pd.DataFrame(id_text)\n",
    "df_id_text = df_id_text.rename(columns={0:'Indonesian'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_text['English'] = df_eng_text['English'].apply(lambda x : x.lstrip())\n",
    "df_id_text['Indonesian'] = df_id_text['Indonesian'].apply(lambda x : x.lstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Indonesian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>French Muslims fined for face veils</td>\n",
       "      <td>Muslimah Prancis didenda karena mengenakan burka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Two French Muslim women have become the first ...</td>\n",
       "      <td>Dua Muslimah Prancis menjadi orang-orang perta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hind Ahmas and Najaite Ali were each ordered t...</td>\n",
       "      <td>Hind Ahmas dan Najaite Ali diperintahkan memba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Both say they'll appeal as far as the European...</td>\n",
       "      <td>Keduanya menyatakan akan mengajukan banding hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Some Muslim groups say veiled women have been ...</td>\n",
       "      <td>Sejumlah organisasi Muslim mengatakan wanita-w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "0                French Muslims fined for face veils   \n",
       "1  Two French Muslim women have become the first ...   \n",
       "2  Hind Ahmas and Najaite Ali were each ordered t...   \n",
       "3  Both say they'll appeal as far as the European...   \n",
       "4  Some Muslim groups say veiled women have been ...   \n",
       "\n",
       "                                          Indonesian  \n",
       "0   Muslimah Prancis didenda karena mengenakan burka  \n",
       "1  Dua Muslimah Prancis menjadi orang-orang perta...  \n",
       "2  Hind Ahmas dan Najaite Ali diperintahkan memba...  \n",
       "3  Keduanya menyatakan akan mengajukan banding hi...  \n",
       "4  Sejumlah organisasi Muslim mengatakan wanita-w...  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df_eng_text, df_id_text], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Singlish sentence: ['<s>', 'french', 'muslims', 'fined', 'for', 'face', 'veils', '</s>']\n",
      "First English sentence: ['<s>', 'muslimah', 'prancis', 'didenda', 'karena', 'mengenakan', 'burka', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# Use a unique string to indicate START and END of a sentence.\n",
    "# Assign a unique index to them.\n",
    "START, START_IDX = '<s>',  0\n",
    "END, END_IDX = '</s>', 1\n",
    "\n",
    "# We use this idiom to tokenize our sentences in the dataframe column:\n",
    "# >>> DataFrame['column'].apply(str.lower).apply(word_tokenize)\n",
    "\n",
    "# Also we added the START and the END symbol to the sentences. \n",
    "english_sents = [START] + df_eng_text['English'].apply(str.lower).apply(word_tokenize) + [END]\n",
    "indo_sents = [START] + df_id_text['Indonesian'].apply(str.lower).apply(word_tokenize) + [END]\n",
    "\n",
    "# We're sort of getting into the data into the shape we want. \n",
    "# But now it's still too humanly readable and redundant.\n",
    "## Cut-away: Computers like it to be simpler, more concise. -_-|||\n",
    "print('First Singlish sentence:', english_sents[0])\n",
    "print('First English sentence:', indo_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 Indonesian words in Dictionary:\n",
      " [(0, '<s>'), (1, '</s>'), (2, 'burka'), (3, 'didenda'), (4, 'karena'), (5, 'mengenakan'), (6, 'muslimah'), (7, 'prancis'), (8, ','), (9, '.')]\n",
      "\n",
      "First 10 English words in Dictionary:\n",
      " [(0, '<s>'), (1, '</s>'), (2, 'face'), (3, 'fined'), (4, 'for'), (5, 'french'), (6, 'muslims'), (7, 'veils'), (8, ','), (9, '.')]\n"
     ]
    }
   ],
   "source": [
    "english_vocab = Dictionary([['<s>'], ['</s>']])\n",
    "english_vocab.add_documents(english_sents)\n",
    "\n",
    "indo_vocab = Dictionary([['<s>'], ['</s>']])\n",
    "indo_vocab.add_documents(indo_sents)\n",
    "\n",
    "# First ten words in the vocabulary.\n",
    "print('First 10 Indonesian words in Dictionary:\\n', sorted(indo_vocab.items())[:10])\n",
    "print()\n",
    "print('First 10 English words in Dictionary:\\n', sorted(english_vocab.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Lets save our dictionaries.\n",
    "with open('indo_vocab.Dictionary.pkl', 'wb') as fout:\n",
    "    pickle.dump(indo_vocab, fout)\n",
    "    \n",
    "with open('english_vocab.Dictionary.pkl', 'wb') as fout:\n",
    "    pickle.dump(english_vocab, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [5],\n",
       "        [6],\n",
       "        [3],\n",
       "        [4],\n",
       "        [2],\n",
       "        [7],\n",
       "        [1]], device='cuda:0')"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorizes a sentence with a given vocab\n",
    "def vectorize_sent(sent, vocab):\n",
    "    return vocab.doc2idx([START] + word_tokenize(sent.lower()) + [END])\n",
    "\n",
    "# Creates a PyTorch variable from a sentence against a given vocab\n",
    "def variable_from_sent(sent, vocab):\n",
    "    vsent = vectorize_sent(sent, vocab)\n",
    "    #print(vsent)\n",
    "    result = Variable(torch.LongTensor(vsent).view(-1, 1))\n",
    "    #print(result)\n",
    "    return result.cuda() if use_cuda else result\n",
    "\n",
    "# Test\n",
    "new_kopi = \"French Muslims fined for face veils\"\n",
    "variable_from_sent(new_kopi, english_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muslimah Prancis didenda karena mengenakan burka\n",
      "tensor([[0],\n",
      "        [6],\n",
      "        [7],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [2],\n",
      "        [1]], device='cuda:0')\n",
      "French Muslims fined for face veils\n",
      "tensor([[0],\n",
      "        [5],\n",
      "        [6],\n",
      "        [3],\n",
      "        [4],\n",
      "        [2],\n",
      "        [7],\n",
      "        [1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Prepare the whole training corpus.\n",
    "indo_tensors = df['Indonesian'].apply(lambda s: variable_from_sent(s, indo_vocab))\n",
    "print(df.iloc[0]['Indonesian'])\n",
    "print(indo_tensors[0])\n",
    "english_tensors = df['English'].apply(lambda s: variable_from_sent(s, english_vocab))\n",
    "print(df.iloc[0]['English'])\n",
    "print(english_tensors[0])\n",
    "# Now, each item in `sent_pairs` is our data point. \n",
    "sent_pairs = list(zip(english_tensors, indo_tensors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        # Set the no. of nodes for the hidden layer.\n",
    "        self.hidden_size = hidden_size\n",
    "        # Initialize the embedding layer with the \n",
    "        # - size of input (i.e. no. of words in input vocab)\n",
    "        # - no. of hidden nodes in the embedding layer\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # Initialize the GRU with the \n",
    "        # - size of the hidden layer from the previous state\n",
    "        # - size of the hidden layer from the current state\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # Feed the input into the embedding layer.\n",
    "        embedded = self.embedding(input).view(1, 1, -1) # Question: why (1, 1, -1)\n",
    "        # Feed the embedded layer with the hidden layer to the GRU.\n",
    "        # Update the output and hidden layer.\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initialize_hidden_states(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        return result.cuda() if use_cuda else result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # Set the no. of nodes for the hidden layer.\n",
    "        self.hidden_size = hidden_size\n",
    "        # Initialize the embedding layer with the \n",
    "        # - size of output (i.e. no. of words in output vocab)\n",
    "        # - no. of hidden nodes in the embedding layer\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        # Initialize the GRU with the \n",
    "        # - size of the hidden layer from the previous state\n",
    "        # - size of the hidden layer from the current state\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        # Set the output layer to output a specific symbol \n",
    "        # from the output vocabulary\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # Feed the input into the embedding layer.\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        # Transform the embedded output with a relu function. \n",
    "        output = F.relu(output)\n",
    "        # Feed the embedded layer with the hidden layer to the GRU.\n",
    "        # Update the output and hidden layer.\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        # Take the updated output and find the most appropriate\n",
    "        # output symbol. \n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initialize_hidden_states(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        return result.cuda() if use_cuda else result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 10\n",
    "learning_rate=0.01\n",
    "batch_size = 2\n",
    "epochs = 30 # Since we are taking batch_size=2 and epochs=30, we only look at 60 data points.\n",
    "criterion = nn.NLLLoss()\n",
    "MAX_LENGTH=80\n",
    "\n",
    "# Initialize the network for encoder and decoder.\n",
    "input_vocab, output_vocab = english_vocab, indo_vocab\n",
    "encoder = EncoderRNN(len(input_vocab), hidden_size)\n",
    "decoder = DecoderRNN(hidden_size, len(output_vocab))\n",
    "if use_cuda:\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "\n",
    "# Initialize the optimizer for encoder and decoder.\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "# If batchsize == 1, choose 1 data points per batch:\n",
    "##training_data = [[random.choice(sent_pairs)] for i in range(epochs)]\n",
    "\n",
    "# If batch_size > 1, use random.sample() instead of random.choice:\n",
    "training_data = [random.sample(sent_pairs, batch_size) for i in range(epochs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 2.3.2. Loop through the batches.\n",
    "#############################################\n",
    "# Start the training.\n",
    "for data_batch in training_data:\n",
    "    # (Re-)Initialize the optimizers, clear all gradients after every batch.\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    # Reset the loss for every batch.\n",
    "    loss = 0\n",
    "    for input_variable, target_variable in data_batch:\n",
    "        # Initialize the hidden_states for the encoder.\n",
    "        encoder_hidden = encoder.initialize_hidden_states()\n",
    "        # Initialize the length of the PyTorch variables.\n",
    "        input_length = input_variable.size()[0]\n",
    "        target_length = target_variable.size()[0]\n",
    "        encoder_outputs = Variable(torch.zeros(MAX_LENGTH, encoder.hidden_size))\n",
    "        encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input variable first: tensor([[   0],\n",
      "        [1446],\n",
      "        [1447],\n",
      "        [1448],\n",
      "        [  23],\n",
      "        [1445],\n",
      "        [   1]], device='cuda:0')\n",
      "Target variable first: tensor([[   0],\n",
      "        [1388],\n",
      "        [  39],\n",
      "        [1387],\n",
      "        [ 502],\n",
      "        [ 970],\n",
      "        [ 682],\n",
      "        [   1]], device='cuda:0')\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "print(\"Input variable first: {0}\".format(data_batch[0][0]))\n",
    "print(\"Target variable first: {0}\".format(data_batch[0][1]))\n",
    "print(input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 2.3.2. Loop through the batches.\n",
    "#############################################\n",
    "# Start the training.\n",
    "for data_batch in training_data:\n",
    "    # (Re-)Initialize the optimizers, clear all gradients after every batch.\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    # Reset the loss for every batch.\n",
    "    loss = 0\n",
    "    for input_variable, target_variable in data_batch:\n",
    "        # Initialize the hidden_states for the encoder.\n",
    "        encoder_hidden = encoder.initialize_hidden_states()\n",
    "        # Initialize the length of the PyTorch variables.\n",
    "        input_length = input_variable.size()[0]\n",
    "        target_length = target_variable.size()[0]\n",
    "        encoder_outputs = Variable(torch.zeros(MAX_LENGTH, encoder.hidden_size))\n",
    "        encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "        #############################################\n",
    "        # 2.3.3.  Iterating through each word in the encoder.\n",
    "        #############################################\n",
    "        # Iterating through each word in the input.\n",
    "        for ei in range(input_length):\n",
    "            # We move forward through each state.\n",
    "            encoder_output, encoder_hidden = encoder(input_variable[ei], encoder_hidden)\n",
    "            # And we save the encoder outputs. \n",
    "            # Note: We're retrieving [0][0] cos remember the weird .view(1,1,-1) for the GRU.\n",
    "            encoder_outputs[ei] = encoder_output[0][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN(\n",
      "  (embedding): Embedding(2562, 10)\n",
      "  (gru): GRU(10, 10)\n",
      ") \n",
      "\n",
      "Dictionary(2562 unique tokens: ['<s>', '</s>', 'face', 'fined', 'for']...)\n",
      "\n",
      "########\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The encoded output for the last sentence in out training_data\"\n",
    "\n",
    "# The encoder has 68 unique words\n",
    "print(encoder, '\\n')\n",
    "print(english_vocab)\n",
    "print('\\n########\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0],\n",
      "        [ 486],\n",
      "        [2067],\n",
      "        [  10],\n",
      "        [1003],\n",
      "        [  31],\n",
      "        [ 482],\n",
      "        [2066],\n",
      "        [  29],\n",
      "        [1304],\n",
      "        [ 167],\n",
      "        [2065],\n",
      "        [  46],\n",
      "        [ 486],\n",
      "        [ 152],\n",
      "        [2062],\n",
      "        [  34],\n",
      "        [  10],\n",
      "        [2063],\n",
      "        [2064],\n",
      "        [   9],\n",
      "        [   1]], device='cuda:0')\n",
      "########\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The last input sentence, in PyTorch Tensor data structure.\n",
    "print(data_batch[-1][0]) \n",
    "print('########\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 486, 2067, 10, 1003, 31, 482, 2066, 29, 1304, 167, 2065, 46, 486, 152, 2062, 34, 10, 2063, 2064, 9, 1] \n",
      "\n",
      "########\n",
      "\n",
      "<s> she wrote a book about her struggle with cancer including photos as she had chemotherapy and a double mastectomy . </s>\n",
      "\n",
      "########\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The last input sentence as list(int)\n",
    "print(list(map(int, data_batch[-1][0])), '\\n')\n",
    "print('########\\n')\n",
    "\n",
    "# The last input sentence as list(int)\n",
    "print(' '.join([english_vocab[i] for i in map(int, data_batch[-1][0])]))\n",
    "print('\\n########\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.1068e-01,  2.7053e-01, -1.4240e-01,  1.2200e-01,  2.7938e-01,\n",
      "         -1.5802e-01, -4.8984e-01,  3.2443e-01, -8.3683e-02,  1.0726e-01],\n",
      "        [-3.0635e-02, -4.7747e-01, -4.0604e-01, -1.2465e-01,  1.4906e-01,\n",
      "         -5.0660e-02,  2.1319e-01, -5.0425e-03,  2.6422e-01,  4.8974e-02],\n",
      "        [ 2.7771e-01, -1.8648e-01, -4.8895e-01, -1.3948e-02,  3.4161e-01,\n",
      "          1.2349e-01,  2.2669e-01, -1.0797e-01, -6.0926e-02,  1.0075e-01],\n",
      "        [ 2.4913e-01, -6.1907e-01, -1.7827e-01,  2.1556e-01, -2.4289e-01,\n",
      "         -2.4005e-01,  3.3462e-01, -7.6853e-02,  1.5469e-02,  1.2493e-01],\n",
      "        [ 2.9518e-01, -4.3141e-01, -9.3533e-02, -5.9908e-02, -1.1571e-01,\n",
      "          1.0901e-01,  5.1036e-01, -1.8484e-01, -3.8715e-01, -2.9339e-01],\n",
      "        [ 3.5782e-01, -3.5483e-01, -6.0122e-01, -1.2791e-01,  3.6083e-01,\n",
      "          2.3073e-01,  6.8259e-01,  2.2391e-01, -2.2050e-01, -2.1383e-01],\n",
      "        [ 2.9457e-01, -5.4439e-01, -4.3347e-01, -4.4074e-01,  1.0597e-01,\n",
      "          3.0176e-01, -2.5668e-01, -9.9883e-02, -1.4035e-01,  1.3208e-02],\n",
      "        [ 3.6338e-01, -5.1945e-01, -5.7920e-01, -3.3748e-01,  2.3680e-01,\n",
      "          4.8845e-01,  3.2425e-01,  1.3579e-01,  1.0623e-01, -3.9070e-01],\n",
      "        [-1.3367e-01, -4.3460e-01, -6.1300e-01, -3.3788e-01,  1.7496e-01,\n",
      "          5.7146e-01,  2.2277e-01, -1.5358e-01, -1.9091e-01, -1.4691e-01],\n",
      "        [-9.8978e-02, -3.2257e-01, -8.2181e-01, -1.8805e-01,  3.4886e-01,\n",
      "          5.4536e-01,  2.4169e-01, -9.9610e-02, -1.8310e-01,  4.4630e-02],\n",
      "        [ 3.2158e-01, -4.4029e-01, -7.0947e-01, -2.0764e-01,  3.2383e-01,\n",
      "          5.1959e-01, -8.8304e-02, -3.5988e-01, -3.2568e-01,  1.4428e-01],\n",
      "        [ 3.9223e-01, -4.6370e-01, -5.9616e-01,  6.2867e-02,  2.7870e-01,\n",
      "          3.2995e-01, -4.6683e-01, -4.0963e-01, -3.3901e-01,  5.7115e-02],\n",
      "        [-2.6689e-01, -6.2866e-01, -7.3593e-01, -3.8295e-02, -2.8140e-01,\n",
      "          3.8945e-01,  3.1605e-01, -3.3373e-01,  5.5288e-01, -3.5317e-01],\n",
      "        [-1.5365e-01, -7.4431e-01, -7.3518e-01, -2.5949e-01, -2.6983e-01,\n",
      "          4.8904e-01,  5.8764e-01, -3.5930e-01,  3.2855e-01, -1.8086e-01],\n",
      "        [ 4.4400e-02, -5.7729e-01, -6.1216e-01, -3.6335e-01, -6.2770e-02,\n",
      "          6.0955e-01,  5.8194e-01, -1.3248e-01,  9.5465e-02, -4.0489e-01],\n",
      "        [ 2.4729e-01, -2.1185e-01, -2.0440e-01, -1.5749e-01,  2.5830e-02,\n",
      "          5.1846e-01, -3.3600e-01,  4.1199e-02, -1.9060e-01, -3.2783e-01],\n",
      "        [ 3.2182e-01,  8.1939e-02, -6.2766e-01,  3.7241e-02,  4.5125e-01,\n",
      "          4.1266e-02, -3.5701e-01,  6.4106e-01, -1.2294e-01, -2.1159e-02],\n",
      "        [ 2.5051e-01, -5.9820e-01, -2.9216e-01,  2.7501e-01, -2.1797e-01,\n",
      "         -2.7838e-01,  1.8893e-01,  5.4831e-01, -5.1672e-02,  3.1215e-02],\n",
      "        [-9.0414e-02, -7.4074e-01, -5.0421e-01, -2.3318e-01, -2.0636e-01,\n",
      "          3.3120e-02,  2.5778e-01,  1.7679e-01, -1.5972e-02, -1.2673e-01],\n",
      "        [ 1.7880e-01, -5.4672e-01, -2.3987e-01,  1.6316e-01, -5.7491e-02,\n",
      "          1.7020e-01,  4.7993e-02,  6.2044e-02, -1.8853e-01, -2.8232e-01],\n",
      "        [ 1.4897e-02, -5.1526e-01, -7.9937e-01,  9.0763e-02,  3.2375e-01,\n",
      "          2.8700e-01, -6.2620e-02, -6.2279e-01, -7.0947e-02, -7.0856e-02],\n",
      "        [-2.1563e-01, -5.8413e-01, -7.0803e-01,  6.0540e-04,  1.0820e-01,\n",
      "          2.8321e-01,  1.6297e-01, -3.8594e-02,  9.0478e-03, -8.6194e-02],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "       device='cuda:0', grad_fn=<CopySlices>)\n",
      "tensor([[[-2.1563e-01, -5.8413e-01, -7.0803e-01,  6.0540e-04,  1.0820e-01,\n",
      "           2.8321e-01,  1.6297e-01, -3.8594e-02,  9.0478e-03, -8.6194e-02]]],\n",
      "       device='cuda:0', grad_fn=<CudnnRnnBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_outputs)\n",
    "# The last hidden state of the last input sentence. \n",
    "# Note: For vanilla RNN (Elman Net), the last hidden state of the encoder\n",
    "#       is the start state of the decoder's hidden state.\n",
    "print(encoder_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 2.3.2. Loop through the batches.\n",
    "#############################################\n",
    "# Start the training.\n",
    "for data_batch in training_data:\n",
    "    # (Re-)Initialize the optimizers, clear all gradients after every batch.\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    # Reset the loss for every batch.\n",
    "    loss = 0\n",
    "    for input_variable, target_variable in data_batch:\n",
    "        # Initialize the hidden_states for the encoder.\n",
    "        encoder_hidden = encoder.initialize_hidden_states()\n",
    "        # Initialize the length of the PyTorch variables.\n",
    "        input_length = input_variable.size()[0]\n",
    "        target_length = target_variable.size()[0]\n",
    "        encoder_outputs = Variable(torch.zeros(MAX_LENGTH, encoder.hidden_size))\n",
    "        encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "        \n",
    "        #############################################\n",
    "        # 2.3.3.  Iterating through each word in the encoder.\n",
    "        #############################################\n",
    "        # Iterating through each word in the input.\n",
    "        for ei in range(input_length):\n",
    "            # We move forward through each state.\n",
    "            encoder_output, encoder_hidden = encoder(input_variable[ei], encoder_hidden)\n",
    "            # And we save the encoder outputs. \n",
    "            # Note: We're retrieving [0][0] cos remember the weird .view(1,1,-1) -_-|||\n",
    "            encoder_outputs[ei] = encoder_output[0][0] \n",
    "            \n",
    "            #############################################\n",
    "            # 2.3.4.  Iterating through each word in the decoder.\n",
    "            #############################################\n",
    "            # Initialize the variable input with the index of the START.\n",
    "            decoder_input = Variable(torch.LongTensor([[START_IDX]]))\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            # As the first state of the decoder, we take the last step of the encoder.\n",
    "            decoder_hidden = encoder_hidden\n",
    "            # Iterate through each state in the decoder.\n",
    "            # Note: when we are training we know the length of the decoder.\n",
    "            #       so we can use the trick to restrict the loop when decoding.\n",
    "            for di in range(target_length):\n",
    "                # We move forward through each state.\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                # What are all these weird syntax, refer to 2.3.4.1\n",
    "                topv, topi = decoder_output.data.topk(1) # topk gives k largest values along a certain dimension, A tuple of (values, indices) is returned,\n",
    "                ni = topi[0][0]\n",
    "\n",
    "                # Replace our decoder input for the next state with the\n",
    "                # embedding of the decoded topi guess. \n",
    "                decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "                decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "                \n",
    "                # Update our loss for this batch.\n",
    "                loss += criterion(decoder_output, target_variable[di])\n",
    "                \n",
    "                # If we see the </s> symbol, break the training.\n",
    "                if ni == END_IDX:\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecoderRNN(\n",
      "  (embedding): Embedding(2342, 10)\n",
      "  (gru): GRU(10, 10)\n",
      "  (softmax): LogSoftmax()\n",
      "  (out): Linear(in_features=10, out_features=2342, bias=True)\n",
      ") \n",
      "\n",
      "Dictionary(2342 unique tokens: ['<s>', '</s>', 'burka', 'didenda', 'karena']...)\n",
      "\n",
      "########\n",
      "\n",
      "<s> she wrote a book about her struggle with cancer including photos as she had chemotherapy and a double mastectomy . </s>\n",
      "<s> ia menulis buku tentang perjuangannya melawan kanker termasuk foto-foto ketika ia menjalani kemoterapi dan dua kali operasi pengangkatan payudara . </s>\n",
      "\n",
      "########\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cut-away: The decoded output for the last sentence in out training_data\"\n",
    "\n",
    "# The encoder has 117 unique words\n",
    "print(decoder, '\\n')\n",
    "print(indo_vocab)\n",
    "print('\\n########\\n')\n",
    "\n",
    "# The last input sentence.\n",
    "print(' '.join([english_vocab[i] for i in map(int, data_batch[-1][0])]))\n",
    "# The last target sentence.\n",
    "print(' '.join([indo_vocab[i] for i in map(int, data_batch[-1][1])]))\n",
    "\n",
    "print('\\n########\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Training Starts Here - Backpropagation Portion Added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 2.3.2. Loop through the batches.\n",
    "#############################################\n",
    "# Start the training.\n",
    "for data_batch in training_data:\n",
    "    # (Re-)Initialize the optimizers, clear all gradients after every batch.\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    # Reset the loss for every batch.\n",
    "    loss = 0\n",
    "    for input_variable, target_variable in data_batch:\n",
    "        # Initialize the hidden_states for the encoder.\n",
    "        encoder_hidden = encoder.initialize_hidden_states()\n",
    "        # Initialize the length of the PyTorch variables.\n",
    "        input_length = input_variable.size()[0]\n",
    "        target_length = target_variable.size()[0]\n",
    "        encoder_outputs = Variable(torch.zeros(MAX_LENGTH, encoder.hidden_size))\n",
    "        encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "        \n",
    "        #############################################\n",
    "        # 2.3.3.  Iterating through each word in the encoder.\n",
    "        #############################################\n",
    "        # Iterating through each word in the input.\n",
    "        for ei in range(input_length):\n",
    "            # We move forward through each state.\n",
    "            encoder_output, encoder_hidden = encoder(input_variable[ei], encoder_hidden)\n",
    "            # And we save the encoder outputs. \n",
    "            # Note: We're retrieving [0][0] cos remember the weird .view(1,1,-1) -_-|||\n",
    "            encoder_outputs[ei] = encoder_output[0][0] \n",
    "            \n",
    "            #############################################\n",
    "            # 2.3.4.  Iterating through each word in the decoder.\n",
    "            #############################################\n",
    "            # Initialize the variable input with the index of the START.\n",
    "            decoder_input = Variable(torch.LongTensor([[START_IDX]]))\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            # As the first state of the decoder, we take the last step of the encoder.\n",
    "            decoder_hidden = encoder_hidden\n",
    "            # Iterate through each state in the decoder.\n",
    "            # Note: when we are training we know the length of the decoder.\n",
    "            #       so we can use the trick to restrict the loop when decoding.\n",
    "            for di in range(target_length):\n",
    "                # We move forward through each state.\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                # What are alll these weird syntax, refer to 2.3.4.1\n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                ni = topi[0][0]\n",
    "\n",
    "                # Replace our decoder input for the next state with the\n",
    "                # embedding of the decoded topi guess. \n",
    "                decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "                decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "                \n",
    "                # Update our loss for this batch.\n",
    "                loss += criterion(decoder_output, target_variable[di])\n",
    "                \n",
    "                # If we see the </s> symbol, break the training.\n",
    "                if ni == END_IDX:\n",
    "                    break\n",
    "    #####################################################\n",
    "    # 2.3.5 Backpropagate the Loss and Optimizers Takes a Step.\n",
    "    #####################################################\n",
    "    loss.backward() # Backpropagate.\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 10])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translator(encoder, decoder, input_variable, max_length=MAX_LENGTH):\n",
    "    # The length of the input.\n",
    "    input_length = input_variable.size()[0]\n",
    "    # For each sentence, initilize the hidden states with zeros.\n",
    "    encoder_hidden = encoder.initialize_hidden_states()\n",
    "    # Initialize the encoder outputs. \n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "    # Iterate through the input words.\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei], encoder_hidden)\n",
    "    # Initialize the decoder with the start symbol <s>.\n",
    "    decoder_input = Variable(torch.LongTensor([[START_IDX]])) \n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    # Use the last encoder hidden state as the first decoder's hidden state.\n",
    "    decoder_hidden = encoder_hidden\n",
    "    # Keep a list of the decoded words.\n",
    "    decoded_words = []\n",
    "    \n",
    "    # Iterate through the decoder states.\n",
    "    for di in range(max_length):\n",
    "        # Very similar to how the training works.\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, \n",
    "                                                 decoder_hidden)\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == END_IDX:\n",
    "            decoded_words.append(END_IDX)\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(ni.item())# changed from ni to ni.item()\n",
    "        # Replace the new decoder input for the next state \n",
    "        # with the top guess of this state.\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0],\n",
       "        [222],\n",
       "        [229],\n",
       "        [ 25],\n",
       "        [220],\n",
       "        [  1]], device='cuda:0')"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'caffeine protects the brain' # if words don't appear in corpus, it will be marked as -1\n",
    "variable_from_sent(sent, english_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 80\n",
    "batches = 100000 # In this case, the PyTorch train_per_epoch() and train() code is using batch_size=1\n",
    "hidden_size = 100\n",
    "\n",
    "my_encoder = EncoderRNN(len(english_vocab), hidden_size)\n",
    "my_decoder = DecoderRNN(hidden_size, len(indo_vocab))\n",
    "\n",
    "if use_cuda:\n",
    "    my_encoder = my_encoder.cuda()\n",
    "    my_decoder = my_decoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_words = translator(my_encoder, my_decoder, \n",
    "                          variable_from_sent(sent, english_vocab))\n",
    "len(output_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['narkotika',\n",
       " 'messi',\n",
       " 'mendatangkan',\n",
       " '$',\n",
       " 'saingan',\n",
       " 'radiasi',\n",
       " 'pesta',\n",
       " '1.600',\n",
       " 'terciptanya',\n",
       " 'batu',\n",
       " 'batu',\n",
       " 'batu',\n",
       " 'menggerebek',\n",
       " 'batu',\n",
       " 'batu',\n",
       " 'batu',\n",
       " 'menggerebek',\n",
       " 'batu',\n",
       " 'batu',\n",
       " 'batu',\n",
       " 'menggerebek',\n",
       " 'batu',\n",
       " 'batu',\n",
       " 'batu',\n",
       " 'menggerebek',\n",
       " 'batu',\n",
       " 'batu',\n",
       " 'batu',\n",
       " 'menggerebek']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[indo_vocab[i] for i in output_words[1:30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# Some Logging and Plotting Candies to Monitor Training\n",
    "#########################################################\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    \n",
    "    \n",
    "#########################################################\n",
    "# Training per epoch,\n",
    "# Iterates across data points per epoch.\n",
    "#########################################################\n",
    "def train_one_epoch(input_variable, target_variable, encoder, decoder, \n",
    "                    encoder_optimizer, decoder_optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Function to put the variables, decoder and optimizers to train per epoch.\n",
    "    \"\"\"\n",
    "    encoder_hidden = encoder.initialize_hidden_states()\n",
    "\n",
    "    # (Re-)Initialize the optimizers, clear all gradients. \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    # Initialize the length of the PyTorch variables.\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "    encoder_outputs = Variable(torch.zeros(MAX_LENGTH, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    loss = 0\n",
    "    \n",
    "    # Iterating through each word in the input.\n",
    "    for ei in range(input_length):\n",
    "        # We move forward through each state.\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei], encoder_hidden)\n",
    "        # And we save the encoder outputs. \n",
    "        encoder_outputs[ei] = encoder_output[0][0]\n",
    "\n",
    "    # Initialize the variable input with the index of the START.\n",
    "    decoder_input = Variable(torch.LongTensor([[START_IDX]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    \n",
    "    # As the first state of the decoder, we take the last step of the encoder.\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    # Without teacher forcing: use its own predictions as the next input\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "        loss += criterion(decoder_output, target_variable[di])\n",
    "        if ni == END_IDX:\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    #return loss.data[0] / target_length\n",
    "    return loss.item() / target_length\n",
    "\n",
    "#########################################################\n",
    "# Top-level function to start the training,\n",
    "# iterates across epochs.\n",
    "#########################################################\n",
    "\n",
    "def train(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    training_pairs = [random.choice(sent_pairs) for i in range(n_iters)]\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_variable = training_pair[0]\n",
    "        target_variable = training_pair[1]\n",
    "\n",
    "        loss = train_one_epoch(input_variable, target_variable, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    \n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 1s (- 31m 24s) (100 0%) 4.2219\n",
      "0m 3s (- 30m 37s) (200 0%) 2.9857\n",
      "0m 5s (- 31m 27s) (300 0%) 3.5959\n",
      "0m 7s (- 31m 54s) (400 0%) 3.4452\n",
      "0m 9s (- 30m 45s) (500 0%) 2.7909\n",
      "0m 10s (- 30m 0s) (600 0%) 2.5164\n",
      "0m 12s (- 30m 1s) (700 0%) 3.3509\n",
      "0m 14s (- 29m 58s) (800 0%) 3.5685\n",
      "0m 16s (- 30m 0s) (900 0%) 3.2444\n",
      "0m 18s (- 29m 45s) (1000 1%) 2.9294\n",
      "0m 19s (- 29m 38s) (1100 1%) 2.8382\n",
      "0m 21s (- 29m 33s) (1200 1%) 2.9714\n",
      "0m 23s (- 29m 22s) (1300 1%) 2.8675\n",
      "0m 24s (- 29m 5s) (1400 1%) 2.7678\n",
      "0m 26s (- 28m 59s) (1500 1%) 3.1358\n",
      "0m 28s (- 28m 55s) (1600 1%) 2.9745\n",
      "0m 29s (- 28m 54s) (1700 1%) 3.4735\n",
      "0m 31s (- 28m 58s) (1800 1%) 3.4964\n",
      "0m 33s (- 29m 1s) (1900 1%) 3.7655\n",
      "0m 35s (- 29m 6s) (2000 2%) 3.8135\n",
      "0m 37s (- 29m 15s) (2100 2%) 3.7885\n",
      "0m 39s (- 29m 21s) (2200 2%) 4.0260\n",
      "0m 41s (- 29m 25s) (2300 2%) 3.9779\n",
      "0m 43s (- 29m 32s) (2400 2%) 4.1159\n",
      "0m 45s (- 29m 29s) (2500 2%) 3.6573\n",
      "0m 47s (- 29m 24s) (2600 2%) 3.3350\n",
      "0m 49s (- 29m 30s) (2700 2%) 4.1997\n",
      "0m 51s (- 29m 39s) (2800 2%) 4.2380\n",
      "0m 53s (- 29m 42s) (2900 2%) 3.9997\n",
      "0m 55s (- 29m 48s) (3000 3%) 3.9203\n",
      "0m 57s (- 29m 56s) (3100 3%) 4.5022\n",
      "0m 59s (- 29m 59s) (3200 3%) 3.9578\n",
      "1m 1s (- 30m 1s) (3300 3%) 3.8921\n",
      "1m 3s (- 30m 2s) (3400 3%) 3.7047\n",
      "1m 5s (- 30m 2s) (3500 3%) 4.0706\n",
      "1m 7s (- 30m 8s) (3600 3%) 4.2376\n",
      "1m 9s (- 30m 11s) (3700 3%) 4.1566\n",
      "1m 11s (- 30m 11s) (3800 3%) 3.9092\n",
      "1m 13s (- 30m 14s) (3900 3%) 4.3054\n",
      "1m 15s (- 30m 14s) (4000 4%) 3.7422\n",
      "1m 18s (- 30m 24s) (4100 4%) 4.0721\n",
      "1m 20s (- 30m 28s) (4200 4%) 4.0437\n",
      "1m 22s (- 30m 26s) (4300 4%) 3.5919\n",
      "1m 24s (- 30m 28s) (4400 4%) 3.8709\n",
      "1m 26s (- 30m 33s) (4500 4%) 4.1290\n",
      "1m 28s (- 30m 37s) (4600 4%) 3.6834\n",
      "1m 30s (- 30m 35s) (4700 4%) 3.7152\n",
      "1m 32s (- 30m 40s) (4800 4%) 3.7092\n",
      "1m 34s (- 30m 43s) (4900 4%) 4.2575\n",
      "1m 37s (- 30m 44s) (5000 5%) 4.0524\n",
      "1m 39s (- 30m 43s) (5100 5%) 4.0144\n",
      "1m 41s (- 30m 46s) (5200 5%) 4.3553\n",
      "1m 43s (- 30m 50s) (5300 5%) 4.3115\n",
      "1m 45s (- 30m 56s) (5400 5%) 4.4587\n",
      "1m 48s (- 30m 55s) (5500 5%) 3.7895\n",
      "1m 50s (- 30m 55s) (5600 5%) 4.0341\n",
      "1m 52s (- 30m 56s) (5700 5%) 4.0792\n",
      "1m 54s (- 30m 56s) (5800 5%) 4.2334\n",
      "1m 56s (- 30m 58s) (5900 5%) 4.0319\n",
      "1m 58s (- 30m 59s) (6000 6%) 4.0189\n",
      "2m 1s (- 31m 2s) (6100 6%) 4.3611\n",
      "2m 3s (- 31m 3s) (6200 6%) 4.0540\n",
      "2m 5s (- 31m 6s) (6300 6%) 4.1065\n",
      "2m 7s (- 31m 7s) (6400 6%) 4.4097\n",
      "2m 10s (- 31m 10s) (6500 6%) 4.4711\n",
      "2m 12s (- 31m 13s) (6600 6%) 4.3367\n",
      "2m 14s (- 31m 14s) (6700 6%) 4.0026\n",
      "2m 16s (- 31m 14s) (6800 6%) 3.9229\n",
      "2m 19s (- 31m 16s) (6900 6%) 3.9404\n",
      "2m 21s (- 31m 15s) (7000 7%) 4.0157\n",
      "2m 23s (- 31m 14s) (7100 7%) 4.1901\n",
      "2m 25s (- 31m 14s) (7200 7%) 3.8616\n",
      "2m 27s (- 31m 13s) (7300 7%) 4.0002\n",
      "2m 29s (- 31m 12s) (7400 7%) 4.1405\n",
      "2m 31s (- 31m 11s) (7500 7%) 3.5705\n",
      "2m 33s (- 31m 11s) (7600 7%) 4.0206\n",
      "2m 36s (- 31m 10s) (7700 7%) 3.5870\n",
      "2m 38s (- 31m 11s) (7800 7%) 3.8719\n",
      "2m 40s (- 31m 12s) (7900 7%) 4.0334\n",
      "2m 42s (- 31m 13s) (8000 8%) 4.2326\n",
      "2m 45s (- 31m 13s) (8100 8%) 3.9695\n",
      "2m 47s (- 31m 13s) (8200 8%) 3.9179\n",
      "2m 49s (- 31m 12s) (8300 8%) 3.7529\n",
      "2m 51s (- 31m 11s) (8400 8%) 3.8991\n",
      "2m 53s (- 31m 11s) (8500 8%) 4.0914\n",
      "2m 56s (- 31m 11s) (8600 8%) 3.8344\n",
      "2m 58s (- 31m 11s) (8700 8%) 4.1796\n",
      "3m 0s (- 31m 8s) (8800 8%) 3.9292\n",
      "3m 2s (- 31m 9s) (8900 8%) 3.9782\n",
      "3m 4s (- 31m 8s) (9000 9%) 3.9787\n",
      "3m 7s (- 31m 8s) (9100 9%) 4.0094\n",
      "3m 9s (- 31m 6s) (9200 9%) 3.9073\n",
      "3m 11s (- 31m 7s) (9300 9%) 4.2565\n",
      "3m 13s (- 31m 5s) (9400 9%) 3.8253\n",
      "3m 15s (- 31m 4s) (9500 9%) 3.8101\n",
      "3m 17s (- 31m 2s) (9600 9%) 3.6424\n",
      "3m 20s (- 31m 2s) (9700 9%) 3.9234\n",
      "3m 22s (- 31m 2s) (9800 9%) 3.7830\n",
      "3m 24s (- 31m 3s) (9900 9%) 4.0347\n",
      "3m 26s (- 31m 2s) (10000 10%) 4.0347\n",
      "3m 28s (- 30m 59s) (10100 10%) 3.6509\n",
      "3m 31s (- 30m 59s) (10200 10%) 3.9058\n",
      "3m 33s (- 30m 59s) (10300 10%) 3.8840\n",
      "3m 35s (- 30m 59s) (10400 10%) 4.1014\n",
      "3m 38s (- 30m 58s) (10500 10%) 3.7346\n",
      "3m 40s (- 30m 59s) (10600 10%) 4.0692\n",
      "3m 42s (- 30m 59s) (10700 10%) 3.6044\n",
      "3m 45s (- 30m 58s) (10800 10%) 3.7185\n",
      "3m 47s (- 30m 58s) (10900 10%) 3.9623\n",
      "3m 49s (- 30m 59s) (11000 11%) 3.8580\n",
      "3m 51s (- 30m 57s) (11100 11%) 3.6536\n",
      "3m 54s (- 30m 57s) (11200 11%) 3.8570\n",
      "3m 56s (- 30m 56s) (11300 11%) 3.7152\n",
      "3m 58s (- 30m 55s) (11400 11%) 3.7638\n",
      "4m 1s (- 30m 55s) (11500 11%) 3.6751\n",
      "4m 3s (- 30m 55s) (11600 11%) 3.6007\n",
      "4m 5s (- 30m 54s) (11700 11%) 3.7621\n",
      "4m 7s (- 30m 52s) (11800 11%) 3.6213\n",
      "4m 10s (- 30m 51s) (11900 11%) 3.8557\n",
      "4m 12s (- 30m 50s) (12000 12%) 3.6827\n",
      "4m 14s (- 30m 48s) (12100 12%) 3.6380\n",
      "4m 16s (- 30m 48s) (12200 12%) 3.4871\n",
      "4m 19s (- 30m 47s) (12300 12%) 3.6474\n",
      "4m 21s (- 30m 45s) (12400 12%) 3.6010\n",
      "4m 23s (- 30m 45s) (12500 12%) 3.7395\n",
      "4m 26s (- 30m 45s) (12600 12%) 3.4503\n",
      "4m 28s (- 30m 44s) (12700 12%) 3.4585\n",
      "4m 30s (- 30m 43s) (12800 12%) 3.3201\n",
      "4m 32s (- 30m 42s) (12900 12%) 3.4385\n",
      "4m 34s (- 30m 39s) (13000 13%) 3.2202\n",
      "4m 37s (- 30m 39s) (13100 13%) 3.5221\n",
      "4m 39s (- 30m 37s) (13200 13%) 3.3432\n",
      "4m 41s (- 30m 35s) (13300 13%) 3.1995\n",
      "4m 43s (- 30m 34s) (13400 13%) 3.3245\n",
      "4m 46s (- 30m 33s) (13500 13%) 3.3605\n",
      "4m 48s (- 30m 34s) (13600 13%) 3.3484\n",
      "4m 51s (- 30m 36s) (13700 13%) 3.4789\n",
      "4m 54s (- 30m 36s) (13800 13%) 3.5654\n",
      "4m 56s (- 30m 37s) (13900 13%) 3.3031\n",
      "4m 59s (- 30m 39s) (14000 14%) 3.5293\n",
      "5m 1s (- 30m 37s) (14100 14%) 3.4453\n",
      "5m 4s (- 30m 37s) (14200 14%) 3.2744\n",
      "5m 6s (- 30m 37s) (14300 14%) 3.1747\n",
      "5m 9s (- 30m 36s) (14400 14%) 3.0901\n",
      "5m 11s (- 30m 37s) (14500 14%) 3.4466\n",
      "5m 13s (- 30m 36s) (14600 14%) 3.3136\n",
      "5m 16s (- 30m 36s) (14700 14%) 3.3955\n",
      "5m 19s (- 30m 37s) (14800 14%) 3.2458\n",
      "5m 21s (- 30m 36s) (14900 14%) 3.3151\n",
      "5m 23s (- 30m 35s) (15000 15%) 3.3200\n",
      "5m 26s (- 30m 35s) (15100 15%) 3.2950\n",
      "5m 29s (- 30m 35s) (15200 15%) 3.4640\n",
      "5m 31s (- 30m 35s) (15300 15%) 3.3433\n",
      "5m 33s (- 30m 33s) (15400 15%) 3.1564\n",
      "5m 36s (- 30m 32s) (15500 15%) 3.3427\n",
      "5m 38s (- 30m 31s) (15600 15%) 3.1346\n",
      "5m 40s (- 30m 29s) (15700 15%) 3.0000\n",
      "5m 43s (- 30m 28s) (15800 15%) 3.1516\n",
      "5m 45s (- 30m 27s) (15900 15%) 3.2646\n",
      "5m 47s (- 30m 25s) (16000 16%) 3.0212\n",
      "5m 50s (- 30m 24s) (16100 16%) 3.0738\n",
      "5m 52s (- 30m 23s) (16200 16%) 3.0363\n",
      "5m 54s (- 30m 22s) (16300 16%) 2.9420\n",
      "5m 57s (- 30m 20s) (16400 16%) 2.8112\n",
      "5m 59s (- 30m 18s) (16500 16%) 2.6738\n",
      "6m 1s (- 30m 17s) (16600 16%) 2.7794\n",
      "6m 4s (- 30m 16s) (16700 16%) 3.0543\n",
      "6m 6s (- 30m 15s) (16800 16%) 3.0074\n",
      "6m 8s (- 30m 13s) (16900 16%) 2.9289\n",
      "6m 10s (- 30m 11s) (17000 17%) 2.8533\n",
      "6m 13s (- 30m 9s) (17100 17%) 2.8504\n",
      "6m 15s (- 30m 7s) (17200 17%) 2.9158\n",
      "6m 17s (- 30m 5s) (17300 17%) 3.0241\n",
      "6m 20s (- 30m 5s) (17400 17%) 2.8737\n",
      "6m 22s (- 30m 3s) (17500 17%) 2.7153\n",
      "6m 24s (- 30m 1s) (17600 17%) 2.6438\n",
      "6m 27s (- 30m 1s) (17700 17%) 2.9254\n",
      "6m 29s (- 30m 0s) (17800 17%) 2.7728\n",
      "6m 32s (- 29m 58s) (17900 17%) 2.5878\n",
      "6m 34s (- 29m 57s) (18000 18%) 2.6296\n",
      "6m 37s (- 29m 57s) (18100 18%) 2.8351\n",
      "6m 39s (- 29m 56s) (18200 18%) 2.5758\n",
      "6m 42s (- 29m 55s) (18300 18%) 2.7663\n",
      "6m 44s (- 29m 54s) (18400 18%) 2.8193\n",
      "6m 47s (- 29m 53s) (18500 18%) 2.8300\n",
      "6m 49s (- 29m 53s) (18600 18%) 2.7162\n",
      "6m 52s (- 29m 52s) (18700 18%) 2.6502\n",
      "6m 54s (- 29m 51s) (18800 18%) 2.6625\n",
      "6m 57s (- 29m 50s) (18900 18%) 2.8175\n",
      "6m 59s (- 29m 49s) (19000 19%) 2.7077\n",
      "7m 2s (- 29m 48s) (19100 19%) 2.3977\n",
      "7m 4s (- 29m 46s) (19200 19%) 2.5823\n",
      "7m 6s (- 29m 44s) (19300 19%) 2.5770\n",
      "7m 9s (- 29m 43s) (19400 19%) 2.6548\n",
      "7m 11s (- 29m 41s) (19500 19%) 2.4117\n",
      "7m 13s (- 29m 40s) (19600 19%) 2.6322\n",
      "7m 16s (- 29m 37s) (19700 19%) 2.3799\n",
      "7m 18s (- 29m 35s) (19800 19%) 2.5046\n",
      "7m 20s (- 29m 34s) (19900 19%) 2.7139\n",
      "7m 23s (- 29m 32s) (20000 20%) 2.4934\n",
      "7m 25s (- 29m 31s) (20100 20%) 2.5412\n",
      "7m 27s (- 29m 29s) (20200 20%) 2.3757\n",
      "7m 30s (- 29m 27s) (20300 20%) 2.4844\n",
      "7m 32s (- 29m 25s) (20400 20%) 2.3500\n",
      "7m 34s (- 29m 23s) (20500 20%) 2.5088\n",
      "7m 37s (- 29m 21s) (20600 20%) 2.4339\n",
      "7m 39s (- 29m 20s) (20700 20%) 2.5808\n",
      "7m 41s (- 29m 18s) (20800 20%) 2.4107\n",
      "7m 44s (- 29m 16s) (20900 20%) 2.4278\n",
      "7m 46s (- 29m 15s) (21000 21%) 2.4621\n",
      "7m 49s (- 29m 14s) (21100 21%) 2.5203\n",
      "7m 51s (- 29m 12s) (21200 21%) 2.2846\n",
      "7m 53s (- 29m 10s) (21300 21%) 2.3140\n",
      "7m 56s (- 29m 9s) (21400 21%) 2.2486\n",
      "7m 58s (- 29m 8s) (21500 21%) 2.2674\n",
      "8m 1s (- 29m 7s) (21600 21%) 2.1869\n",
      "8m 4s (- 29m 6s) (21700 21%) 2.2004\n",
      "8m 6s (- 29m 5s) (21800 21%) 2.1944\n",
      "8m 9s (- 29m 4s) (21900 21%) 2.2232\n",
      "8m 11s (- 29m 2s) (22000 22%) 2.1832\n",
      "8m 14s (- 29m 1s) (22100 22%) 2.4821\n",
      "8m 16s (- 29m 1s) (22200 22%) 2.2362\n",
      "8m 19s (- 28m 59s) (22300 22%) 2.2257\n",
      "8m 21s (- 28m 57s) (22400 22%) 2.1972\n",
      "8m 24s (- 28m 56s) (22500 22%) 2.4079\n",
      "8m 26s (- 28m 54s) (22600 22%) 2.0411\n",
      "8m 28s (- 28m 53s) (22700 22%) 2.2415\n",
      "8m 31s (- 28m 51s) (22800 22%) 2.1916\n",
      "8m 33s (- 28m 50s) (22900 22%) 2.1628\n",
      "8m 36s (- 28m 48s) (23000 23%) 2.0477\n",
      "8m 38s (- 28m 47s) (23100 23%) 2.1726\n",
      "8m 41s (- 28m 46s) (23200 23%) 2.1870\n",
      "8m 44s (- 28m 44s) (23300 23%) 2.0025\n",
      "8m 46s (- 28m 43s) (23400 23%) 2.0734\n",
      "8m 48s (- 28m 42s) (23500 23%) 2.2774\n",
      "8m 51s (- 28m 40s) (23600 23%) 2.2218\n",
      "8m 54s (- 28m 39s) (23700 23%) 2.0122\n",
      "8m 56s (- 28m 37s) (23800 23%) 1.8900\n",
      "8m 59s (- 28m 36s) (23900 23%) 2.2144\n",
      "9m 1s (- 28m 35s) (24000 24%) 2.1402\n",
      "9m 4s (- 28m 33s) (24100 24%) 1.9461\n",
      "9m 6s (- 28m 32s) (24200 24%) 2.0974\n",
      "9m 9s (- 28m 31s) (24300 24%) 2.0480\n",
      "9m 11s (- 28m 29s) (24400 24%) 1.9902\n",
      "9m 14s (- 28m 27s) (24500 24%) 2.0353\n",
      "9m 16s (- 28m 26s) (24600 24%) 2.1946\n",
      "9m 19s (- 28m 25s) (24700 24%) 2.0404\n",
      "9m 22s (- 28m 24s) (24800 24%) 2.2549\n",
      "9m 24s (- 28m 22s) (24900 24%) 1.8493\n",
      "9m 27s (- 28m 21s) (25000 25%) 1.9432\n",
      "9m 29s (- 28m 19s) (25100 25%) 2.3331\n",
      "9m 32s (- 28m 18s) (25200 25%) 2.3464\n",
      "9m 34s (- 28m 17s) (25300 25%) 2.1391\n",
      "9m 37s (- 28m 15s) (25400 25%) 1.9650\n",
      "9m 39s (- 28m 13s) (25500 25%) 2.0930\n",
      "9m 42s (- 28m 12s) (25600 25%) 1.9922\n",
      "9m 44s (- 28m 10s) (25700 25%) 2.2651\n",
      "9m 47s (- 28m 9s) (25800 25%) 2.2854\n",
      "9m 50s (- 28m 8s) (25900 25%) 2.0903\n",
      "9m 52s (- 28m 6s) (26000 26%) 2.0343\n",
      "9m 55s (- 28m 5s) (26100 26%) 1.9976\n",
      "9m 57s (- 28m 3s) (26200 26%) 1.9031\n",
      "10m 0s (- 28m 1s) (26300 26%) 1.9411\n",
      "10m 2s (- 28m 0s) (26400 26%) 1.8757\n",
      "10m 5s (- 27m 58s) (26500 26%) 1.7729\n",
      "10m 7s (- 27m 56s) (26600 26%) 1.7801\n",
      "10m 10s (- 27m 54s) (26700 26%) 1.9320\n",
      "10m 12s (- 27m 53s) (26800 26%) 2.0213\n",
      "10m 15s (- 27m 51s) (26900 26%) 1.8285\n",
      "10m 17s (- 27m 49s) (27000 27%) 1.8452\n",
      "10m 20s (- 27m 48s) (27100 27%) 2.0268\n",
      "10m 22s (- 27m 46s) (27200 27%) 1.9351\n",
      "10m 25s (- 27m 44s) (27300 27%) 1.8023\n",
      "10m 27s (- 27m 43s) (27400 27%) 1.8480\n",
      "10m 30s (- 27m 41s) (27500 27%) 1.7413\n",
      "10m 32s (- 27m 39s) (27600 27%) 1.8194\n",
      "10m 35s (- 27m 38s) (27700 27%) 1.8500\n",
      "10m 37s (- 27m 36s) (27800 27%) 1.7864\n",
      "10m 40s (- 27m 34s) (27900 27%) 1.8959\n",
      "10m 42s (- 27m 33s) (28000 28%) 1.9109\n",
      "10m 45s (- 27m 31s) (28100 28%) 1.7187\n",
      "10m 47s (- 27m 29s) (28200 28%) 2.0505\n",
      "10m 50s (- 27m 28s) (28300 28%) 1.8290\n",
      "10m 53s (- 27m 26s) (28400 28%) 1.8226\n",
      "10m 55s (- 27m 24s) (28500 28%) 1.7691\n",
      "10m 58s (- 27m 22s) (28600 28%) 1.9084\n",
      "11m 0s (- 27m 21s) (28700 28%) 1.9026\n",
      "11m 2s (- 27m 19s) (28800 28%) 1.6411\n",
      "11m 5s (- 27m 17s) (28900 28%) 1.9029\n",
      "11m 8s (- 27m 15s) (29000 28%) 1.7125\n",
      "11m 10s (- 27m 14s) (29100 29%) 1.6527\n",
      "11m 13s (- 27m 13s) (29200 29%) 1.8162\n",
      "11m 16s (- 27m 12s) (29300 29%) 1.8330\n",
      "11m 18s (- 27m 10s) (29400 29%) 1.8091\n",
      "11m 21s (- 27m 9s) (29500 29%) 1.6473\n",
      "11m 24s (- 27m 8s) (29600 29%) 1.8430\n",
      "11m 27s (- 27m 6s) (29700 29%) 1.7578\n",
      "11m 30s (- 27m 5s) (29800 29%) 1.6894\n",
      "11m 32s (- 27m 4s) (29900 29%) 1.6907\n",
      "11m 35s (- 27m 2s) (30000 30%) 1.7178\n",
      "11m 38s (- 27m 1s) (30100 30%) 1.7457\n",
      "11m 40s (- 26m 59s) (30200 30%) 1.7828\n",
      "11m 43s (- 26m 58s) (30300 30%) 1.8107\n",
      "11m 46s (- 26m 57s) (30400 30%) 1.8020\n",
      "11m 48s (- 26m 54s) (30500 30%) 1.5759\n",
      "11m 51s (- 26m 53s) (30600 30%) 1.8238\n",
      "11m 53s (- 26m 51s) (30700 30%) 1.6911\n",
      "11m 56s (- 26m 49s) (30800 30%) 1.6941\n",
      "11m 58s (- 26m 47s) (30900 30%) 1.7886\n",
      "12m 1s (- 26m 46s) (31000 31%) 1.8637\n",
      "12m 4s (- 26m 44s) (31100 31%) 1.8251\n",
      "12m 6s (- 26m 42s) (31200 31%) 1.5861\n",
      "12m 9s (- 26m 40s) (31300 31%) 2.1147\n",
      "12m 11s (- 26m 38s) (31400 31%) 3.7222\n",
      "12m 14s (- 26m 36s) (31500 31%) 2.9208\n",
      "12m 16s (- 26m 34s) (31600 31%) 2.8250\n",
      "12m 19s (- 26m 32s) (31700 31%) 2.6258\n",
      "12m 21s (- 26m 30s) (31800 31%) 2.5485\n",
      "12m 24s (- 26m 28s) (31900 31%) 2.3524\n",
      "12m 26s (- 26m 26s) (32000 32%) 2.5594\n",
      "12m 29s (- 26m 24s) (32100 32%) 3.7141\n",
      "12m 31s (- 26m 22s) (32200 32%) 3.8426\n",
      "12m 34s (- 26m 20s) (32300 32%) 3.8047\n",
      "12m 36s (- 26m 17s) (32400 32%) 3.1888\n",
      "12m 38s (- 26m 15s) (32500 32%) 3.2187\n",
      "12m 41s (- 26m 13s) (32600 32%) 3.4191\n",
      "12m 43s (- 26m 11s) (32700 32%) 3.1054\n",
      "12m 45s (- 26m 9s) (32800 32%) 3.1544\n",
      "12m 48s (- 26m 7s) (32900 32%) 3.1698\n",
      "12m 50s (- 26m 5s) (33000 33%) 3.1625\n",
      "12m 53s (- 26m 3s) (33100 33%) 3.2503\n",
      "12m 55s (- 26m 0s) (33200 33%) 2.8795\n",
      "12m 58s (- 25m 58s) (33300 33%) 3.0265\n",
      "13m 0s (- 25m 56s) (33400 33%) 2.9447\n",
      "13m 3s (- 25m 54s) (33500 33%) 2.7984\n",
      "13m 5s (- 25m 52s) (33600 33%) 2.7985\n",
      "13m 7s (- 25m 50s) (33700 33%) 2.6923\n",
      "13m 10s (- 25m 47s) (33800 33%) 2.5803\n",
      "13m 12s (- 25m 45s) (33900 33%) 2.8595\n",
      "13m 15s (- 25m 43s) (34000 34%) 2.6277\n",
      "13m 17s (- 25m 41s) (34100 34%) 2.7380\n",
      "13m 19s (- 25m 39s) (34200 34%) 2.9470\n",
      "13m 22s (- 25m 37s) (34300 34%) 2.6174\n",
      "13m 24s (- 25m 34s) (34400 34%) 2.5098\n",
      "13m 27s (- 25m 32s) (34500 34%) 2.6192\n",
      "13m 29s (- 25m 30s) (34600 34%) 2.6620\n",
      "13m 32s (- 25m 28s) (34700 34%) 2.6471\n",
      "13m 34s (- 25m 26s) (34800 34%) 2.4896\n",
      "13m 36s (- 25m 23s) (34900 34%) 2.4707\n",
      "13m 39s (- 25m 22s) (35000 35%) 2.6978\n",
      "13m 42s (- 25m 20s) (35100 35%) 2.6041\n",
      "13m 44s (- 25m 17s) (35200 35%) 2.5727\n",
      "13m 47s (- 25m 15s) (35300 35%) 2.4514\n",
      "13m 49s (- 25m 14s) (35400 35%) 2.5658\n",
      "13m 52s (- 25m 12s) (35500 35%) 2.4472\n",
      "13m 54s (- 25m 9s) (35600 35%) 2.3513\n",
      "13m 56s (- 25m 7s) (35700 35%) 2.3956\n",
      "13m 59s (- 25m 5s) (35800 35%) 2.5207\n",
      "14m 1s (- 25m 3s) (35900 35%) 2.3082\n",
      "14m 4s (- 25m 0s) (36000 36%) 2.3886\n",
      "14m 6s (- 24m 58s) (36100 36%) 2.4303\n",
      "14m 9s (- 24m 56s) (36200 36%) 2.5431\n",
      "14m 11s (- 24m 54s) (36300 36%) 2.3571\n",
      "14m 14s (- 24m 53s) (36400 36%) 2.5982\n",
      "14m 16s (- 24m 50s) (36500 36%) 2.1997\n",
      "14m 19s (- 24m 48s) (36600 36%) 2.5799\n",
      "14m 21s (- 24m 46s) (36700 36%) 2.7031\n",
      "14m 24s (- 24m 44s) (36800 36%) 2.5468\n",
      "14m 26s (- 24m 42s) (36900 36%) 2.4482\n",
      "14m 29s (- 24m 40s) (37000 37%) 2.6983\n",
      "14m 31s (- 24m 38s) (37100 37%) 2.7977\n",
      "14m 34s (- 24m 36s) (37200 37%) 2.4286\n",
      "14m 36s (- 24m 34s) (37300 37%) 2.7003\n",
      "14m 39s (- 24m 31s) (37400 37%) 2.5880\n",
      "14m 41s (- 24m 29s) (37500 37%) 2.6063\n",
      "14m 44s (- 24m 27s) (37600 37%) 2.4889\n",
      "14m 46s (- 24m 24s) (37700 37%) 2.2200\n",
      "14m 48s (- 24m 22s) (37800 37%) 2.3462\n",
      "14m 51s (- 24m 20s) (37900 37%) 2.3477\n",
      "14m 54s (- 24m 18s) (38000 38%) 2.5542\n",
      "14m 56s (- 24m 16s) (38100 38%) 2.5108\n",
      "14m 59s (- 24m 14s) (38200 38%) 2.2856\n",
      "15m 1s (- 24m 12s) (38300 38%) 2.4117\n",
      "15m 4s (- 24m 11s) (38400 38%) 2.5401\n",
      "15m 7s (- 24m 9s) (38500 38%) 2.2967\n",
      "15m 10s (- 24m 7s) (38600 38%) 2.5645\n",
      "15m 12s (- 24m 6s) (38700 38%) 2.3267\n",
      "15m 15s (- 24m 3s) (38800 38%) 2.3899\n",
      "15m 18s (- 24m 2s) (38900 38%) 2.5279\n",
      "15m 20s (- 24m 0s) (39000 39%) 2.4669\n",
      "15m 23s (- 23m 58s) (39100 39%) 2.4360\n",
      "15m 25s (- 23m 56s) (39200 39%) 2.6702\n",
      "15m 28s (- 23m 53s) (39300 39%) 2.3738\n",
      "15m 30s (- 23m 51s) (39400 39%) 2.4011\n",
      "15m 33s (- 23m 49s) (39500 39%) 2.4195\n",
      "15m 36s (- 23m 47s) (39600 39%) 2.5795\n",
      "15m 38s (- 23m 45s) (39700 39%) 2.3653\n",
      "15m 41s (- 23m 43s) (39800 39%) 2.2245\n",
      "15m 43s (- 23m 41s) (39900 39%) 2.3697\n",
      "15m 46s (- 23m 39s) (40000 40%) 2.1705\n",
      "15m 48s (- 23m 37s) (40100 40%) 2.4088\n",
      "15m 51s (- 23m 35s) (40200 40%) 2.2825\n",
      "15m 53s (- 23m 33s) (40300 40%) 2.3175\n",
      "15m 56s (- 23m 30s) (40400 40%) 2.1177\n",
      "15m 58s (- 23m 28s) (40500 40%) 2.2014\n",
      "16m 1s (- 23m 26s) (40600 40%) 2.2705\n",
      "16m 4s (- 23m 24s) (40700 40%) 2.2676\n",
      "16m 6s (- 23m 22s) (40800 40%) 2.2945\n",
      "16m 9s (- 23m 20s) (40900 40%) 2.1582\n",
      "16m 11s (- 23m 18s) (41000 41%) 2.2019\n",
      "16m 14s (- 23m 16s) (41100 41%) 2.1112\n",
      "16m 16s (- 23m 14s) (41200 41%) 2.2132\n",
      "16m 19s (- 23m 11s) (41300 41%) 2.0041\n",
      "16m 21s (- 23m 9s) (41400 41%) 2.0133\n",
      "16m 24s (- 23m 7s) (41500 41%) 2.0371\n",
      "16m 26s (- 23m 5s) (41600 41%) 1.8999\n",
      "16m 29s (- 23m 3s) (41700 41%) 2.0363\n",
      "16m 31s (- 23m 0s) (41800 41%) 1.7257\n",
      "16m 34s (- 22m 59s) (41900 41%) 2.2973\n",
      "16m 37s (- 22m 57s) (42000 42%) 1.9315\n",
      "16m 40s (- 22m 55s) (42100 42%) 1.9584\n",
      "16m 42s (- 22m 53s) (42200 42%) 2.1031\n",
      "16m 45s (- 22m 51s) (42300 42%) 2.1638\n",
      "16m 47s (- 22m 48s) (42400 42%) 2.0140\n",
      "16m 50s (- 22m 46s) (42500 42%) 2.0084\n",
      "16m 52s (- 22m 44s) (42600 42%) 1.9419\n",
      "16m 55s (- 22m 42s) (42700 42%) 1.9295\n",
      "16m 58s (- 22m 40s) (42800 42%) 2.1863\n",
      "17m 0s (- 22m 38s) (42900 42%) 2.0813\n",
      "17m 3s (- 22m 36s) (43000 43%) 1.9570\n",
      "17m 5s (- 22m 34s) (43100 43%) 1.8900\n",
      "17m 8s (- 22m 31s) (43200 43%) 1.8367\n",
      "17m 10s (- 22m 29s) (43300 43%) 2.1173\n",
      "17m 13s (- 22m 27s) (43400 43%) 1.8742\n",
      "17m 15s (- 22m 25s) (43500 43%) 1.8835\n",
      "17m 18s (- 22m 23s) (43600 43%) 1.8530\n",
      "17m 21s (- 22m 21s) (43700 43%) 1.7708\n",
      "17m 23s (- 22m 19s) (43800 43%) 2.0407\n",
      "17m 26s (- 22m 16s) (43900 43%) 1.6817\n",
      "17m 28s (- 22m 14s) (44000 44%) 1.7943\n",
      "17m 30s (- 22m 12s) (44100 44%) 1.6845\n",
      "17m 33s (- 22m 9s) (44200 44%) 1.7853\n",
      "17m 35s (- 22m 7s) (44300 44%) 1.6491\n",
      "17m 38s (- 22m 5s) (44400 44%) 1.7876\n",
      "17m 41s (- 22m 3s) (44500 44%) 1.7946\n",
      "17m 43s (- 22m 1s) (44600 44%) 1.9196\n",
      "17m 46s (- 21m 59s) (44700 44%) 1.8156\n",
      "17m 48s (- 21m 56s) (44800 44%) 1.7725\n",
      "17m 51s (- 21m 54s) (44900 44%) 1.6986\n",
      "17m 53s (- 21m 52s) (45000 45%) 1.7559\n",
      "17m 56s (- 21m 50s) (45100 45%) 1.8412\n",
      "17m 59s (- 21m 48s) (45200 45%) 1.7620\n",
      "18m 1s (- 21m 46s) (45300 45%) 1.7961\n",
      "18m 4s (- 21m 43s) (45400 45%) 1.6681\n",
      "18m 6s (- 21m 41s) (45500 45%) 2.3597\n",
      "18m 8s (- 21m 38s) (45600 45%) 2.1779\n",
      "18m 10s (- 21m 36s) (45700 45%) 2.0788\n",
      "18m 13s (- 21m 33s) (45800 45%) 1.9460\n",
      "18m 15s (- 21m 31s) (45900 45%) 2.0184\n",
      "18m 18s (- 21m 29s) (46000 46%) 2.1553\n",
      "18m 20s (- 21m 26s) (46100 46%) 2.3380\n",
      "18m 23s (- 21m 24s) (46200 46%) 2.0071\n",
      "18m 25s (- 21m 22s) (46300 46%) 1.9124\n",
      "18m 27s (- 21m 19s) (46400 46%) 1.8585\n",
      "18m 29s (- 21m 16s) (46500 46%) 1.7316\n",
      "18m 32s (- 21m 14s) (46600 46%) 1.8729\n",
      "18m 34s (- 21m 12s) (46700 46%) 1.7567\n",
      "18m 36s (- 21m 9s) (46800 46%) 1.8018\n",
      "18m 39s (- 21m 7s) (46900 46%) 1.8489\n",
      "18m 41s (- 21m 4s) (47000 47%) 1.9702\n",
      "18m 44s (- 21m 2s) (47100 47%) 1.9410\n",
      "18m 46s (- 21m 0s) (47200 47%) 1.8577\n",
      "18m 49s (- 20m 57s) (47300 47%) 1.9471\n",
      "18m 51s (- 20m 55s) (47400 47%) 1.9728\n",
      "18m 53s (- 20m 53s) (47500 47%) 1.8132\n",
      "18m 56s (- 20m 50s) (47600 47%) 1.8170\n",
      "18m 58s (- 20m 48s) (47700 47%) 1.7185\n",
      "19m 0s (- 20m 45s) (47800 47%) 1.8289\n",
      "19m 3s (- 20m 43s) (47900 47%) 1.8875\n",
      "19m 5s (- 20m 40s) (48000 48%) 1.7986\n",
      "19m 7s (- 20m 38s) (48100 48%) 1.6875\n",
      "19m 10s (- 20m 36s) (48200 48%) 1.7865\n",
      "19m 12s (- 20m 33s) (48300 48%) 1.7632\n",
      "19m 15s (- 20m 31s) (48400 48%) 1.9061\n",
      "19m 17s (- 20m 29s) (48500 48%) 1.8198\n",
      "19m 19s (- 20m 26s) (48600 48%) 1.6661\n",
      "19m 22s (- 20m 24s) (48700 48%) 2.0501\n",
      "19m 24s (- 20m 22s) (48800 48%) 2.1333\n",
      "19m 27s (- 20m 19s) (48900 48%) 2.1171\n",
      "19m 29s (- 20m 17s) (49000 49%) 1.7789\n",
      "19m 32s (- 20m 15s) (49100 49%) 1.7889\n",
      "19m 34s (- 20m 12s) (49200 49%) 1.9458\n",
      "19m 36s (- 20m 10s) (49300 49%) 1.9727\n",
      "19m 39s (- 20m 7s) (49400 49%) 1.8836\n",
      "19m 41s (- 20m 5s) (49500 49%) 1.7518\n",
      "19m 43s (- 20m 2s) (49600 49%) 1.6337\n",
      "19m 46s (- 20m 0s) (49700 49%) 1.5964\n",
      "19m 48s (- 19m 58s) (49800 49%) 1.7086\n",
      "19m 51s (- 19m 55s) (49900 49%) 1.8096\n",
      "19m 53s (- 19m 53s) (50000 50%) 1.8388\n",
      "19m 55s (- 19m 51s) (50100 50%) 1.8462\n",
      "19m 58s (- 19m 48s) (50200 50%) 1.6935\n",
      "20m 0s (- 19m 46s) (50300 50%) 1.8090\n",
      "20m 3s (- 19m 43s) (50400 50%) 1.9902\n",
      "20m 5s (- 19m 41s) (50500 50%) 1.9791\n",
      "20m 7s (- 19m 39s) (50600 50%) 1.9628\n",
      "20m 10s (- 19m 36s) (50700 50%) 1.9659\n",
      "20m 12s (- 19m 34s) (50800 50%) 1.9081\n",
      "20m 15s (- 19m 32s) (50900 50%) 1.9496\n",
      "20m 17s (- 19m 29s) (51000 51%) 1.8988\n",
      "20m 19s (- 19m 27s) (51100 51%) 1.9696\n",
      "20m 22s (- 19m 25s) (51200 51%) 2.0407\n",
      "20m 24s (- 19m 22s) (51300 51%) 1.9283\n",
      "20m 27s (- 19m 20s) (51400 51%) 1.8393\n",
      "20m 29s (- 19m 18s) (51500 51%) 2.0637\n",
      "20m 32s (- 19m 15s) (51600 51%) 1.9842\n",
      "20m 34s (- 19m 13s) (51700 51%) 2.0153\n",
      "20m 37s (- 19m 11s) (51800 51%) 1.8935\n",
      "20m 39s (- 19m 8s) (51900 51%) 1.7972\n",
      "20m 42s (- 19m 6s) (52000 52%) 1.8204\n",
      "20m 44s (- 19m 4s) (52100 52%) 1.8314\n",
      "20m 46s (- 19m 1s) (52200 52%) 1.8308\n",
      "20m 49s (- 18m 59s) (52300 52%) 1.8483\n",
      "20m 51s (- 18m 56s) (52400 52%) 1.7565\n",
      "20m 53s (- 18m 54s) (52500 52%) 1.6859\n",
      "20m 56s (- 18m 52s) (52600 52%) 1.6910\n",
      "20m 58s (- 18m 49s) (52700 52%) 1.8195\n",
      "21m 1s (- 18m 47s) (52800 52%) 1.6671\n",
      "21m 3s (- 18m 45s) (52900 52%) 1.7120\n",
      "21m 5s (- 18m 42s) (53000 53%) 1.5900\n",
      "21m 8s (- 18m 40s) (53100 53%) 1.8093\n",
      "21m 10s (- 18m 37s) (53200 53%) 1.6053\n",
      "21m 13s (- 18m 35s) (53300 53%) 1.5947\n",
      "21m 15s (- 18m 33s) (53400 53%) 1.9439\n",
      "21m 17s (- 18m 30s) (53500 53%) 2.8476\n",
      "21m 20s (- 18m 28s) (53600 53%) 2.8693\n",
      "21m 22s (- 18m 25s) (53700 53%) 2.7051\n",
      "21m 24s (- 18m 23s) (53800 53%) 3.2853\n",
      "21m 26s (- 18m 20s) (53900 53%) 3.6356\n",
      "21m 29s (- 18m 18s) (54000 54%) 4.3443\n",
      "21m 31s (- 18m 15s) (54100 54%) 4.6554\n",
      "21m 33s (- 18m 12s) (54200 54%) 4.6434\n",
      "21m 34s (- 18m 9s) (54300 54%) 4.2823\n",
      "21m 36s (- 18m 7s) (54400 54%) 4.5006\n",
      "21m 38s (- 18m 4s) (54500 54%) 4.1059\n",
      "21m 40s (- 18m 1s) (54600 54%) 4.1200\n",
      "21m 42s (- 17m 58s) (54700 54%) 3.5318\n",
      "21m 43s (- 17m 55s) (54800 54%) 3.5913\n",
      "21m 46s (- 17m 52s) (54900 54%) 4.1389\n",
      "21m 47s (- 17m 50s) (55000 55%) 3.8590\n",
      "21m 50s (- 17m 47s) (55100 55%) 4.1842\n",
      "21m 52s (- 17m 44s) (55200 55%) 3.9183\n",
      "21m 54s (- 17m 42s) (55300 55%) 3.7768\n",
      "21m 56s (- 17m 39s) (55400 55%) 4.0201\n",
      "21m 58s (- 17m 36s) (55500 55%) 3.4976\n",
      "21m 59s (- 17m 34s) (55600 55%) 3.7242\n",
      "22m 1s (- 17m 31s) (55700 55%) 3.6689\n",
      "22m 3s (- 17m 28s) (55800 55%) 4.0315\n",
      "22m 5s (- 17m 25s) (55900 55%) 3.6787\n",
      "22m 7s (- 17m 23s) (56000 56%) 3.0980\n",
      "22m 9s (- 17m 20s) (56100 56%) 3.4748\n",
      "22m 11s (- 17m 17s) (56200 56%) 3.8295\n",
      "22m 13s (- 17m 14s) (56300 56%) 3.1937\n",
      "22m 15s (- 17m 12s) (56400 56%) 3.7190\n",
      "22m 17s (- 17m 9s) (56500 56%) 3.6740\n",
      "22m 19s (- 17m 6s) (56600 56%) 3.3276\n",
      "22m 20s (- 17m 4s) (56700 56%) 2.9166\n",
      "22m 22s (- 17m 1s) (56800 56%) 3.2225\n",
      "22m 24s (- 16m 58s) (56900 56%) 3.0234\n",
      "22m 26s (- 16m 55s) (57000 56%) 3.4917\n",
      "22m 28s (- 16m 52s) (57100 57%) 3.6770\n",
      "22m 30s (- 16m 50s) (57200 57%) 3.6444\n",
      "22m 32s (- 16m 47s) (57300 57%) 3.4654\n",
      "22m 33s (- 16m 44s) (57400 57%) 3.3205\n",
      "22m 35s (- 16m 42s) (57500 57%) 3.4807\n",
      "22m 37s (- 16m 39s) (57600 57%) 3.0585\n",
      "22m 39s (- 16m 36s) (57700 57%) 3.1211\n",
      "22m 41s (- 16m 33s) (57800 57%) 3.3629\n",
      "22m 43s (- 16m 31s) (57900 57%) 3.1587\n",
      "22m 45s (- 16m 28s) (58000 57%) 3.6458\n",
      "22m 47s (- 16m 25s) (58100 58%) 3.2928\n",
      "22m 48s (- 16m 23s) (58200 58%) 3.3877\n",
      "22m 50s (- 16m 20s) (58300 58%) 3.0888\n",
      "22m 52s (- 16m 17s) (58400 58%) 3.6259\n",
      "22m 54s (- 16m 15s) (58500 58%) 4.0931\n",
      "22m 56s (- 16m 12s) (58600 58%) 3.6447\n",
      "22m 58s (- 16m 10s) (58700 58%) 3.0401\n",
      "23m 0s (- 16m 7s) (58800 58%) 3.5327\n",
      "23m 2s (- 16m 4s) (58900 58%) 3.6265\n",
      "23m 4s (- 16m 2s) (59000 59%) 3.5189\n",
      "23m 6s (- 15m 59s) (59100 59%) 3.2408\n",
      "23m 8s (- 15m 57s) (59200 59%) 3.4682\n",
      "23m 10s (- 15m 54s) (59300 59%) 3.4459\n",
      "23m 13s (- 15m 52s) (59400 59%) 3.7552\n",
      "23m 15s (- 15m 49s) (59500 59%) 3.3145\n",
      "23m 16s (- 15m 46s) (59600 59%) 3.2691\n",
      "23m 18s (- 15m 44s) (59700 59%) 3.4136\n",
      "23m 20s (- 15m 41s) (59800 59%) 3.3706\n",
      "23m 22s (- 15m 39s) (59900 59%) 3.6530\n",
      "23m 24s (- 15m 36s) (60000 60%) 3.5144\n",
      "23m 26s (- 15m 34s) (60100 60%) 3.7250\n",
      "23m 28s (- 15m 31s) (60200 60%) 3.5098\n",
      "23m 30s (- 15m 28s) (60300 60%) 3.3667\n",
      "23m 32s (- 15m 26s) (60400 60%) 3.0640\n",
      "23m 34s (- 15m 23s) (60500 60%) 3.7266\n",
      "23m 36s (- 15m 21s) (60600 60%) 3.4715\n",
      "23m 38s (- 15m 18s) (60700 60%) 3.4996\n",
      "23m 40s (- 15m 15s) (60800 60%) 3.6935\n",
      "23m 42s (- 15m 13s) (60900 60%) 3.5897\n",
      "23m 44s (- 15m 10s) (61000 61%) 3.3059\n",
      "23m 46s (- 15m 8s) (61100 61%) 3.2855\n",
      "23m 48s (- 15m 5s) (61200 61%) 3.4582\n",
      "23m 50s (- 15m 3s) (61300 61%) 3.5721\n",
      "23m 52s (- 15m 0s) (61400 61%) 3.2808\n",
      "23m 54s (- 14m 58s) (61500 61%) 3.6293\n",
      "23m 56s (- 14m 55s) (61600 61%) 3.2907\n",
      "23m 58s (- 14m 53s) (61700 61%) 3.6437\n",
      "24m 0s (- 14m 50s) (61800 61%) 3.4738\n",
      "24m 3s (- 14m 48s) (61900 61%) 3.6252\n",
      "24m 5s (- 14m 45s) (62000 62%) 3.5143\n",
      "24m 6s (- 14m 43s) (62100 62%) 3.1736\n",
      "24m 9s (- 14m 40s) (62200 62%) 3.5312\n",
      "24m 11s (- 14m 38s) (62300 62%) 3.8611\n",
      "24m 13s (- 14m 35s) (62400 62%) 3.1613\n",
      "24m 15s (- 14m 33s) (62500 62%) 3.4185\n",
      "24m 17s (- 14m 30s) (62600 62%) 3.5335\n",
      "24m 19s (- 14m 28s) (62700 62%) 3.5006\n",
      "24m 21s (- 14m 25s) (62800 62%) 3.5356\n",
      "24m 23s (- 14m 23s) (62900 62%) 3.3997\n",
      "24m 25s (- 14m 20s) (63000 63%) 3.5778\n",
      "24m 27s (- 14m 18s) (63100 63%) 3.6178\n",
      "24m 29s (- 14m 15s) (63200 63%) 3.3575\n",
      "24m 31s (- 14m 13s) (63300 63%) 3.1965\n",
      "24m 33s (- 14m 10s) (63400 63%) 3.4945\n",
      "24m 35s (- 14m 8s) (63500 63%) 3.2019\n",
      "24m 37s (- 14m 5s) (63600 63%) 3.5215\n",
      "24m 39s (- 14m 3s) (63700 63%) 3.2181\n",
      "24m 41s (- 14m 0s) (63800 63%) 3.4896\n",
      "24m 43s (- 13m 58s) (63900 63%) 3.4338\n",
      "24m 45s (- 13m 55s) (64000 64%) 3.5254\n",
      "24m 47s (- 13m 53s) (64100 64%) 3.1440\n",
      "24m 50s (- 13m 50s) (64200 64%) 3.4678\n",
      "24m 52s (- 13m 48s) (64300 64%) 3.5881\n",
      "24m 54s (- 13m 46s) (64400 64%) 3.7777\n",
      "24m 56s (- 13m 43s) (64500 64%) 3.2642\n",
      "24m 58s (- 13m 41s) (64600 64%) 3.5209\n",
      "25m 0s (- 13m 38s) (64700 64%) 3.1491\n",
      "25m 2s (- 13m 36s) (64800 64%) 3.6358\n",
      "25m 4s (- 13m 33s) (64900 64%) 3.4038\n",
      "25m 6s (- 13m 31s) (65000 65%) 3.5004\n",
      "25m 8s (- 13m 28s) (65100 65%) 3.4628\n",
      "25m 10s (- 13m 26s) (65200 65%) 3.4220\n",
      "25m 12s (- 13m 23s) (65300 65%) 3.4987\n",
      "25m 14s (- 13m 21s) (65400 65%) 3.4214\n",
      "25m 16s (- 13m 18s) (65500 65%) 3.4115\n",
      "25m 18s (- 13m 16s) (65600 65%) 2.8652\n",
      "25m 20s (- 13m 13s) (65700 65%) 3.2387\n",
      "25m 22s (- 13m 11s) (65800 65%) 3.4018\n",
      "25m 24s (- 13m 9s) (65900 65%) 3.5643\n",
      "25m 26s (- 13m 6s) (66000 66%) 3.3011\n",
      "25m 28s (- 13m 4s) (66100 66%) 3.7052\n",
      "25m 31s (- 13m 1s) (66200 66%) 3.5578\n",
      "25m 33s (- 12m 59s) (66300 66%) 3.7519\n",
      "25m 35s (- 12m 56s) (66400 66%) 3.3446\n",
      "25m 37s (- 12m 54s) (66500 66%) 3.7235\n",
      "25m 39s (- 12m 51s) (66600 66%) 3.2986\n",
      "25m 41s (- 12m 49s) (66700 66%) 3.5531\n",
      "25m 43s (- 12m 47s) (66800 66%) 3.5269\n",
      "25m 45s (- 12m 44s) (66900 66%) 3.8060\n",
      "25m 47s (- 12m 42s) (67000 67%) 3.2391\n",
      "25m 50s (- 12m 40s) (67100 67%) 3.3426\n",
      "25m 52s (- 12m 37s) (67200 67%) 3.3083\n",
      "25m 54s (- 12m 35s) (67300 67%) 3.3858\n",
      "25m 56s (- 12m 32s) (67400 67%) 3.3961\n",
      "25m 58s (- 12m 30s) (67500 67%) 3.1694\n",
      "26m 0s (- 12m 27s) (67600 67%) 3.0967\n",
      "26m 2s (- 12m 25s) (67700 67%) 3.2325\n",
      "26m 4s (- 12m 23s) (67800 67%) 3.4438\n",
      "26m 6s (- 12m 20s) (67900 67%) 3.1045\n",
      "26m 9s (- 12m 18s) (68000 68%) 3.3539\n",
      "26m 11s (- 12m 15s) (68100 68%) 3.4011\n",
      "26m 13s (- 12m 13s) (68200 68%) 3.3147\n",
      "26m 15s (- 12m 11s) (68300 68%) 3.3427\n",
      "26m 17s (- 12m 8s) (68400 68%) 3.6093\n",
      "26m 19s (- 12m 6s) (68500 68%) 3.4716\n",
      "26m 21s (- 12m 3s) (68600 68%) 3.4519\n",
      "26m 23s (- 12m 1s) (68700 68%) 3.5051\n",
      "26m 25s (- 11m 59s) (68800 68%) 3.3115\n",
      "26m 28s (- 11m 56s) (68900 68%) 3.6299\n",
      "26m 30s (- 11m 54s) (69000 69%) 2.9895\n",
      "26m 32s (- 11m 51s) (69100 69%) 3.3435\n",
      "26m 34s (- 11m 49s) (69200 69%) 3.1931\n",
      "26m 36s (- 11m 47s) (69300 69%) 3.1175\n",
      "26m 38s (- 11m 44s) (69400 69%) 3.4221\n",
      "26m 40s (- 11m 42s) (69500 69%) 3.3180\n",
      "26m 42s (- 11m 39s) (69600 69%) 3.0361\n",
      "26m 44s (- 11m 37s) (69700 69%) 3.4720\n",
      "26m 46s (- 11m 35s) (69800 69%) 3.1719\n",
      "26m 48s (- 11m 32s) (69900 69%) 3.0791\n",
      "26m 50s (- 11m 30s) (70000 70%) 3.3207\n",
      "26m 52s (- 11m 27s) (70100 70%) 3.3093\n",
      "26m 54s (- 11m 25s) (70200 70%) 3.1855\n",
      "26m 56s (- 11m 23s) (70300 70%) 3.2046\n",
      "26m 58s (- 11m 20s) (70400 70%) 3.0708\n",
      "27m 0s (- 11m 18s) (70500 70%) 2.9328\n",
      "27m 2s (- 11m 15s) (70600 70%) 3.1235\n",
      "27m 4s (- 11m 13s) (70700 70%) 3.0276\n",
      "27m 6s (- 11m 10s) (70800 70%) 3.2418\n",
      "27m 8s (- 11m 8s) (70900 70%) 3.2825\n",
      "27m 11s (- 11m 6s) (71000 71%) 3.3708\n",
      "27m 13s (- 11m 3s) (71100 71%) 3.3503\n",
      "27m 15s (- 11m 1s) (71200 71%) 3.5633\n",
      "27m 17s (- 10m 59s) (71300 71%) 3.3431\n",
      "27m 19s (- 10m 56s) (71400 71%) 3.2164\n",
      "27m 21s (- 10m 54s) (71500 71%) 3.2089\n",
      "27m 24s (- 10m 52s) (71600 71%) 3.0037\n",
      "27m 26s (- 10m 49s) (71700 71%) 3.2506\n",
      "27m 28s (- 10m 47s) (71800 71%) 3.1207\n",
      "27m 30s (- 10m 44s) (71900 71%) 3.1747\n",
      "27m 32s (- 10m 42s) (72000 72%) 3.0473\n",
      "27m 34s (- 10m 40s) (72100 72%) 3.1616\n",
      "27m 36s (- 10m 37s) (72200 72%) 3.2403\n",
      "27m 38s (- 10m 35s) (72300 72%) 3.1422\n",
      "27m 41s (- 10m 33s) (72400 72%) 2.9027\n",
      "27m 43s (- 10m 30s) (72500 72%) 3.2867\n",
      "27m 45s (- 10m 28s) (72600 72%) 2.7795\n",
      "27m 47s (- 10m 26s) (72700 72%) 3.2209\n",
      "27m 49s (- 10m 23s) (72800 72%) 3.3381\n",
      "27m 51s (- 10m 21s) (72900 72%) 2.7296\n",
      "27m 53s (- 10m 18s) (73000 73%) 3.1338\n",
      "27m 55s (- 10m 16s) (73100 73%) 3.0978\n",
      "27m 57s (- 10m 14s) (73200 73%) 3.2991\n",
      "27m 59s (- 10m 11s) (73300 73%) 3.0974\n",
      "28m 2s (- 10m 9s) (73400 73%) 3.4167\n",
      "28m 4s (- 10m 7s) (73500 73%) 3.2726\n",
      "28m 6s (- 10m 4s) (73600 73%) 2.9160\n",
      "28m 8s (- 10m 2s) (73700 73%) 2.9237\n",
      "28m 10s (- 10m 0s) (73800 73%) 3.0306\n",
      "28m 12s (- 9m 57s) (73900 73%) 2.9936\n",
      "28m 14s (- 9m 55s) (74000 74%) 3.3686\n",
      "28m 16s (- 9m 53s) (74100 74%) 3.1847\n",
      "28m 18s (- 9m 50s) (74200 74%) 3.3725\n",
      "28m 20s (- 9m 48s) (74300 74%) 3.2478\n",
      "28m 23s (- 9m 46s) (74400 74%) 3.1259\n",
      "28m 25s (- 9m 43s) (74500 74%) 2.9106\n",
      "28m 27s (- 9m 41s) (74600 74%) 3.1947\n",
      "28m 29s (- 9m 38s) (74700 74%) 3.2782\n",
      "28m 31s (- 9m 36s) (74800 74%) 3.2744\n",
      "28m 33s (- 9m 34s) (74900 74%) 2.8839\n",
      "28m 35s (- 9m 31s) (75000 75%) 2.7665\n",
      "28m 37s (- 9m 29s) (75100 75%) 3.2856\n",
      "28m 39s (- 9m 27s) (75200 75%) 3.2146\n",
      "28m 41s (- 9m 24s) (75300 75%) 3.0157\n",
      "28m 43s (- 9m 22s) (75400 75%) 3.0014\n",
      "28m 45s (- 9m 20s) (75500 75%) 2.8953\n",
      "28m 47s (- 9m 17s) (75600 75%) 3.1043\n",
      "28m 50s (- 9m 15s) (75700 75%) 3.1591\n",
      "28m 52s (- 9m 13s) (75800 75%) 3.1087\n",
      "28m 54s (- 9m 10s) (75900 75%) 3.0176\n",
      "28m 56s (- 9m 8s) (76000 76%) 3.4878\n",
      "28m 58s (- 9m 5s) (76100 76%) 2.9323\n",
      "29m 0s (- 9m 3s) (76200 76%) 3.1261\n",
      "29m 2s (- 9m 1s) (76300 76%) 3.1462\n",
      "29m 4s (- 8m 59s) (76400 76%) 3.1657\n",
      "29m 7s (- 8m 56s) (76500 76%) 3.1463\n",
      "29m 9s (- 8m 54s) (76600 76%) 3.0808\n",
      "29m 11s (- 8m 51s) (76700 76%) 3.0602\n",
      "29m 13s (- 8m 49s) (76800 76%) 2.9440\n",
      "29m 15s (- 8m 47s) (76900 76%) 2.6676\n",
      "29m 17s (- 8m 44s) (77000 77%) 2.9937\n",
      "29m 19s (- 8m 42s) (77100 77%) 3.2193\n",
      "29m 21s (- 8m 40s) (77200 77%) 2.9226\n",
      "29m 23s (- 8m 37s) (77300 77%) 3.0471\n",
      "29m 25s (- 8m 35s) (77400 77%) 2.9456\n",
      "29m 27s (- 8m 33s) (77500 77%) 3.1749\n",
      "29m 29s (- 8m 30s) (77600 77%) 2.9636\n",
      "29m 31s (- 8m 28s) (77700 77%) 2.8269\n",
      "29m 33s (- 8m 26s) (77800 77%) 3.2229\n",
      "29m 36s (- 8m 23s) (77900 77%) 3.0291\n",
      "29m 38s (- 8m 21s) (78000 78%) 3.1076\n",
      "29m 40s (- 8m 19s) (78100 78%) 3.1922\n",
      "29m 42s (- 8m 16s) (78200 78%) 2.7983\n",
      "29m 44s (- 8m 14s) (78300 78%) 2.9028\n",
      "29m 46s (- 8m 12s) (78400 78%) 2.7134\n",
      "29m 48s (- 8m 9s) (78500 78%) 3.1664\n",
      "29m 50s (- 8m 7s) (78600 78%) 3.4087\n",
      "29m 53s (- 8m 5s) (78700 78%) 2.9107\n",
      "29m 55s (- 8m 2s) (78800 78%) 2.9590\n",
      "29m 57s (- 8m 0s) (78900 78%) 3.1611\n",
      "29m 59s (- 7m 58s) (79000 79%) 3.0473\n",
      "30m 1s (- 7m 55s) (79100 79%) 2.8242\n",
      "30m 3s (- 7m 53s) (79200 79%) 3.0594\n",
      "30m 5s (- 7m 51s) (79300 79%) 3.0722\n",
      "30m 8s (- 7m 49s) (79400 79%) 3.2479\n",
      "30m 10s (- 7m 46s) (79500 79%) 2.8463\n",
      "30m 12s (- 7m 44s) (79600 79%) 2.9241\n",
      "30m 14s (- 7m 42s) (79700 79%) 3.3877\n",
      "30m 16s (- 7m 39s) (79800 79%) 2.7954\n",
      "30m 18s (- 7m 37s) (79900 79%) 2.6496\n",
      "30m 20s (- 7m 35s) (80000 80%) 2.9505\n",
      "30m 22s (- 7m 32s) (80100 80%) 3.0009\n",
      "30m 24s (- 7m 30s) (80200 80%) 2.9322\n",
      "30m 26s (- 7m 28s) (80300 80%) 2.9547\n",
      "30m 29s (- 7m 25s) (80400 80%) 2.9247\n",
      "30m 31s (- 7m 23s) (80500 80%) 3.4894\n",
      "30m 33s (- 7m 21s) (80600 80%) 2.9663\n",
      "30m 35s (- 7m 18s) (80700 80%) 2.7855\n",
      "30m 37s (- 7m 16s) (80800 80%) 2.9148\n",
      "30m 39s (- 7m 14s) (80900 80%) 3.1924\n",
      "30m 42s (- 7m 12s) (81000 81%) 2.9217\n",
      "30m 44s (- 7m 9s) (81100 81%) 2.6992\n",
      "30m 46s (- 7m 7s) (81200 81%) 3.1467\n",
      "30m 48s (- 7m 5s) (81300 81%) 2.8673\n",
      "30m 50s (- 7m 2s) (81400 81%) 3.0183\n",
      "30m 52s (- 7m 0s) (81500 81%) 3.1459\n",
      "30m 54s (- 6m 58s) (81600 81%) 2.8909\n",
      "30m 56s (- 6m 55s) (81700 81%) 2.8173\n",
      "30m 59s (- 6m 53s) (81800 81%) 2.9631\n",
      "31m 1s (- 6m 51s) (81900 81%) 2.9267\n",
      "31m 3s (- 6m 48s) (82000 82%) 2.6759\n",
      "31m 5s (- 6m 46s) (82100 82%) 2.9364\n",
      "31m 7s (- 6m 44s) (82200 82%) 2.7987\n",
      "31m 9s (- 6m 41s) (82300 82%) 2.6019\n",
      "31m 11s (- 6m 39s) (82400 82%) 2.9212\n",
      "31m 13s (- 6m 37s) (82500 82%) 2.9589\n",
      "31m 15s (- 6m 35s) (82600 82%) 3.4408\n",
      "31m 17s (- 6m 32s) (82700 82%) 3.3136\n",
      "31m 20s (- 6m 30s) (82800 82%) 3.2528\n",
      "31m 22s (- 6m 28s) (82900 82%) 2.9791\n",
      "31m 24s (- 6m 26s) (83000 83%) 2.9968\n",
      "31m 26s (- 6m 23s) (83100 83%) 2.8327\n",
      "31m 29s (- 6m 21s) (83200 83%) 3.1766\n",
      "31m 31s (- 6m 19s) (83300 83%) 3.0986\n",
      "31m 34s (- 6m 17s) (83400 83%) 2.9778\n",
      "31m 36s (- 6m 14s) (83500 83%) 3.0052\n",
      "31m 38s (- 6m 12s) (83600 83%) 2.7589\n",
      "31m 40s (- 6m 10s) (83700 83%) 2.7519\n",
      "31m 42s (- 6m 7s) (83800 83%) 2.6451\n",
      "31m 44s (- 6m 5s) (83900 83%) 2.8275\n",
      "31m 46s (- 6m 3s) (84000 84%) 3.3512\n",
      "31m 48s (- 6m 0s) (84100 84%) 2.5889\n",
      "31m 50s (- 5m 58s) (84200 84%) 3.0528\n",
      "31m 52s (- 5m 56s) (84300 84%) 2.9118\n",
      "31m 54s (- 5m 53s) (84400 84%) 2.9019\n",
      "31m 57s (- 5m 51s) (84500 84%) 3.0579\n",
      "31m 59s (- 5m 49s) (84600 84%) 2.8970\n",
      "32m 1s (- 5m 47s) (84700 84%) 3.1649\n",
      "32m 3s (- 5m 44s) (84800 84%) 3.0707\n",
      "32m 5s (- 5m 42s) (84900 84%) 2.8931\n",
      "32m 8s (- 5m 40s) (85000 85%) 3.1828\n",
      "32m 10s (- 5m 38s) (85100 85%) 3.1007\n",
      "32m 12s (- 5m 35s) (85200 85%) 3.0312\n",
      "32m 14s (- 5m 33s) (85300 85%) 2.8548\n",
      "32m 16s (- 5m 31s) (85400 85%) 2.6372\n",
      "32m 19s (- 5m 28s) (85500 85%) 3.0838\n",
      "32m 21s (- 5m 26s) (85600 85%) 2.7956\n",
      "32m 23s (- 5m 24s) (85700 85%) 2.6368\n",
      "32m 25s (- 5m 21s) (85800 85%) 2.8658\n",
      "32m 27s (- 5m 19s) (85900 85%) 2.7766\n",
      "32m 29s (- 5m 17s) (86000 86%) 2.6464\n",
      "32m 31s (- 5m 15s) (86100 86%) 2.6364\n",
      "32m 33s (- 5m 12s) (86200 86%) 2.7261\n",
      "32m 35s (- 5m 10s) (86300 86%) 2.9059\n",
      "32m 37s (- 5m 8s) (86400 86%) 3.0380\n",
      "32m 39s (- 5m 5s) (86500 86%) 2.7573\n",
      "32m 41s (- 5m 3s) (86600 86%) 2.7761\n",
      "32m 43s (- 5m 1s) (86700 86%) 2.6062\n",
      "32m 46s (- 4m 59s) (86800 86%) 2.8082\n",
      "32m 48s (- 4m 56s) (86900 86%) 2.8603\n",
      "32m 50s (- 4m 54s) (87000 87%) 2.3955\n",
      "32m 52s (- 4m 52s) (87100 87%) 2.6118\n",
      "32m 54s (- 4m 49s) (87200 87%) 2.8003\n",
      "32m 56s (- 4m 47s) (87300 87%) 2.6436\n",
      "32m 58s (- 4m 45s) (87400 87%) 2.8413\n",
      "33m 0s (- 4m 42s) (87500 87%) 2.6178\n",
      "33m 2s (- 4m 40s) (87600 87%) 2.6401\n",
      "33m 4s (- 4m 38s) (87700 87%) 2.7547\n",
      "33m 6s (- 4m 36s) (87800 87%) 2.5135\n",
      "33m 8s (- 4m 33s) (87900 87%) 2.8841\n",
      "33m 11s (- 4m 31s) (88000 88%) 2.8267\n",
      "33m 13s (- 4m 29s) (88100 88%) 2.7333\n",
      "33m 15s (- 4m 26s) (88200 88%) 2.6302\n",
      "33m 17s (- 4m 24s) (88300 88%) 2.5633\n",
      "33m 19s (- 4m 22s) (88400 88%) 2.6420\n",
      "33m 21s (- 4m 20s) (88500 88%) 2.7351\n",
      "33m 23s (- 4m 17s) (88600 88%) 2.5879\n",
      "33m 25s (- 4m 15s) (88700 88%) 2.6330\n",
      "33m 27s (- 4m 13s) (88800 88%) 2.4198\n",
      "33m 29s (- 4m 10s) (88900 88%) 3.0129\n",
      "33m 31s (- 4m 8s) (89000 89%) 2.4976\n",
      "33m 33s (- 4m 6s) (89100 89%) 2.5681\n",
      "33m 36s (- 4m 4s) (89200 89%) 2.7103\n",
      "33m 38s (- 4m 1s) (89300 89%) 2.7940\n",
      "33m 40s (- 3m 59s) (89400 89%) 2.6020\n",
      "33m 42s (- 3m 57s) (89500 89%) 2.6149\n",
      "33m 44s (- 3m 55s) (89600 89%) 2.8502\n",
      "33m 46s (- 3m 52s) (89700 89%) 2.7135\n",
      "33m 48s (- 3m 50s) (89800 89%) 2.5930\n",
      "33m 50s (- 3m 48s) (89900 89%) 2.5468\n",
      "33m 53s (- 3m 45s) (90000 90%) 2.6300\n",
      "33m 55s (- 3m 43s) (90100 90%) 2.4299\n",
      "33m 57s (- 3m 41s) (90200 90%) 2.4414\n",
      "33m 59s (- 3m 39s) (90300 90%) 2.5516\n",
      "34m 1s (- 3m 36s) (90400 90%) 2.4926\n",
      "34m 3s (- 3m 34s) (90500 90%) 2.3845\n",
      "34m 5s (- 3m 32s) (90600 90%) 2.7043\n",
      "34m 7s (- 3m 29s) (90700 90%) 2.4470\n",
      "34m 9s (- 3m 27s) (90800 90%) 2.5681\n",
      "34m 11s (- 3m 25s) (90900 90%) 2.4202\n",
      "34m 13s (- 3m 23s) (91000 91%) 2.8219\n",
      "34m 16s (- 3m 20s) (91100 91%) 2.7003\n",
      "34m 18s (- 3m 18s) (91200 91%) 2.5432\n",
      "34m 20s (- 3m 16s) (91300 91%) 2.7033\n",
      "34m 22s (- 3m 14s) (91400 91%) 2.3241\n",
      "34m 24s (- 3m 11s) (91500 91%) 2.4357\n",
      "34m 26s (- 3m 9s) (91600 91%) 2.5313\n",
      "34m 29s (- 3m 7s) (91700 91%) 2.7624\n",
      "34m 31s (- 3m 5s) (91800 91%) 2.7529\n",
      "34m 33s (- 3m 2s) (91900 91%) 2.6326\n",
      "34m 35s (- 3m 0s) (92000 92%) 2.6365\n",
      "34m 37s (- 2m 58s) (92100 92%) 2.5771\n",
      "34m 39s (- 2m 55s) (92200 92%) 2.5814\n",
      "34m 42s (- 2m 53s) (92300 92%) 2.5124\n",
      "34m 44s (- 2m 51s) (92400 92%) 2.3484\n",
      "34m 46s (- 2m 49s) (92500 92%) 2.5965\n",
      "34m 48s (- 2m 46s) (92600 92%) 2.8539\n",
      "34m 50s (- 2m 44s) (92700 92%) 2.6726\n",
      "34m 52s (- 2m 42s) (92800 92%) 2.6977\n",
      "34m 54s (- 2m 40s) (92900 92%) 2.4047\n",
      "34m 56s (- 2m 37s) (93000 93%) 2.8541\n",
      "34m 59s (- 2m 35s) (93100 93%) 2.6594\n",
      "35m 1s (- 2m 33s) (93200 93%) 2.6524\n",
      "35m 3s (- 2m 31s) (93300 93%) 2.6611\n",
      "35m 5s (- 2m 28s) (93400 93%) 2.7224\n",
      "35m 7s (- 2m 26s) (93500 93%) 2.4887\n",
      "35m 9s (- 2m 24s) (93600 93%) 2.5663\n",
      "35m 11s (- 2m 21s) (93700 93%) 2.4401\n",
      "35m 14s (- 2m 19s) (93800 93%) 2.3809\n",
      "35m 16s (- 2m 17s) (93900 93%) 2.5839\n",
      "35m 18s (- 2m 15s) (94000 94%) 2.7352\n",
      "35m 20s (- 2m 12s) (94100 94%) 2.5138\n",
      "35m 22s (- 2m 10s) (94200 94%) 2.6528\n",
      "35m 24s (- 2m 8s) (94300 94%) 2.4220\n",
      "35m 26s (- 2m 6s) (94400 94%) 2.6766\n",
      "35m 28s (- 2m 3s) (94500 94%) 2.4305\n",
      "35m 31s (- 2m 1s) (94600 94%) 2.5176\n",
      "35m 33s (- 1m 59s) (94700 94%) 2.6472\n",
      "35m 35s (- 1m 57s) (94800 94%) 2.3565\n",
      "35m 37s (- 1m 54s) (94900 94%) 2.5381\n",
      "35m 39s (- 1m 52s) (95000 95%) 2.7433\n",
      "35m 41s (- 1m 50s) (95100 95%) 2.5816\n",
      "35m 43s (- 1m 48s) (95200 95%) 2.5692\n",
      "35m 45s (- 1m 45s) (95300 95%) 2.9091\n",
      "35m 48s (- 1m 43s) (95400 95%) 2.8328\n",
      "35m 50s (- 1m 41s) (95500 95%) 2.6519\n",
      "35m 52s (- 1m 39s) (95600 95%) 2.4791\n",
      "35m 54s (- 1m 36s) (95700 95%) 2.3098\n",
      "35m 56s (- 1m 34s) (95800 95%) 2.9186\n",
      "35m 58s (- 1m 32s) (95900 95%) 2.5700\n",
      "36m 0s (- 1m 30s) (96000 96%) 2.4970\n",
      "36m 2s (- 1m 27s) (96100 96%) 2.6798\n",
      "36m 5s (- 1m 25s) (96200 96%) 2.5192\n",
      "36m 7s (- 1m 23s) (96300 96%) 2.7077\n",
      "36m 9s (- 1m 21s) (96400 96%) 2.5269\n",
      "36m 11s (- 1m 18s) (96500 96%) 2.3897\n",
      "36m 13s (- 1m 16s) (96600 96%) 2.5019\n",
      "36m 15s (- 1m 14s) (96700 96%) 2.3962\n",
      "36m 17s (- 1m 11s) (96800 96%) 2.2377\n",
      "36m 19s (- 1m 9s) (96900 96%) 2.5851\n",
      "36m 22s (- 1m 7s) (97000 97%) 2.6867\n",
      "36m 24s (- 1m 5s) (97100 97%) 2.5626\n",
      "36m 26s (- 1m 2s) (97200 97%) 2.5978\n",
      "36m 28s (- 1m 0s) (97300 97%) 2.4487\n",
      "36m 30s (- 0m 58s) (97400 97%) 2.6399\n",
      "36m 32s (- 0m 56s) (97500 97%) 2.9073\n",
      "36m 35s (- 0m 53s) (97600 97%) 2.3003\n",
      "36m 37s (- 0m 51s) (97700 97%) 2.6252\n",
      "36m 39s (- 0m 49s) (97800 97%) 2.3905\n",
      "36m 41s (- 0m 47s) (97900 97%) 2.3406\n",
      "36m 43s (- 0m 44s) (98000 98%) 2.6801\n",
      "36m 46s (- 0m 42s) (98100 98%) 2.5298\n",
      "36m 48s (- 0m 40s) (98200 98%) 2.4305\n",
      "36m 50s (- 0m 38s) (98300 98%) 2.8674\n",
      "36m 53s (- 0m 35s) (98400 98%) 2.6806\n",
      "36m 55s (- 0m 33s) (98500 98%) 2.3843\n",
      "36m 57s (- 0m 31s) (98600 98%) 2.3700\n",
      "36m 59s (- 0m 29s) (98700 98%) 2.3737\n",
      "37m 1s (- 0m 26s) (98800 98%) 2.2466\n",
      "37m 3s (- 0m 24s) (98900 98%) 2.5969\n",
      "37m 5s (- 0m 22s) (99000 99%) 2.1685\n",
      "37m 7s (- 0m 20s) (99100 99%) 2.6133\n",
      "37m 10s (- 0m 17s) (99200 99%) 2.4924\n",
      "37m 12s (- 0m 15s) (99300 99%) 2.4992\n",
      "37m 14s (- 0m 13s) (99400 99%) 2.3312\n",
      "37m 16s (- 0m 11s) (99500 99%) 2.2621\n",
      "37m 18s (- 0m 8s) (99600 99%) 2.5504\n",
      "37m 21s (- 0m 6s) (99700 99%) 2.3342\n",
      "37m 23s (- 0m 4s) (99800 99%) 2.5788\n",
      "37m 25s (- 0m 2s) (99900 99%) 2.5777\n",
      "37m 27s (- 0m 0s) (100000 100%) 2.5796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_LENGTH = 80\n",
    "batches = 100000 # In this case, the PyTorch train_per_epoch() and train() code is using batch_size=1\n",
    "hidden_size = 100\n",
    "\n",
    "my_encoder = EncoderRNN(len(english_vocab), hidden_size)\n",
    "my_decoder = DecoderRNN(hidden_size, len(indo_vocab))\n",
    "\n",
    "if use_cuda:\n",
    "    my_encoder = my_encoder.cuda()\n",
    "    my_decoder = my_decoder.cuda()\n",
    "\n",
    "train(my_encoder, my_decoder, batches, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100000\n",
      "encoder_vanilla_100_100000.pkl\n"
     ]
    }
   ],
   "source": [
    "# Here's a nice bleeding edge Python trick, (only works on Python3.6)\n",
    "# F-strings for the win!!\n",
    "# See https://www.python.org/dev/peps/pep-0498/\n",
    "print(hidden_size, batches)\n",
    "print(f'encoder_vanilla_{hidden_size}_{batches}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "# In Python >= 3.6\n",
    "with open(f'encoder_vanilla_{hidden_size}_{batches}.pkl', 'wb') as fout:\n",
    "    pickle.dump(my_encoder, fout)\n",
    "with open(f'decoder_vanilla_{hidden_size}_{batches}.pkl', 'wb') as fout:\n",
    "    pickle.dump(my_decoder, fout)\n",
    "\n",
    "# For Python < 3.6\n",
    "with open('encoder_vanilla_{}_{}.pkl'.format(hidden_size, batches), 'wb') as fout:\n",
    "    pickle.dump(my_encoder, fout)\n",
    "with open('decoder_vanilla_{}_{}.pkl'.format(hidden_size, batches), 'wb') as fout:\n",
    "    pickle.dump(my_decoder, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'translate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-196-2c3662b00cf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'French Muslims fined for face veils'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'translate' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(kopi_order):\n",
    "    output_words = translator(my_encoder, my_decoder, variable_from_sent(kopi_order, english_vocab))\n",
    "    print(output_words)\n",
    "    output_sentence = [indo_vocab[i] for i in output_words[1:output_words.index(1)]]\n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 6, 7, 3, 4, 5, 2, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'muslimah prancis didenda karena mengenakan burka'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('French Muslims fined for face veils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
