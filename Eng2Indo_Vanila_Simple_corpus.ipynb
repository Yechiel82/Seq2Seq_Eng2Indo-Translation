{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Eng2Indo Vanila Simple corpus.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "gDr-usY9p-V7"
      },
      "cell_type": "markdown",
      "source": [
        "## English to Indonesian translation using attention"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "pIR6m0HWp-V8",
        "pycharm": {},
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import os\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "oeRMKk0hp-V_",
        "pycharm": {},
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Loss function: https://pytorch.org/docs/stable/nn.html#torch.nn.NLLLoss\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "use_cuda = torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6BwzOdjOp-WB",
        "pycharm": {},
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from nltk import word_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "P6eOyoJEp-WE",
        "outputId": "12c20a2d-620e-4989-8b30-55c2bd0d59a3",
        "pycharm": {},
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "cell_type": "code",
      "source": [
        "fp = open('eng-indo-augmented.txt', 'r')\n",
        "text = fp.read()\n",
        "text = text.splitlines()\n",
        "fp.close()\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "text_dict = {\"English\": [], \"Indonesian\": []}\n",
        "for l in text:\n",
        "    split_text = l.split(\"\\t\")\n",
        "    text_dict[\"English\"].append(normalizeString(split_text[0]))\n",
        "    text_dict[\"Indonesian\"].append(normalizeString(split_text[1]))\n",
        "    \n",
        "df = pd.DataFrame.from_dict(text_dict)\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(22068, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Indonesian</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>run !</td>\n",
              "      <td>lari !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>who ?</td>\n",
              "      <td>siapa ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>wow !</td>\n",
              "      <td>wow !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>help !</td>\n",
              "      <td>tolong !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>jump !</td>\n",
              "      <td>lompat !</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  English Indonesian\n",
              "0   run !     lari !\n",
              "1   who ?    siapa ?\n",
              "2   wow !      wow !\n",
              "3  help !   tolong !\n",
              "4  jump !   lompat !"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eGVYxutbp-WG",
        "pycharm": {},
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 25\n",
        "MIN_LENGTH = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "i0YKRNLpp-WI",
        "outputId": "a7b15194-f0e0-46e8-f6cc-688f2428cfe1",
        "pycharm": {},
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "cell_type": "code",
      "source": [
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s\",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \",\n",
        "    \"tom is\", \"tom s\",\n",
        "    \"what s\", \"what a\",\n",
        "   \"are you\", \"do you\",\n",
        "   \"what is\", \"tom was\",\n",
        "   \"don t\", \"it s\", \"where s\",\n",
        "   \"where did\", \"where is\",\n",
        ")\n",
        "\n",
        "def should_keep_row(row):\n",
        "    \"\"\" Should the current row be kept as training set\"\"\"\n",
        "    # indo_num_words = len(word_tokenize(row[\"Indonesian\"]))\n",
        "    eng_num_words = len(word_tokenize(row[\"English\"]))\n",
        "    max_words_required = MAX_LENGTH - 2\n",
        "\n",
        "    return eng_num_words <= max_words_required\n",
        "\n",
        "df[\"keep_row\"] = df.apply(should_keep_row, axis=1)\n",
        "print(df.shape)\n",
        "df.head()\n",
        "\n",
        "print(\"Current shape: \" + str(df.shape))\n",
        "df = df[df[\"keep_row\"]]\n",
        "print(\"New shape: \" + str(df.shape))\n",
        "df.head()\n",
        "df = df.reset_index().drop(columns=[\"keep_row\"])\n",
        "df.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(22068, 3)\n",
            "Current shape: (22068, 3)\n",
            "New shape: (22057, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>English</th>\n",
              "      <th>Indonesian</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>run !</td>\n",
              "      <td>lari !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>who ?</td>\n",
              "      <td>siapa ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>wow !</td>\n",
              "      <td>wow !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>help !</td>\n",
              "      <td>tolong !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>jump !</td>\n",
              "      <td>lompat !</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index English Indonesian\n",
              "0      0   run !     lari !\n",
              "1      1   who ?    siapa ?\n",
              "2      2   wow !      wow !\n",
              "3      3  help !   tolong !\n",
              "4      4  jump !   lompat !"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "pHnwWXPlp-WM",
        "outputId": "aaa5de9c-bb0e-4a39-d5a4-165d3d4889b8",
        "pycharm": {},
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "cell_type": "code",
      "source": [
        "# Use a unique string to indicate START and END of a sentence.\n",
        "# Assign a unique index to them.\n",
        "START, START_IDX = '<s>',  0\n",
        "END, END_IDX = '</s>', 1\n",
        "UNK, UNK_IDX = 'UNK', 2\n",
        "\n",
        "SOS_token = START_IDX\n",
        "EOS_token = END_IDX\n",
        "\n",
        "# We use this idiom to tokenize our sentences in the dataframe column:\n",
        "# >>> DataFrame['column'].apply(str.lower).apply(word_tokenize)\n",
        "\n",
        "# Also we added the START and the END symbol to the sentences. \n",
        "english_sents = [START] + df['English'].apply(str.lower).apply(word_tokenize) + [END]\n",
        "indo_sents = [START] + df['Indonesian'].apply(str.lower).apply(word_tokenize) + [END]\n",
        "\n",
        "# We're sort of getting into the data into the shape we want. \n",
        "# But now it's still too humanly readable and redundant.\n",
        "## Cut-away: Computers like it to be simpler, more concise. -_-|||\n",
        "print('First English sentence:', english_sents[0])\n",
        "print('First Indo sentence:', indo_sents[0])\n",
        "\n",
        "english_vocab = Dictionary([['<s>'], ['</s>'],['UNK']])\n",
        "english_vocab.add_documents(english_sents)\n",
        "\n",
        "indo_vocab = Dictionary([['<s>'], ['</s>'], ['UNK']])\n",
        "indo_vocab.add_documents(indo_sents)\n",
        "\n",
        "# First ten words in the vocabulary.\n",
        "print('First 10 Indonesian words in Dictionary:\\n', sorted(indo_vocab.items())[:10])\n",
        "print()\n",
        "print('First 10 English words in Dictionary:\\n', sorted(english_vocab.items())[:10])\n",
        "\n",
        "english_vocab = Dictionary([['<s>'], ['</s>'],['UNK']])\n",
        "english_vocab.add_documents(english_sents)\n",
        "\n",
        "indo_vocab = Dictionary([['<s>'], ['</s>'], ['UNK']])\n",
        "indo_vocab.add_documents(indo_sents)\n",
        "\n",
        "# First ten words in the vocabulary.\n",
        "print('First 10 Indonesian words in Dictionary:\\n', sorted(indo_vocab.items())[:10])\n",
        "print()\n",
        "print('First 10 English words in Dictionary:\\n', sorted(english_vocab.items())[:10])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First English sentence: ['<s>', 'run', '!', '</s>']\n",
            "First Indo sentence: ['<s>', 'lari', '!', '</s>']\n",
            "First 10 Indonesian words in Dictionary:\n",
            " [(0, '<s>'), (1, '</s>'), (2, 'UNK'), (3, '!'), (4, 'lari'), (5, '?'), (6, 'siapa'), (7, 'wow'), (8, 'tolong'), (9, 'lompat')]\n",
            "\n",
            "First 10 English words in Dictionary:\n",
            " [(0, '<s>'), (1, '</s>'), (2, 'UNK'), (3, '!'), (4, 'run'), (5, '?'), (6, 'who'), (7, 'wow'), (8, 'help'), (9, 'jump')]\n",
            "First 10 Indonesian words in Dictionary:\n",
            " [(0, '<s>'), (1, '</s>'), (2, 'UNK'), (3, '!'), (4, 'lari'), (5, '?'), (6, 'siapa'), (7, 'wow'), (8, 'tolong'), (9, 'lompat')]\n",
            "\n",
            "First 10 English words in Dictionary:\n",
            " [(0, '<s>'), (1, '</s>'), (2, 'UNK'), (3, '!'), (4, 'run'), (5, '?'), (6, 'who'), (7, 'wow'), (8, 'help'), (9, 'jump')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "czemQI48p-WO"
      },
      "cell_type": "markdown",
      "source": [
        "## BLEU score"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "9a9U3uOlp-WP",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#input val_sent_pairs[0] english input to translate output is candidate\n",
        "#val_sent_pairs[1] reference \n",
        "def calculate_bleu_score(reference_sent,candidate_sent):\n",
        "    reference = [word_tokenize(reference_sent)]\n",
        "    candidate = word_tokenize(candidate_sent)\n",
        "    \n",
        "    if '<s>' in candidate:\n",
        "        candidate.remove('<s>')\n",
        "    if '</s>' in candidate:\n",
        "        candidate.remove('</s>')         \n",
        "    gram_1_score = sentence_bleu(reference,candidate,weights=(1, 0, 0, 0))\n",
        "    gram_2_score = sentence_bleu(reference,candidate,weights=(0.5, 0.5, 0, 0))\n",
        "    gram_3_score = sentence_bleu(reference,candidate,weights=(0.33, 0.33, 0.33, 0))\n",
        "    gram_4_score = sentence_bleu(reference,candidate,weights=(0.25, 0.25, 0.25, 0.25))\n",
        "    blue_score = (gram_1_score+gram_2_score+gram_3_score+gram_4_score)/4\n",
        "    #print(blue_score)\n",
        "    return blue_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ILYUNA9Bp-WR",
        "pycharm": {},
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# Lets save our dictionaries.\n",
        "#with open('./vocabs/simple_indo_vocab.Dictionary.pkl', 'wb') as fout:\n",
        "#    pickle.dump(indo_vocab, fout)\n",
        "    \n",
        "#with open('./vocabs/simple_english_vocab.Dictionary.pkl', 'wb') as fout:\n",
        "#    pickle.dump(english_vocab, fout)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HntAoFqGp-WW",
        "outputId": "f5435af6-697c-4fac-fb48-97cbbf8101b4",
        "pycharm": {},
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "cell_type": "code",
      "source": [
        "# Vectorizes a sentence with a given vocab\n",
        "def vectorize_sent(sent, vocab):\n",
        "    return vocab.doc2idx([START] + word_tokenize(sent.lower()) + [END], unknown_word_index=2)\n",
        "\n",
        "# Creates a PyTorch variable from a sentence against a given vocab\n",
        "def variable_from_sent(sent, vocab):\n",
        "    vsent = vectorize_sent(sent, vocab)\n",
        "    #print(vsent)\n",
        "    result = Variable(torch.LongTensor(vsent).view(-1, 1))\n",
        "    #print(result)\n",
        "    return result.cuda() if use_cuda else result\n",
        "\n",
        "# Test\n",
        "new_kopi = \"Is it love?\"\n",
        "variable_from_sent(new_kopi, english_vocab)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0],\n",
              "        [110],\n",
              "        [ 23],\n",
              "        [129],\n",
              "        [  5],\n",
              "        [  1]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "VJsfM5MRp-WZ",
        "pycharm": {}
      },
      "cell_type": "markdown",
      "source": [
        "## Split into train and validation"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2FHDR32sp-WZ",
        "outputId": "a5a90cd0-a03f-4613-8838-dc16f3970cb7",
        "pycharm": {},
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "df_train, df_val = train_test_split(df, test_size=0.15)\n",
        "print(df_train.shape)\n",
        "print(df_val.shape)\n",
        "\n",
        "df_train = df_train.reset_index(drop=True)\n",
        "df_val = df_val.reset_index(drop=True)\n",
        "df_train.head()\n",
        "\n",
        "indo_tensors = df_train['Indonesian'].apply(lambda s: variable_from_sent(s, indo_vocab))\n",
        "print(df_train.iloc[0]['Indonesian'])\n",
        "df_train\n",
        "\n",
        "english_tensors = df_train['English'].apply(lambda s: variable_from_sent(s, english_vocab))\n",
        "#print(df_train.iloc[0]['English'])\n",
        "#print(english_tensors[0])\n",
        "# Now, each item in `sent_pairs` is our data point. \n",
        "#print(\"############################\")\n",
        "sent_pairs = list(zip(english_tensors.values, indo_tensors.values))\n",
        "#print(sent_pairs[:5])\n",
        "#print(\"############################\")\n",
        "pairs = list(zip(df_train['English'], df_train['Indonesian']))\n",
        "print(pairs[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(18748, 3)\n",
            "(3309, 3)\n",
            "tom tidak tahu apa yang mary pikirkan tentang hal itu .\n",
            "('tom didn t know what mary thought about it .', 'tom tidak tahu apa yang mary pikirkan tentang hal itu .')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2z5n6tFjp-Wc",
        "outputId": "d0c9b1bc-2770-422d-eac1-a54a100a8d9a",
        "pycharm": {},
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def get_validation_pairs(df_val_in): #MOD Anurag\n",
        "    indo_val_tensors = df_val_in['Indonesian'].apply(lambda s: variable_from_sent(s, indo_vocab))\n",
        "    english_val_tensors = df_val_in['English'].apply(lambda s: variable_from_sent(s, english_vocab))\n",
        "    val_sent_tensor_pairs = list(zip(english_val_tensors.values, indo_val_tensors.values))\n",
        "    val_sent_pairs = list(zip(df_val_in['English'], df_val_in['Indonesian']))\n",
        "    return val_sent_pairs, val_sent_tensor_pairs\n",
        "\n",
        "\n",
        "val_sent_pairs, val_sent_tensor_pairs = get_validation_pairs(df_val) #MOD Anurag\n",
        "print(val_sent_pairs[0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('jack always wears goggles when he swims .', 'jack selalu mengenakan kacamata ketika dia berenang .')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "S5roJrm8p-Wf",
        "outputId": "c5adab0b-ad21-45aa-cc7c-e8862350c933",
        "pycharm": {},
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(val_sent_pairs[-1])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('tom waited on the front porch for johnny .', 'tom menunggu johnny di beranda depan .')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "apAoSCbup-Wh",
        "outputId": "06d68f39-46b1-4e77-9eb0-40a57780f286",
        "pycharm": {},
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(val_sent_pairs[154])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('michael looks tired doesn t he ?', 'bukankah michael terlihat lelah ?')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jhK0Iqjzp-Wj",
        "outputId": "d6e67a2c-fd59-471d-bdbd-a4b137b41e59",
        "pycharm": {},
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "cell_type": "code",
      "source": [
        "print(val_sent_pairs[154][0])\n",
        "\n",
        "for w in val_sent_pairs[154][0].split(' '):\n",
        "    print(english_vocab.doc2idx([w]))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "michael looks tired doesn t he ?\n",
            "[3496]\n",
            "[307]\n",
            "[258]\n",
            "[653]\n",
            "[118]\n",
            "[72]\n",
            "[5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "TIMz1Kzfp-Wl",
        "pycharm": {}
      },
      "cell_type": "markdown",
      "source": [
        "## Define model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DLZyDHRZp-Wm",
        "pycharm": {},
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "2BYgy7OPp-Wo",
        "pycharm": {}
      },
      "cell_type": "markdown",
      "source": [
        "## Get training and validation loss"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aisQ_LGVp-Wp",
        "pycharm": {},
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def get_train_loss(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "    \n",
        "    #print(\"Train\")\n",
        "    #print(\"Input tensor shape: {0}\".format(input_tensor.shape))\n",
        "    #print(\"Target tensor shape: {0}\".format(target_tensor.shape))\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length\n",
        "\n",
        "def get_validation_loss(input_tensor, target_tensor, encoder, decoder, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "    \n",
        "    #print(\"Validation\")\n",
        "    #print(\"Input tensor shape: {0}\".format(input_tensor.shape))\n",
        "    #print(\"Target tensor shape: {0}\".format(target_tensor.shape))\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss = criterion(decoder_output, target_tensor[di])\n",
        "            total_loss += float(loss.item())\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    return total_loss / target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "sXA4vX8dp-Wr",
        "pycharm": {}
      },
      "cell_type": "markdown",
      "source": [
        "## Utilities"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cAxSFCEXp-Ws",
        "pycharm": {},
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "\n",
        "SAVE_PATH = 'results/vanila_seq2seq'\n",
        "\n",
        "if not os.path.exists(SAVE_PATH):\n",
        "  os.makedirs(SAVE_PATH)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points, filename): # pier mod\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "    plt.savefig(filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "YbjQrQPPp-Wv",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        # input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_tensor = variable_from_sent(sentence, english_vocab)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('</s>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(indo_vocab.id2token[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "nXUXH83fp-Wy",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def translate(input_sentence, enc, dec):\n",
        "    output_words = evaluate(enc, dec, input_sentence)\n",
        "#     print('input =', input_sentence)\n",
        "#     print('output =', ' '.join(output_words))\n",
        "    candidate = ' '.join(output_words)\n",
        "    return candidate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "9qNoZvZHp-W0",
        "pycharm": {}
      },
      "cell_type": "markdown",
      "source": [
        "## Training loop and get evaluation result"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DG9pppqwp-W1",
        "pycharm": {},
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainIters(encoder, decoder, n_iters, batch_size=1, print_every=1000, save_every=1000, plot_every=100,\n",
        "               learning_rate=0.0001):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    val_losses = []\n",
        "    bleu_scores = []\n",
        "    \n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    # training_pairs = [sent_pairs[i] for i in range(n_iters)]\n",
        "    training_pairs = [random.sample(sent_pairs, batch_size) for i in range(n_iters)]\n",
        "\n",
        "    # training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "    MAX_PATIENCE = 50\n",
        "    patience = MAX_PATIENCE  \n",
        "    prev_val_loss =lowest_so_far = prev_bleu =  999\n",
        "    highest_so_far = -np.inf # for bleu\n",
        "    stopping_criteria_on = True\n",
        "    using_bleu_stopping = False\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        # print(\"################################\")\n",
        "        # print(training_pair)\n",
        "        input_tensor = training_pair[0][0]\n",
        "        target_tensor = training_pair[0][1]\n",
        "        # print(\"printing tensors for training...\")\n",
        "        # print(input_tensor)\n",
        "        # print(target_tensor)\n",
        "\n",
        "        loss = get_train_loss(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "                              criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        stopping_delta = 0.001  # if improvement is not more than this amount after n tries, exit the loop\n",
        "\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('Training loss: %s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                                        iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "            total_val_loss = 0\n",
        "            total_bleu_score = 0\n",
        "            total_val_pairs = len(val_sent_tensor_pairs)\n",
        "\n",
        "            for itr in range(0, len(val_sent_tensor_pairs)):\n",
        "                val_input_tensor = val_sent_tensor_pairs[itr][0]\n",
        "                val_target_tensor = val_sent_tensor_pairs[itr][1]\n",
        "                # print(\"Validation record: {0}\".format(itr))\n",
        "                # print(val_sent_pairs[itr])\n",
        "                #calc blue score\n",
        "                reference_sent = val_sent_pairs[itr][1]\n",
        "                candidate_sent = translate(val_sent_pairs[itr][0], encoder, decoder)\n",
        "                bleu_score = calculate_bleu_score(reference_sent,candidate_sent)\n",
        "                total_bleu_score += bleu_score\n",
        "                val_loss = get_validation_loss(val_input_tensor, val_target_tensor, encoder, decoder, criterion)\n",
        "                total_val_loss += val_loss\n",
        "\n",
        "            avg_val_loss = total_val_loss / total_val_pairs\n",
        "            val_losses.append(avg_val_loss)\n",
        "            avg_bleu_scores = total_bleu_score / total_val_pairs\n",
        "            bleu_scores.append(avg_bleu_scores)\n",
        "            \n",
        "            print('Validation loss: %s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                                          iter, iter / n_iters * 100, avg_val_loss))\n",
        "            print('Bleu scores: %s (%d %d%%) %.8f' % (timeSince(start, iter / n_iters),\n",
        "                                                          iter, iter / n_iters * 100, avg_bleu_scores))\n",
        "            if  stopping_criteria_on:\n",
        "                if not using_bleu_stopping:\n",
        "                    if (prev_val_loss - avg_val_loss) > stopping_delta and avg_val_loss < lowest_so_far:\n",
        "                        print(f\"Improvement in validation loss, saving model. Prev {prev_val_loss} Curr {avg_val_loss}\")\n",
        "                        lowest_so_far = avg_val_loss\n",
        "                        encoder_save_path = '%s/%s.pth' % (SAVE_PATH, 'best_encoder')\n",
        "                        print('save encoder weights to ', encoder_save_path)\n",
        "                        torch.save(encoder.state_dict(), encoder_save_path)\n",
        "                        decoder_save_path = '%s/%s.pth' % (SAVE_PATH, 'best_decoder')\n",
        "                        print('save decoder weights to ', decoder_save_path)\n",
        "                        torch.save(decoder.state_dict(), decoder_save_path)\n",
        "                        patience = MAX_PATIENCE # reset to max\n",
        "                    else:\n",
        "                        print(f\"No improvement in validation loss, losing patience {patience}\")\n",
        "                        patience -= 1\n",
        "\n",
        "                    if patience == 0:  # break out of training\n",
        "                        break\n",
        "\n",
        "                    prev_val_loss = avg_val_loss\n",
        "                else: # bleu\n",
        "                    if (avg_bleu_scores - prev_bleu) > stopping_delta and avg_bleu_scores > highest_so_far: \n",
        "                        print(f\"Improvement in bleu scores, saving model. Prev {prev_bleu} Curr {avg_bleu_scores}\")\n",
        "                        highest_so_far = avg_bleu_scores\n",
        "                        encoder_save_path = '%s/%s.pth' % (SAVE_PATH, 'best_encoder')\n",
        "                        print('save encoder weights to ', encoder_save_path)\n",
        "                        torch.save(encoder.state_dict(), encoder_save_path)\n",
        "                        decoder_save_path = '%s/%s.pth' % (SAVE_PATH, 'best_decoder')\n",
        "                        print('save decoder weights to ', decoder_save_path)\n",
        "                        torch.save(decoder.state_dict(), decoder_save_path)\n",
        "                        patience = MAX_PATIENCE # reset to max\n",
        "                    else:\n",
        "                        print(f\"No improvement in bleu scores, losing patience {patience}\")\n",
        "                        patience -= 1\n",
        "\n",
        "                    if patience == 0:  # break out of training\n",
        "                        break \n",
        "                    \n",
        "                    prev_bleu = avg_bleu_scores\n",
        "                \n",
        "\n",
        "            print(\"##########################################################\")\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "        # # save trained encoder and decoder\n",
        "        # if iter % save_every == 0:\n",
        "        #     encoder_save_path = '%s/%s-%d.pth' % (SAVE_PATH, 'encoder', iter)\n",
        "        #     print('save encoder weights to ', encoder_save_path)\n",
        "        #     torch.save(encoder.state_dict(), encoder_save_path)\n",
        "        #     decoder_save_path = '%s/%s-%d.pth' % (SAVE_PATH, 'decoder', iter)\n",
        "        #     print('save decoder weights to ', decoder_save_path)\n",
        "        #     torch.save(decoder.state_dict(), decoder_save_path)\n",
        "\n",
        "    showPlot(plot_losses, 'train_plot.png')\n",
        "    showPlot(val_losses, 'validation_plot.png')\n",
        "    showPlot(bleu_scores,'bleu_scores_plot.png')\n",
        "    return plot_losses, val_losses, bleu_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bJ3YTGgZp-W3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "LRio-rM_p-W5",
        "pycharm": {}
      },
      "cell_type": "markdown",
      "source": [
        "## Perform training"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "VRZp2jlYp-W5",
        "outputId": "344ece78-a596-4b37-84e5-8dd5c51260bb",
        "pycharm": {},
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3465
        }
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 512\n",
        "encoder1 = EncoderRNN(len(english_vocab), hidden_size).to(device)\n",
        "decoder1 = DecoderRNN(hidden_size, len(indo_vocab)).to(device)\n",
        "\n",
        "plot_losses, val_losses, bleu_scores = trainIters(encoder1, decoder1, 75000, print_every=1000)\n",
        "\n",
        "showPlot(plot_losses, 'train_plot.png')\n",
        "showPlot(val_losses, 'validation_plot.png')\n",
        "showPlot(bleu_scores, 'bleu_plot.png')\n",
        "\n",
        "evaluateRandomly(encoder1, decoder1)\n",
        "\n",
        "output_words = evaluate(encoder1, decoder1, \"do you love me?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss: 0m 28s (- 35m 44s) (1000 1%) 4.0061\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 1m 22s (- 101m 39s) (1000 1%) 3.2447\n",
            "Bleu scores: 1m 22s (- 101m 39s) (1000 1%) 0.36465009\n",
            "Improvement in validation loss, saving model. Prev 999 Curr 3.2447235189731005\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 1m 53s (- 69m 11s) (2000 2%) 3.9172\n",
            "Validation loss: 2m 57s (- 108m 9s) (2000 2%) 3.8995\n",
            "Bleu scores: 2m 57s (- 108m 9s) (2000 2%) 0.38206147\n",
            "No improvement in validation loss, losing patience 50\n",
            "##########################################################\n",
            "Training loss: 3m 28s (- 83m 23s) (3000 4%) 3.8560\n",
            "Validation loss: 4m 31s (- 108m 26s) (3000 4%) 3.8425\n",
            "Bleu scores: 4m 31s (- 108m 26s) (3000 4%) 0.37916557\n",
            "No improvement in validation loss, losing patience 49\n",
            "##########################################################\n",
            "Training loss: 5m 2s (- 89m 27s) (4000 5%) 3.7582\n",
            "Validation loss: 6m 8s (- 108m 58s) (4000 5%) 3.8527\n",
            "Bleu scores: 6m 8s (- 108m 58s) (4000 5%) 0.38846189\n",
            "No improvement in validation loss, losing patience 48\n",
            "##########################################################\n",
            "Training loss: 6m 40s (- 93m 21s) (5000 6%) 3.6033\n",
            "Validation loss: 7m 46s (- 108m 53s) (5000 6%) 3.7002\n",
            "Bleu scores: 7m 46s (- 108m 53s) (5000 6%) 0.40341000\n",
            "No improvement in validation loss, losing patience 47\n",
            "##########################################################\n",
            "Training loss: 8m 18s (- 95m 32s) (6000 8%) 3.5339\n",
            "Validation loss: 9m 25s (- 108m 19s) (6000 8%) 3.5481\n",
            "Bleu scores: 9m 25s (- 108m 19s) (6000 8%) 0.40924495\n",
            "No improvement in validation loss, losing patience 46\n",
            "##########################################################\n",
            "Training loss: 9m 57s (- 96m 41s) (7000 9%) 3.3862\n",
            "Validation loss: 11m 7s (- 108m 2s) (7000 9%) 3.5961\n",
            "Bleu scores: 11m 7s (- 108m 2s) (7000 9%) 0.39838962\n",
            "No improvement in validation loss, losing patience 45\n",
            "##########################################################\n",
            "Training loss: 11m 38s (- 97m 33s) (8000 10%) 3.1708\n",
            "Validation loss: 12m 43s (- 106m 30s) (8000 10%) 3.3508\n",
            "Bleu scores: 12m 43s (- 106m 30s) (8000 10%) 0.41192460\n",
            "No improvement in validation loss, losing patience 44\n",
            "##########################################################\n",
            "Training loss: 13m 14s (- 97m 8s) (9000 12%) 3.1341\n",
            "Validation loss: 14m 22s (- 105m 22s) (9000 12%) 3.3794\n",
            "Bleu scores: 14m 22s (- 105m 22s) (9000 12%) 0.39502286\n",
            "No improvement in validation loss, losing patience 43\n",
            "##########################################################\n",
            "Training loss: 14m 54s (- 96m 51s) (10000 13%) 3.0002\n",
            "Validation loss: 16m 1s (- 104m 6s) (10000 13%) 3.2918\n",
            "Bleu scores: 16m 1s (- 104m 6s) (10000 13%) 0.39802083\n",
            "No improvement in validation loss, losing patience 42\n",
            "##########################################################\n",
            "Training loss: 16m 33s (- 96m 17s) (11000 14%) 2.9846\n",
            "Validation loss: 17m 37s (- 102m 35s) (11000 14%) 3.1220\n",
            "Bleu scores: 17m 37s (- 102m 35s) (11000 14%) 0.40334729\n",
            "Improvement in validation loss, saving model. Prev 3.2918349036748444 Curr 3.1220221224260722\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 18m 9s (- 95m 21s) (12000 16%) 2.8992\n",
            "Validation loss: 19m 19s (- 101m 28s) (12000 16%) 3.1380\n",
            "Bleu scores: 19m 19s (- 101m 28s) (12000 16%) 0.38712542\n",
            "No improvement in validation loss, losing patience 50\n",
            "##########################################################\n",
            "Training loss: 19m 51s (- 94m 43s) (13000 17%) 2.7432\n",
            "Validation loss: 20m 57s (- 99m 58s) (13000 17%) 3.0265\n",
            "Bleu scores: 20m 57s (- 99m 58s) (13000 17%) 0.39026815\n",
            "Improvement in validation loss, saving model. Prev 3.138017777120754 Curr 3.026507193076561\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 21m 29s (- 93m 40s) (14000 18%) 2.7164\n",
            "Validation loss: 22m 35s (- 98m 25s) (14000 18%) 2.9449\n",
            "Bleu scores: 22m 35s (- 98m 25s) (14000 18%) 0.38783791\n",
            "Improvement in validation loss, saving model. Prev 3.026507193076561 Curr 2.944876438681712\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 23m 7s (- 92m 28s) (15000 20%) 2.6393\n",
            "Validation loss: 24m 12s (- 96m 48s) (15000 20%) 2.8666\n",
            "Bleu scores: 24m 12s (- 96m 48s) (15000 20%) 0.38858686\n",
            "Improvement in validation loss, saving model. Prev 2.944876438681712 Curr 2.8666280043733363\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 24m 44s (- 91m 12s) (16000 21%) 2.5080\n",
            "Validation loss: 25m 49s (- 95m 11s) (16000 21%) 2.8016\n",
            "Bleu scores: 25m 49s (- 95m 11s) (16000 21%) 0.38195646\n",
            "Improvement in validation loss, saving model. Prev 2.8666280043733363 Curr 2.8016051841834515\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 26m 21s (- 89m 55s) (17000 22%) 2.5053\n",
            "Validation loss: 27m 27s (- 93m 40s) (17000 22%) 2.7492\n",
            "Bleu scores: 27m 27s (- 93m 40s) (17000 22%) 0.38024856\n",
            "Improvement in validation loss, saving model. Prev 2.8016051841834515 Curr 2.7491732833197466\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 27m 59s (- 88m 37s) (18000 24%) 2.3380\n",
            "Validation loss: 29m 9s (- 92m 21s) (18000 24%) 2.7032\n",
            "Bleu scores: 29m 9s (- 92m 21s) (18000 24%) 0.37323258\n",
            "Improvement in validation loss, saving model. Prev 2.7491732833197466 Curr 2.703152307028549\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 29m 41s (- 87m 31s) (19000 25%) 2.3340\n",
            "Validation loss: 30m 48s (- 90m 48s) (19000 25%) 2.6621\n",
            "Bleu scores: 30m 48s (- 90m 48s) (19000 25%) 0.37365163\n",
            "Improvement in validation loss, saving model. Prev 2.703152307028549 Curr 2.6620783963162253\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 31m 20s (- 86m 11s) (20000 26%) 2.3106\n",
            "Validation loss: 32m 27s (- 89m 15s) (20000 26%) 2.5517\n",
            "Bleu scores: 32m 27s (- 89m 15s) (20000 26%) 0.37449664\n",
            "Improvement in validation loss, saving model. Prev 2.6620783963162253 Curr 2.551688672086947\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 32m 59s (- 84m 49s) (21000 28%) 2.1626\n",
            "Validation loss: 34m 4s (- 87m 38s) (21000 28%) 2.4577\n",
            "Bleu scores: 34m 4s (- 87m 38s) (21000 28%) 0.37535159\n",
            "Improvement in validation loss, saving model. Prev 2.551688672086947 Curr 2.4577000197509653\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 34m 37s (- 83m 24s) (22000 29%) 2.1782\n",
            "Validation loss: 35m 44s (- 86m 6s) (22000 29%) 2.4583\n",
            "Bleu scores: 35m 44s (- 86m 6s) (22000 29%) 0.37155466\n",
            "No improvement in validation loss, losing patience 50\n",
            "##########################################################\n",
            "Training loss: 36m 16s (- 82m 0s) (23000 30%) 2.0555\n",
            "Validation loss: 37m 25s (- 84m 36s) (23000 30%) 2.3392\n",
            "Bleu scores: 37m 25s (- 84m 36s) (23000 30%) 0.37549094\n",
            "Improvement in validation loss, saving model. Prev 2.458283836425782 Curr 2.3391915906076948\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 37m 57s (- 80m 40s) (24000 32%) 1.9388\n",
            "Validation loss: 39m 6s (- 83m 6s) (24000 32%) 2.3028\n",
            "Bleu scores: 39m 6s (- 83m 6s) (24000 32%) 0.37212332\n",
            "Improvement in validation loss, saving model. Prev 2.3391915906076948 Curr 2.302799182462723\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 39m 38s (- 79m 17s) (25000 33%) 1.8513\n",
            "Validation loss: 40m 46s (- 81m 32s) (25000 33%) 2.2062\n",
            "Bleu scores: 40m 46s (- 81m 32s) (25000 33%) 0.37311172\n",
            "Improvement in validation loss, saving model. Prev 2.302799182462723 Curr 2.2061825166449283\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 41m 18s (- 77m 50s) (26000 34%) 1.8136\n",
            "Validation loss: 42m 25s (- 79m 57s) (26000 34%) 2.1196\n",
            "Bleu scores: 42m 25s (- 79m 57s) (26000 34%) 0.37921942\n",
            "Improvement in validation loss, saving model. Prev 2.2061825166449283 Curr 2.1196143280842574\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 42m 57s (- 76m 22s) (27000 36%) 1.7637\n",
            "Validation loss: 44m 5s (- 78m 22s) (27000 36%) 2.0950\n",
            "Bleu scores: 44m 5s (- 78m 22s) (27000 36%) 0.37755405\n",
            "Improvement in validation loss, saving model. Prev 2.1196143280842574 Curr 2.095005805690755\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 44m 37s (- 74m 53s) (28000 37%) 1.6517\n",
            "Validation loss: 45m 43s (- 76m 45s) (28000 37%) 1.9949\n",
            "Bleu scores: 45m 43s (- 76m 45s) (28000 37%) 0.38269672\n",
            "Improvement in validation loss, saving model. Prev 2.095005805690755 Curr 1.9949270450923489\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 46m 16s (- 73m 23s) (29000 38%) 1.7564\n",
            "Validation loss: 47m 25s (- 75m 14s) (29000 38%) 1.9463\n",
            "Bleu scores: 47m 25s (- 75m 14s) (29000 38%) 0.38101407\n",
            "Improvement in validation loss, saving model. Prev 1.9949270450923489 Curr 1.94629826890329\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 47m 58s (- 71m 57s) (30000 40%) 1.5888\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "3jjv9Fiyp-W7",
        "pycharm": {}
      },
      "cell_type": "markdown",
      "source": [
        "## Check some translations"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "XHkh_erOp-W9",
        "pycharm": {},
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(translate(\"tom is playing with ball .\", encoder1, decoder1))\n",
        "\n",
        "print(translate(\"she is standing there .\", encoder1, decoder1))\n",
        "\n",
        "print(translate(\"he is a bad man .\", encoder1, decoder1))\n",
        "\n",
        "print(translate(\"he wants to sleep .\", encoder1, decoder1))\n",
        "\n",
        "print(translate(\"i can't see you crying .\", encoder1, decoder1))\n",
        "\n",
        "print(translate(\"my dog is running around .\", encoder1, decoder1))\n",
        "\n",
        "print(translate(\"it is very popular .\", encoder1, decoder1))\n",
        "\n",
        "print(translate(\"she speaks american english to tom's father .\", encoder1, decoder1))\n",
        "\n",
        "print(translate(\"please eat lunch in the afternoon .\", encoder1, decoder1))\n",
        "\n",
        "print(translate(\"i see red roses in the garden .\", encoder1, decoder1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "F_0qlyXup-W_",
        "pycharm": {},
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}