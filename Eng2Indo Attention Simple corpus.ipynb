{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English to Indonesian translation using attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Loss function: https://pytorch.org/docs/stable/nn.html#torch.nn.NLLLoss\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6752, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Indonesian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>run !</td>\n",
       "      <td>lari !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>who ?</td>\n",
       "      <td>siapa ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wow !</td>\n",
       "      <td>wow !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>help !</td>\n",
       "      <td>tolong !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jump !</td>\n",
       "      <td>lompat !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English Indonesian\n",
       "0   run !     lari !\n",
       "1   who ?    siapa ?\n",
       "2   wow !      wow !\n",
       "3  help !   tolong !\n",
       "4  jump !   lompat !"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = open('./corpus/eng-indo.txt', 'r')\n",
    "text = fp.read()\n",
    "text = text.splitlines()\n",
    "fp.close()\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "text_dict = {\"English\": [], \"Indonesian\": []}\n",
    "for l in text:\n",
    "    split_text = l.split(\"\\t\")\n",
    "    text_dict[\"English\"].append(normalizeString(split_text[0]))\n",
    "    text_dict[\"Indonesian\"].append(normalizeString(split_text[1]))\n",
    "    \n",
    "df = pd.DataFrame.from_dict(text_dict)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 15\n",
    "MIN_LENGTH = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6752, 3)\n",
      "Current shape: (6752, 3)\n",
      "New shape: (6609, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>English</th>\n",
       "      <th>Indonesian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34</td>\n",
       "      <td>i m sad .</td>\n",
       "      <td>saya sedih .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>it s me !</td>\n",
       "      <td>ini aku !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53</td>\n",
       "      <td>i get it .</td>\n",
       "      <td>aku mengerti .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>i got it .</td>\n",
       "      <td>aku mengerti .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>i m okay .</td>\n",
       "      <td>aku baik baik saja .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     English            Indonesian\n",
       "0     34   i m sad .          saya sedih .\n",
       "1     35   it s me !             ini aku !\n",
       "2     53  i get it .        aku mengerti .\n",
       "3     54  i got it .        aku mengerti .\n",
       "4     57  i m okay .  aku baik baik saja ."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \",\n",
    "    \"tom is\", \"tom s\",\n",
    "    \"what s\", \"what a\",\n",
    "   \"are you\", \"do you\",\n",
    "   \"what is\", \"tom was\",\n",
    "   \"don t\", \"it s\", \"where s\",\n",
    "   \"where did\", \"where is\",\n",
    ")\n",
    "\n",
    "def should_keep_row(row):\n",
    "    \"\"\" Should the current row be kept as training set\"\"\"\n",
    "    # indo_num_words = len(word_tokenize(row[\"Indonesian\"]))\n",
    "    eng_num_words = len(word_tokenize(row[\"English\"]))\n",
    "    max_words_required = MAX_LENGTH - 2\n",
    "    min_words_required = MIN_LENGTH\n",
    "\n",
    "    return eng_num_words <= max_words_required\n",
    "\n",
    "df[\"keep_row\"] = df.apply(should_keep_row, axis=1)\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "print(\"Current shape: \" + str(df.shape))\n",
    "df = df[df[\"keep_row\"]]\n",
    "print(\"New shape: \" + str(df.shape))\n",
    "df.head()\n",
    "df = df.reset_index().drop(columns=[\"keep_row\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First English sentence: ['<s>', 'i', 'm', 'sad', '.', '</s>']\n",
      "First Indo sentence: ['<s>', 'saya', 'sedih', '.', '</s>']\n",
      "First 10 Indonesian words in Dictionary:\n",
      " [(0, '<s>'), (1, '</s>'), (2, 'UNK'), (3, '.'), (4, 'saya'), (5, 'sedih'), (6, '!'), (7, 'aku'), (8, 'ini'), (9, 'mengerti')]\n",
      "\n",
      "First 10 English words in Dictionary:\n",
      " [(0, '<s>'), (1, '</s>'), (2, 'UNK'), (3, '.'), (4, 'i'), (5, 'm'), (6, 'sad'), (7, '!'), (8, 'it'), (9, 'me')]\n",
      "First 10 Indonesian words in Dictionary:\n",
      " [(0, '<s>'), (1, '</s>'), (2, 'UNK'), (3, '.'), (4, 'saya'), (5, 'sedih'), (6, '!'), (7, 'aku'), (8, 'ini'), (9, 'mengerti')]\n",
      "\n",
      "First 10 English words in Dictionary:\n",
      " [(0, '<s>'), (1, '</s>'), (2, 'UNK'), (3, '.'), (4, 'i'), (5, 'm'), (6, 'sad'), (7, '!'), (8, 'it'), (9, 'me')]\n"
     ]
    }
   ],
   "source": [
    "# Use a unique string to indicate START and END of a sentence.\n",
    "# Assign a unique index to them.\n",
    "START, START_IDX = '<s>',  0\n",
    "END, END_IDX = '</s>', 1\n",
    "UNK, UNK_IDX = 'UNK', 2\n",
    "\n",
    "SOS_token = START_IDX\n",
    "EOS_token = END_IDX\n",
    "\n",
    "# We use this idiom to tokenize our sentences in the dataframe column:\n",
    "# >>> DataFrame['column'].apply(str.lower).apply(word_tokenize)\n",
    "\n",
    "# Also we added the START and the END symbol to the sentences. \n",
    "english_sents = [START] + df['English'].apply(str.lower).apply(word_tokenize) + [END]\n",
    "indo_sents = [START] + df['Indonesian'].apply(str.lower).apply(word_tokenize) + [END]\n",
    "\n",
    "# We're sort of getting into the data into the shape we want. \n",
    "# But now it's still too humanly readable and redundant.\n",
    "## Cut-away: Computers like it to be simpler, more concise. -_-|||\n",
    "print('First English sentence:', english_sents[0])\n",
    "print('First Indo sentence:', indo_sents[0])\n",
    "\n",
    "english_vocab = Dictionary([['<s>'], ['</s>'],['UNK']])\n",
    "english_vocab.add_documents(english_sents)\n",
    "\n",
    "indo_vocab = Dictionary([['<s>'], ['</s>'], ['UNK']])\n",
    "indo_vocab.add_documents(indo_sents)\n",
    "\n",
    "# First ten words in the vocabulary.\n",
    "print('First 10 Indonesian words in Dictionary:\\n', sorted(indo_vocab.items())[:10])\n",
    "print()\n",
    "print('First 10 English words in Dictionary:\\n', sorted(english_vocab.items())[:10])\n",
    "\n",
    "english_vocab = Dictionary([['<s>'], ['</s>'],['UNK']])\n",
    "english_vocab.add_documents(english_sents)\n",
    "\n",
    "indo_vocab = Dictionary([['<s>'], ['</s>'], ['UNK']])\n",
    "indo_vocab.add_documents(indo_sents)\n",
    "\n",
    "# First ten words in the vocabulary.\n",
    "print('First 10 Indonesian words in Dictionary:\\n', sorted(indo_vocab.items())[:10])\n",
    "print()\n",
    "print('First 10 English words in Dictionary:\\n', sorted(english_vocab.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Lets save our dictionaries.\n",
    "#with open('./vocabs/simple_indo_vocab.Dictionary.pkl', 'wb') as fout:\n",
    "#    pickle.dump(indo_vocab, fout)\n",
    "    \n",
    "#with open('./vocabs/simple_english_vocab.Dictionary.pkl', 'wb') as fout:\n",
    "#    pickle.dump(english_vocab, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0],\n",
       "        [32],\n",
       "        [ 8],\n",
       "        [45],\n",
       "        [15],\n",
       "        [ 1]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorizes a sentence with a given vocab\n",
    "def vectorize_sent(sent, vocab):\n",
    "    return vocab.doc2idx([START] + word_tokenize(sent.lower()) + [END], unknown_word_index=2)\n",
    "\n",
    "# Creates a PyTorch variable from a sentence against a given vocab\n",
    "def variable_from_sent(sent, vocab):\n",
    "    vsent = vectorize_sent(sent, vocab)\n",
    "    #print(vsent)\n",
    "    result = Variable(torch.LongTensor(vsent).view(-1, 1))\n",
    "    #print(result)\n",
    "    return result.cuda() if use_cuda else result\n",
    "\n",
    "# Test\n",
    "new_kopi = \"Is it love?\"\n",
    "variable_from_sent(new_kopi, english_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5617, 3)\n",
      "(992, 3)\n",
      "perutku sakit .\n",
      "('my stomach hurts .', 'perutku sakit .')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_val = train_test_split(df, test_size=0.15)\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_train.head()\n",
    "\n",
    "indo_tensors = df_train['Indonesian'].apply(lambda s: variable_from_sent(s, indo_vocab))\n",
    "print(df_train.iloc[0]['Indonesian'])\n",
    "df_train\n",
    "\n",
    "english_tensors = df_train['English'].apply(lambda s: variable_from_sent(s, english_vocab))\n",
    "#print(df_train.iloc[0]['English'])\n",
    "#print(english_tensors[0])\n",
    "# Now, each item in `sent_pairs` is our data point. \n",
    "#print(\"############################\")\n",
    "sent_pairs = list(zip(english_tensors.values, indo_tensors.values))\n",
    "#print(sent_pairs[:5])\n",
    "#print(\"############################\")\n",
    "pairs = list(zip(df_train['English'], df_train['Indonesian']))\n",
    "print(pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tom poured cold water over his head .', 'tom menuangkan air dingin ke kepalanya .')\n"
     ]
    }
   ],
   "source": [
    "def get_validation_pairs(df_val_in): #MOD Anurag\n",
    "    indo_val_tensors = df_val_in['Indonesian'].apply(lambda s: variable_from_sent(s, indo_vocab))\n",
    "    english_val_tensors = df_val_in['English'].apply(lambda s: variable_from_sent(s, english_vocab))\n",
    "    val_sent_tensor_pairs = list(zip(english_val_tensors.values, indo_val_tensors.values))\n",
    "    val_sent_pairs = list(zip(df_val_in['English'], df_val_in['Indonesian']))\n",
    "    return val_sent_pairs, val_sent_tensor_pairs\n",
    "\n",
    "\n",
    "val_sent_pairs, val_sent_tensor_pairs = get_validation_pairs(df_val) #MOD Anurag\n",
    "print(val_sent_pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('have you ever been to hawaii ?', 'pernahkah kau ke hawaii ?')\n"
     ]
    }
   ],
   "source": [
    "print(val_sent_pairs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('don t be a wet blanket .', 'jangan suka mengganggu kesenangan orang lain .')\n"
     ]
    }
   ],
   "source": [
    "print(val_sent_pairs[154])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "don t be a wet blanket .\n",
      "[38]\n",
      "[40]\n",
      "[104]\n",
      "[72]\n",
      "[935]\n",
      "[1112]\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "print(val_sent_pairs[154][0])\n",
    "\n",
    "for w in val_sent_pairs[154][0].split(' '):\n",
    "    print(english_vocab.doc2idx([w]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get training and validation loss <- Bug here for evaluation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def get_train_loss(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    #print(\"Train\")\n",
    "    #print(\"Input tensor shape: {0}\".format(input_tensor.shape))\n",
    "    #print(\"Target tensor shape: {0}\".format(target_tensor.shape))\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "\n",
    "def get_validation_loss(input_tensor, target_tensor, encoder, decoder, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    #print(\"Validation\")\n",
    "    #print(\"Input tensor shape: {0}\".format(input_tensor.shape))\n",
    "    #print(\"Target tensor shape: {0}\".format(target_tensor.shape))\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss = criterion(decoder_output, target_tensor[di])\n",
    "            total_loss += float(loss.item())\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    return total_loss / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "SAVE_PATH = 'results'\n",
    "\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "  os.makedirs(SAVE_PATH)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points, filename): # pier mod\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop and get evaluation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, batch_size=1, print_every=1000, save_every=1000, plot_every=100,\n",
    "               learning_rate=0.0001):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    val_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    # training_pairs = [sent_pairs[i] for i in range(n_iters)]\n",
    "    training_pairs = [random.sample(sent_pairs, batch_size) for i in range(n_iters)]\n",
    "\n",
    "    # training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "    MAX_PATIENCE = 10\n",
    "    patience = MAX_PATIENCE  # mod Pier\n",
    "    prev_val_loss =lowest_so_far = 999\n",
    "    stopping_criteria_on = False\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        # print(\"################################\")\n",
    "        # print(training_pair)\n",
    "        input_tensor = training_pair[0][0]\n",
    "        target_tensor = training_pair[0][1]\n",
    "        # print(\"printing tensors for training...\")\n",
    "        # print(input_tensor)\n",
    "        # print(target_tensor)\n",
    "\n",
    "        loss = get_train_loss(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "                              criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        stopping_delta = 0.001  # if improvement is not more than this amount after n tries, exit the loop\n",
    "\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('Training loss: %s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                                        iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "            total_val_loss = 0\n",
    "            total_val_pairs = len(val_sent_tensor_pairs)\n",
    "\n",
    "            for itr in range(0, len(val_sent_tensor_pairs)):\n",
    "                val_input_tensor = val_sent_tensor_pairs[itr][0]\n",
    "                val_target_tensor = val_sent_tensor_pairs[itr][1]\n",
    "                # print(\"Validation record: {0}\".format(itr))\n",
    "                # print(val_sent_pairs[itr])\n",
    "                val_loss = get_validation_loss(val_input_tensor, val_target_tensor, encoder, decoder, criterion)\n",
    "                total_val_loss += val_loss\n",
    "\n",
    "            avg_val_loss = total_val_loss / total_val_pairs\n",
    "            val_losses.append(avg_val_loss)\n",
    "            print('Validation loss: %s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                                          iter, iter / n_iters * 100, avg_val_loss))\n",
    "            \n",
    "            if  stopping_criteria_on:\n",
    "                # mod Pier\n",
    "                if (prev_val_loss - avg_val_loss) > stopping_delta and avg_val_loss < lowest_so_far:\n",
    "                    print(f\"Improvement in validation loss, saving model. Prev {prev_val_loss} Curr {avg_val_loss}\")\n",
    "                    lowest_so_far = avg_val_loss\n",
    "                    encoder_save_path = '%s/%s-%d.pth' % (SAVE_PATH, 'encoder', iter)\n",
    "                    print('save encoder weights to ', encoder_save_path)\n",
    "                    torch.save(encoder.state_dict(), encoder_save_path)\n",
    "                    decoder_save_path = '%s/%s-%d.pth' % (SAVE_PATH, 'decoder', iter)\n",
    "                    print('save decoder weights to ', decoder_save_path)\n",
    "                    torch.save(decoder.state_dict(), decoder_save_path)\n",
    "                    patience = MAX_PATIENCE # reset to max\n",
    "                else:\n",
    "                    print(f\"No improvement in validation loss, losing patience {patience}\")\n",
    "                    patience -= 1\n",
    "\n",
    "                if patience == 0:  # break out of training\n",
    "                    break\n",
    "\n",
    "                prev_val_loss = avg_val_loss\n",
    "                # end mod Pier\n",
    "\n",
    "            print(\"##########################################################\")\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "        # # save trained encoder and decoder\n",
    "        # if iter % save_every == 0:\n",
    "        #     encoder_save_path = '%s/%s-%d.pth' % (SAVE_PATH, 'encoder', iter)\n",
    "        #     print('save encoder weights to ', encoder_save_path)\n",
    "        #     torch.save(encoder.state_dict(), encoder_save_path)\n",
    "        #     decoder_save_path = '%s/%s-%d.pth' % (SAVE_PATH, 'decoder', iter)\n",
    "        #     print('save decoder weights to ', decoder_save_path)\n",
    "        #     torch.save(decoder.state_dict(), decoder_save_path)\n",
    "\n",
    "    showPlot(plot_losses, 'train_plot.png')\n",
    "    showPlot(val_losses, 'validation_plot.png')\n",
    "\n",
    "    return plot_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trainIters(encoder, decoder, n_iters, batch_size=1, print_every=1000, save_every=1000, plot_every=100,\n",
    "#                learning_rate=0.0001):\n",
    "#     start = time.time()\n",
    "#     plot_losses = []\n",
    "#     val_losses = []\n",
    "#     print_loss_total = 0  # Reset every print_every\n",
    "#     plot_loss_total = 0  # Reset every plot_every\n",
    "# \n",
    "#     encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "#     decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "#     # training_pairs = [sent_pairs[i] for i in range(n_iters)]\n",
    "#     training_pairs = [random.sample(sent_pairs, batch_size) for i in range(n_iters)]\n",
    "# \n",
    "#     # training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
    "#     criterion = nn.NLLLoss()\n",
    "# \n",
    "#     patience = 10  # mod Pier\n",
    "# \n",
    "#     for iter in range(1, n_iters + 1):\n",
    "#         training_pair = training_pairs[iter - 1]\n",
    "#         # print(\"################################\")\n",
    "#         # print(training_pair)\n",
    "#         input_tensor = training_pair[0][0]\n",
    "#         target_tensor = training_pair[0][1]\n",
    "#         # print(\"printing tensors for training...\")\n",
    "#         # print(input_tensor)\n",
    "#         # print(target_tensor)\n",
    "# \n",
    "#         loss = get_train_loss(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "#                               criterion)\n",
    "#         print_loss_total += loss\n",
    "#         plot_loss_total += loss\n",
    "# \n",
    "#         stopping_delta = 0.01  # if improvement is not more than this amount after n tries, exit the loop\n",
    "#         prev_val_loss = 999\n",
    "# \n",
    "#         if iter % print_every == 0:\n",
    "#             print_loss_avg = print_loss_total / print_every\n",
    "#             print_loss_total = 0\n",
    "#             print('Training loss: %s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "#                                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "# \n",
    "#             total_val_loss = 0\n",
    "#             total_val_pairs = len(val_sent_tensor_pairs)\n",
    "# \n",
    "#             for itr in range(0, len(val_sent_tensor_pairs)):\n",
    "#                 val_input_tensor = val_sent_tensor_pairs[itr][0]\n",
    "#                 val_target_tensor = val_sent_tensor_pairs[itr][1]\n",
    "#                 # print(\"Validation record: {0}\".format(itr))\n",
    "#                 # print(val_sent_pairs[itr])\n",
    "#                 val_loss = get_validation_loss(val_input_tensor, val_target_tensor, encoder, decoder, criterion)\n",
    "#                 total_val_loss += val_loss\n",
    "# \n",
    "#             avg_val_loss = total_val_loss / total_val_pairs\n",
    "#             val_losses.append(avg_val_loss)\n",
    "#             print('Validation loss: %s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "#                                                           iter, iter / n_iters * 100, avg_val_loss))\n",
    "# \n",
    "#             # mod P_ier\n",
    "#             if abs(avg_val_loss - prev_val_loss) > stopping_delta:\n",
    "#                 print(f\"No improvement in validation loss, losing patience, saving model : {patience}\")\n",
    "#                 encoder_save_path = '%s/%s-%d.pth' % (SAVE_PATH, 'encoder', iter)\n",
    "#                 print('save encoder weights to ', encoder_save_path)\n",
    "#                 torch.save(encoder.state_dict(), encoder_save_path)\n",
    "#                 decoder_save_path = '%s/%s-%d.pth' % (SAVE_PATH, 'decoder', iter)\n",
    "#                 print('save decoder weights to ', decoder_save_path)\n",
    "#                 torch.save(decoder.state_dict(), decoder_save_path)\n",
    "# \n",
    "#                 patience -= 1\n",
    "# \n",
    "#             if patience == 0:  # break out of training\n",
    "#                 break\n",
    "# \n",
    "#             prev_val_loss = avg_val_loss\n",
    "#             # end mod Pier\n",
    "# \n",
    "#             print(\"##########################################################\")\n",
    "# \n",
    "#         if iter % plot_every == 0:\n",
    "#             plot_loss_avg = plot_loss_total / plot_every\n",
    "#             plot_losses.append(plot_loss_avg)\n",
    "#             plot_loss_total = 0\n",
    "# \n",
    "#         # # save trained encoder and decoder\n",
    "#         # if iter % save_every == 0:\n",
    "#         #     encoder_save_path = '%s/%s-%d.pth' % (SAVE_PATH, 'encoder', iter)\n",
    "#         #     print('save encoder weights to ', encoder_save_path)\n",
    "#         #     torch.save(encoder.state_dict(), encoder_save_path)\n",
    "#         #     decoder_save_path = '%s/%s-%d.pth' % (SAVE_PATH, 'decoder', iter)\n",
    "#         #     print('save decoder weights to ', decoder_save_path)\n",
    "#         #     torch.save(decoder.state_dict(), decoder_save_path)\n",
    "# \n",
    "#     showPlot(plot_losses, 'train_plot.png')\n",
    "#     showPlot(val_losses, 'validation_plot.png')\n",
    "# \n",
    "#     return plot_losses, val_losses\n",
    "\n",
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        # input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_tensor = variable_from_sent(sentence, english_vocab)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('</s>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(indo_vocab.id2token[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0m 3s (- 62m 36s) (100 0%) 8.2664\n",
      "Validation loss: 0m 10s (- 170m 54s) (100 0%) 8.2347\n",
      "##########################################################\n",
      "Training loss: 0m 13s (- 115m 31s) (200 0%) 8.1076\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 1024\n",
    "encoder1 = EncoderRNN(len(english_vocab), hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, len(indo_vocab), dropout_p=0.3).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 100000, print_every=100)\n",
    "\n",
    "evaluateRandomly(encoder1, attn_decoder1)\n",
    "\n",
    "output_words, attentions = evaluate(\n",
    "    encoder1, attn_decoder1, \"do you love me?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check some translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttnDecoderRNN(\n",
       "  (embedding): Embedding(4187, 1024)\n",
       "  (attn): Linear(in_features=2048, out_features=25, bias=True)\n",
       "  (attn_combine): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (gru): GRU(1024, 1024)\n",
       "  (out): Linear(in_features=1024, out_features=4187, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the best model \n",
    "#hidden_size = 1024\n",
    "#encoder1 = EncoderRNN(len(english_vocab), hidden_size).to(device)\n",
    "#attn_decoder1 = AttnDecoderRNN(hidden_size, len(indo_vocab), dropout_p=0.1).to(device)\n",
    "#encoder1.load_state_dict(torch.load(\"./results/encoder-1800.pth\"))\n",
    "#encoder1.eval()\n",
    "\n",
    "#attn_decoder1.load_state_dict(torch.load(\"./results/decoder-1800.pth\"))\n",
    "#attn_decoder1.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = tom is playing with ball.\n",
      "output = <s> <s> . </s>\n",
      "input = she is standing there .\n",
      "output = <s> <s> . </s>\n",
      "input = he is a bad man .\n",
      "output = <s> <s> . </s>\n",
      "input = he wants to sleep .\n",
      "output = <s> <s> . </s>\n"
     ]
    }
   ],
   "source": [
    "def translate(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "\n",
    "translate(\"tom is playing with ball.\")\n",
    "\n",
    "translate(\"she is standing there .\")\n",
    "\n",
    "translate(\"he is a bad man .\")\n",
    "\n",
    "translate(\"he wants to sleep .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
